{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a00172b-9894-4971-a537-441334e1f38d",
   "metadata": {},
   "source": [
    "### First Models\n",
    "\n",
    "LogReg *baseline model*\n",
    "\n",
    "Other Models *more advanced but still not tuned*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96435299-050b-4010-b57c-cc490b83b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters\n",
    "\n",
    "# If you need to plot or visualize data later on\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For any data preprocessing or manipulation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Depending on the models you plan to use\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38da3325-d76d-421e-94d0-0596c6b95d3b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37dbff3e-7b6b-4798-9190-3d0180925068",
   "metadata": {},
   "outputs": [],
   "source": [
    "fazeli_mitbih_train_df = pd.read_csv('../data/mitbih_train.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74aed28a-031c-4298-a7f8-3d01d46e93be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    72471\n",
       "4.0     6431\n",
       "2.0     5788\n",
       "1.0     2223\n",
       "3.0      641\n",
       "Name: 187, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_187 = fazeli_mitbih_train_df.iloc[:, 187]\n",
    "column_187.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee19804-7f3c-4139-8068-df2c0700db04",
   "metadata": {},
   "source": [
    "## Comprehensive Feature Extraction\n",
    "\n",
    "using tsfresh time series comprehensive feature extraction package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e950b2da-a4ef-4a60-b0bf-c3be621ba4da",
   "metadata": {},
   "source": [
    "### Next cell takes 15-30 min to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb38587e-4ec4-449d-80dd-366aa28a7e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|███████████████████████| 35/35 [01:03<00:00,  1.81s/it]\n",
      "Feature Extraction: 100%|███████████████████████| 35/35 [01:08<00:00,  1.95s/it]\n",
      "Feature Extraction: 100%|███████████████████████| 35/35 [01:08<00:00,  1.96s/it]\n",
      "Feature Extraction: 100%|███████████████████████| 35/35 [01:10<00:00,  2.00s/it]\n",
      "Feature Extraction: 100%|███████████████████████| 35/35 [01:11<00:00,  2.04s/it]\n",
      "Feature Extraction: 100%|███████████████████████| 35/35 [01:09<00:00,  2.00s/it]\n",
      "Feature Extraction: 100%|███████████████████████| 35/35 [01:11<00:00,  2.05s/it]\n",
      "Feature Extraction: 100%|███████████████████████| 35/35 [01:11<00:00,  2.06s/it]\n",
      "Feature Extraction: 100%|███████████████████████| 35/35 [00:58<00:00,  1.68s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters\n",
    "\n",
    "# Load your dataset\n",
    "fazeli_mitbih_train_df = pd.read_csv('../data/mitbih_train.csv', header=None)\n",
    "\n",
    "# Assign a unique ID to each row and separate the target variable\n",
    "fazeli_mitbih_train_df['id'] = range(len(fazeli_mitbih_train_df))\n",
    "target_series = fazeli_mitbih_train_df[187]\n",
    "\n",
    "# Keep only features and the unique ID for feature extraction\n",
    "fazeli_mitbih_train_df_features_only = fazeli_mitbih_train_df.drop(columns=[187])\n",
    "\n",
    "# Convert to long format, preserving the 'id' for direct mapping\n",
    "long_df = fazeli_mitbih_train_df_features_only.melt(id_vars='id', var_name='time', value_name='amplitude')\n",
    "\n",
    "# Define feature extraction settings\n",
    "extraction_settings = ComprehensiveFCParameters()\n",
    "\n",
    "# Incremental extraction setup\n",
    "unique_ids = long_df['id'].unique()\n",
    "subset_size = 10000  # Adjust based on your dataset size and memory constraints\n",
    "extracted_features_list = []\n",
    "\n",
    "for i in range(0, len(unique_ids), subset_size):\n",
    "    subset_ids = unique_ids[i:i+subset_size]\n",
    "    subset_df = long_df[long_df['id'].isin(subset_ids)]\n",
    "    \n",
    "    # Extract features for this subset\n",
    "    subset_features = extract_features(subset_df, column_id='id', column_sort='time',\n",
    "                                       default_fc_parameters=extraction_settings, n_jobs=7)\n",
    "    extracted_features_list.append(subset_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ceb8dc3-bb1a-4fc1-8789-1b8cc965c9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label\n",
      "0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "3    0.0\n",
      "4    0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Combine extracted features from all subsets\n",
    "extracted_features = pd.concat(extracted_features_list)\n",
    "\n",
    "# Re-associate the target labels using the 'id' column\n",
    "# This step correctly maps the original labels to the extracted features based on 'id'\n",
    "extracted_features['label'] = extracted_features.index.map(lambda idx: target_series.loc[idx])\n",
    "\n",
    "# Verify the re-association of labels\n",
    "print(extracted_features[['label']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db76b1fa-119a-4165-82db-dd016b4388b7",
   "metadata": {},
   "source": [
    "## Taking a look at the extracted data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc003bcb-40b3-4b03-abd5-4f53301947ad",
   "metadata": {},
   "source": [
    "extracted_features.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0931577-2d38-4fb8-9a4c-a3edbf536c5a",
   "metadata": {},
   "source": [
    "extracted_features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5edb65e-c011-4e06-98a0-b51193ca40f5",
   "metadata": {},
   "source": [
    "extracted_features.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c7c590-5e6c-416a-9ea6-78f610f1a681",
   "metadata": {},
   "source": [
    "extracted_features.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdfd5fa-666a-43a1-9c56-befb33d4867d",
   "metadata": {},
   "source": [
    "extracted_features.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58082168-ea63-41ed-92ca-4a2a80362104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    72471\n",
       "4.0     6431\n",
       "2.0     5788\n",
       "1.0     2223\n",
       "3.0      641\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_187_extracted = extracted_features['label']\n",
    "column_187_extracted.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2dadb1-d85e-471a-9e3b-1c9914d0e1ec",
   "metadata": {},
   "source": [
    "RandomForestClassifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c62a9e-4a10-42fe-ab0e-f70e12238336",
   "metadata": {},
   "source": [
    "## Set X and y from extracted features, Perform train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08955da9-01b4-4570-98af-fd2227d80f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting X and y from my extracted features\n",
    "X = extracted_features.drop('label', axis=1)\n",
    "y = extracted_features['label']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d57de74-29b0-4773-ad89-7f98ae2f4959",
   "metadata": {},
   "source": [
    "## Baseline LogReg Model on PCA-derived Components Arranged in Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbb48b29-bd6d-4f70-8cd9-4b08f96cb131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Model Accuracy: 0.9661355719262178\n",
      "\n",
      "Pipeline Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98     14579\n",
      "         1.0       0.86      0.58      0.69       426\n",
      "         2.0       0.90      0.84      0.87      1112\n",
      "         3.0       0.80      0.70      0.75       145\n",
      "         4.0       0.97      0.95      0.96      1249\n",
      "\n",
      "    accuracy                           0.97     17511\n",
      "   macro avg       0.90      0.81      0.85     17511\n",
      "weighted avg       0.96      0.97      0.96     17511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),  # Incorporate PCA directly into the pipeline\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"Pipeline Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nPipeline Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72efb587-1475-43af-b10a-001de2edaa3b",
   "metadata": {},
   "source": [
    "## Further Models trained PCA-derived Component Features Arranged in Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd34c184-aba2-4ca5-bc0a-0553a106644d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Pipeline Model Accuracy: 0.9567700302666895\n",
      "\n",
      "Random Forest Pipeline Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98     14579\n",
      "         1.0       0.97      0.46      0.62       426\n",
      "         2.0       0.94      0.74      0.83      1112\n",
      "         3.0       0.92      0.32      0.47       145\n",
      "         4.0       1.00      0.91      0.95      1249\n",
      "\n",
      "    accuracy                           0.96     17511\n",
      "   macro avg       0.96      0.68      0.77     17511\n",
      "weighted avg       0.96      0.96      0.95     17511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline_rf = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=7))\n",
    "])\n",
    "\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = pipeline_rf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Pipeline Model Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"\\nRandom Forest Pipeline Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58dbf4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Pipeline Model Accuracy: 0.9758437553537777\n",
      "\n",
      "XGBoost Pipeline Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99     14579\n",
      "         1.0       0.96      0.62      0.75       426\n",
      "         2.0       0.95      0.88      0.92      1112\n",
      "         3.0       0.92      0.71      0.80       145\n",
      "         4.0       0.99      0.97      0.98      1249\n",
      "\n",
      "    accuracy                           0.98     17511\n",
      "   macro avg       0.96      0.83      0.89     17511\n",
      "weighted avg       0.98      0.98      0.97     17511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "pipeline_xgb = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42, n_jobs=7))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_xgb = pipeline_xgb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"XGBoost Pipeline Model Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"\\nXGBoost Pipeline Classification Report:\\n\", classification_report(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "563322d9-430e-4598-8cf0-05db9b9649ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022952 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 64260\n",
      "[LightGBM] [Info] Number of data points in the train set: 70043, number of used features: 252\n",
      "[LightGBM] [Info] Start training from score -0.190530\n",
      "[LightGBM] [Info] Start training from score -3.662991\n",
      "[LightGBM] [Info] Start training from score -2.706666\n",
      "[LightGBM] [Info] Start training from score -4.950289\n",
      "[LightGBM] [Info] Start training from score -2.603918\n",
      "LightGBM Pipeline Model Accuracy: 0.9739592256296042\n",
      "\n",
      "LightGBM Pipeline Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99     14579\n",
      "         1.0       0.95      0.62      0.75       426\n",
      "         2.0       0.94      0.88      0.91      1112\n",
      "         3.0       0.83      0.57      0.68       145\n",
      "         4.0       0.99      0.97      0.98      1249\n",
      "\n",
      "    accuracy                           0.97     17511\n",
      "   macro avg       0.94      0.81      0.86     17511\n",
      "weighted avg       0.97      0.97      0.97     17511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipeline_lgbm = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),  # Optional, depending on necessity of PCA\n",
    "    ('classifier', LGBMClassifier(random_state=42, n_jobs=7))\n",
    "])\n",
    "\n",
    "pipeline_lgbm.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lgbm = pipeline_lgbm.predict(X_test)\n",
    "\n",
    "print(\"LightGBM Pipeline Model Accuracy:\", accuracy_score(y_test, y_pred_lgbm))\n",
    "print(\"\\nLightGBM Pipeline Classification Report:\\n\", classification_report(y_test, y_pred_lgbm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "104afdc7-c4d4-4d5a-be33-a5f530011e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Pipeline Model Accuracy: 0.9756724344697618\n",
      "\n",
      "CatBoost Pipeline Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99     14579\n",
      "         1.0       0.94      0.65      0.77       426\n",
      "         2.0       0.94      0.89      0.91      1112\n",
      "         3.0       0.94      0.68      0.79       145\n",
      "         4.0       0.99      0.97      0.98      1249\n",
      "\n",
      "    accuracy                           0.98     17511\n",
      "   macro avg       0.96      0.83      0.89     17511\n",
      "weighted avg       0.98      0.98      0.97     17511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "n_jobs = 7\n",
    "\n",
    "pipeline_catboost = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('classifier', CatBoostClassifier(silent=True, random_state=42, thread_count=n_jobs))\n",
    "])\n",
    "\n",
    "\n",
    "pipeline_catboost.fit(X_train, y_train)\n",
    "\n",
    "y_pred_catboost = pipeline_catboost.predict(X_test)\n",
    "\n",
    "print(\"CatBoost Pipeline Model Accuracy:\", accuracy_score(y_test, y_pred_catboost))\n",
    "print(\"\\nCatBoost Pipeline Classification Report:\\n\", classification_report(y_test, y_pred_catboost))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab7031-a594-4d98-b8a6-0cb911a52200",
   "metadata": {},
   "source": [
    "### having trouble running the tensorflow neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3752e9f-7d95-4eb6-b309-17f3758c4b8d",
   "metadata": {},
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming 'extracted_features' and 'label' preparation is done\n",
    "X = extracted_features.drop('label', axis=1)\n",
    "y = extracted_features['label']\n",
    "\n",
    "# One-hot encoding of labels\n",
    "y_encoded = to_categorical(y)\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Neural network architecture\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(y_train_encoded.shape[1], activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(X_train_scaled, y_train_encoded, epochs=100, batch_size=128, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test_encoded, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13471df3-b6c2-4d76-8613-a7b9a52d557b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Model Accuracy: 0.9789846382273999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "pipeline_svc = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', SVC(kernel='rbf', random_state=42))\n",
    "])\n",
    "\n",
    "pipeline_svc.fit(X_train, y_train)\n",
    "y_pred_svc = pipeline_svc.predict(X_test)\n",
    "\n",
    "print(\"SVC Model Accuracy:\", accuracy_score(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56b1976-e836-4fad-9d89-125a20dd06ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
