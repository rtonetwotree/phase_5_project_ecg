{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8a00172b-9894-4971-a537-441334e1f38d",
      "metadata": {
        "id": "8a00172b-9894-4971-a537-441334e1f38d"
      },
      "source": [
        "## First Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tsfresh\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWrZla58tcRp",
        "outputId": "8b97414e-b0b2-4209-b99d-79af14fd18aa"
      },
      "id": "RWrZla58tcRp",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tsfresh\n",
            "  Downloading tsfresh-0.20.2-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/95.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m92.2/95.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (2.31.0)\n",
            "Requirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (1.5.3)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (1.11.4)\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (0.14.1)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (0.5.6)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (4.66.2)\n",
            "Collecting stumpy>=1.7.2 (from tsfresh)\n",
            "  Downloading stumpy-1.12.0-py3-none-any.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.1/169.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from tsfresh) (2.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.0->tsfresh) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.0->tsfresh) (2023.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.4.1->tsfresh) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.1->tsfresh) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.1->tsfresh) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.1->tsfresh) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.1->tsfresh) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->tsfresh) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->tsfresh) (3.3.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13->tsfresh) (23.2)\n",
            "Requirement already satisfied: numba>=0.55.2 in /usr/local/lib/python3.10/dist-packages (from stumpy>=1.7.2->tsfresh) (0.58.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.55.2->stumpy>=1.7.2->tsfresh) (0.41.1)\n",
            "Installing collected packages: stumpy, tsfresh\n",
            "Successfully installed stumpy-1.12.0 tsfresh-0.20.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install keras-tuner\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AYlBpR4rLRL",
        "outputId": "7f6cca59-c4f4-4ffd-9e22-5eeea58f4615"
      },
      "id": "1AYlBpR4rLRL",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m122.9/129.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPQBOk7Rugm3",
        "outputId": "3dca5a24-f4bd-40e1-e087-722e695d8cbf"
      },
      "id": "BPQBOk7Rugm3",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "96435299-050b-4010-b57c-cc490b83b705",
      "metadata": {
        "id": "96435299-050b-4010-b57c-cc490b83b705"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tsfresh import extract_features\n",
        "from tsfresh.feature_extraction import ComprehensiveFCParameters\n",
        "\n",
        "# If you need to plot or visualize data later on\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# For any data preprocessing or manipulation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Depending on the models you plan to use\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38da3325-d76d-421e-94d0-0596c6b95d3b",
      "metadata": {
        "id": "38da3325-d76d-421e-94d0-0596c6b95d3b"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "37dbff3e-7b6b-4798-9190-3d0180925068",
      "metadata": {
        "id": "37dbff3e-7b6b-4798-9190-3d0180925068"
      },
      "outputs": [],
      "source": [
        "fazeli_mitbih_train_df = pd.read_csv('/content/drive/MyDrive/mitbih_train.csv', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fazeli_mitbih_test_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/mitbih_test.csv', header=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "KwjAVYgmE55a",
        "outputId": "a79a6644-9cf1-4f92-f0ed-43b827bcca04"
      },
      "id": "KwjAVYgmE55a",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c60300b76c09>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfazeli_mitbih_test_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/mitbih_test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1776\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "74aed28a-031c-4298-a7f8-3d01d46e93be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74aed28a-031c-4298-a7f8-3d01d46e93be",
        "outputId": "cbc1b04a-a8f5-44e8-ef40-2c8b969fd72d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    72471\n",
              "4.0     6431\n",
              "2.0     5788\n",
              "1.0     2223\n",
              "3.0      641\n",
              "Name: 187, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "column_187 = fazeli_mitbih_train_df.iloc[:, 187]\n",
        "column_187.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3376e4c2-e6a3-41fc-8edd-3580bf98081c",
      "metadata": {
        "id": "3376e4c2-e6a3-41fc-8edd-3580bf98081c"
      },
      "source": [
        "# WIP Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dense, Dropout\n",
        "\n",
        "# Assuming your input shape is (timesteps, features)\n",
        "input_shape = (187, 1)  # Adjusted input shape to match your dataset\n",
        "\n",
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=input_shape),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.1),\n",
        "\n",
        "    Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.1),\n",
        "\n",
        "    Conv1D(filters=256, kernel_size=3, activation='relu'),\n",
        "    GlobalAveragePooling1D(),\n",
        "\n",
        "    Dense(100, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(5, activation='softmax')  # Adjusted for 5 classes\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "11PLqEnMyWwo",
        "outputId": "c1abad8c-e65c-450a-eb70-db389ffc54b9"
      },
      "id": "11PLqEnMyWwo",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 183, 64)           384       \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 91, 64)            0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 91, 64)            0         \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 89, 128)           24704     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 44, 128)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 44, 128)           0         \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 42, 256)           98560     \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 256)               0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               25700     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 505       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 149853 (585.36 KB)\n",
            "Trainable params: 149853 (585.36 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a8249bc9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "a8249bc9",
        "outputId": "85d6ff71-ab0c-45be-f54f-8082bb90fd55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2189/2189 [==============================] - 16s 5ms/step - loss: 0.4301 - accuracy: 0.8802 - val_loss: 0.2590 - val_accuracy: 0.9272\n",
            "Epoch 2/10\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.2541 - accuracy: 0.9300 - val_loss: 0.2095 - val_accuracy: 0.9407\n",
            "Epoch 3/10\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.2117 - accuracy: 0.9406 - val_loss: 0.1713 - val_accuracy: 0.9485\n",
            "Epoch 4/10\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.1876 - accuracy: 0.9476 - val_loss: 0.1489 - val_accuracy: 0.9556\n",
            "Epoch 5/10\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.1692 - accuracy: 0.9517 - val_loss: 0.1395 - val_accuracy: 0.9603\n",
            "Epoch 6/10\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.1561 - accuracy: 0.9564 - val_loss: 0.1250 - val_accuracy: 0.9633\n",
            "Epoch 7/10\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.1436 - accuracy: 0.9600 - val_loss: 0.1104 - val_accuracy: 0.9689\n",
            "Epoch 8/10\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.1336 - accuracy: 0.9631 - val_loss: 0.1069 - val_accuracy: 0.9691\n",
            "Epoch 9/10\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.1294 - accuracy: 0.9652 - val_loss: 0.1021 - val_accuracy: 0.9702\n",
            "Epoch 10/10\n",
            "2189/2189 [==============================] - 10s 4ms/step - loss: 0.1232 - accuracy: 0.9660 - val_loss: 0.1192 - val_accuracy: 0.9651\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x790a5f51b280>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming fazeli_mitbih_train_df is your DataFrame and the last column is the label\n",
        "X = fazeli_mitbih_train_df.iloc[:, :-1].values\n",
        "y = fazeli_mitbih_train_df.iloc[:, -1].values\n",
        "\n",
        "# Reshape X to fit the Conv1D input requirements and convert y to categorical\n",
        "X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "y_categorical = to_categorical(y)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# Assuming fazeli_mitbih_train_df is your DataFrame and the last column is the label\n",
        "X = fazeli_mitbih_train_df.iloc[:, :-1].values\n",
        "y = fazeli_mitbih_train_df.iloc[:, -1].values\n",
        "\n",
        "# Reshape X to fit the Conv1D input requirements and convert y to categorical\n",
        "X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "y_categorical = to_categorical(y)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the ModelCheckpoint callback\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'best_model.h5',  # Path where to save the model\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,  # Only save the best model\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Define the early stopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Define the ReduceLROnPlateau callback\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=5,\n",
        "    min_lr=0.001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model with the callbacks\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=100,  # Adjust the number of epochs if necessary\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, reduce_lr, model_checkpoint]  # Add ModelCheckpoint to the callbacks list\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sRK7-VA-9SNz",
        "outputId": "3dfa4832-f99d-4295-9799-2c5a54ede835"
      },
      "id": "sRK7-VA-9SNz",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2180/2189 [============================>.] - ETA: 0s - loss: 0.1183 - accuracy: 0.9681\n",
            "Epoch 1: val_loss improved from inf to 0.09765, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.1184 - accuracy: 0.9681 - val_loss: 0.0976 - val_accuracy: 0.9736 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "  39/2189 [..............................] - ETA: 8s - loss: 0.1191 - accuracy: 0.9696"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2181/2189 [============================>.] - ETA: 0s - loss: 0.1127 - accuracy: 0.9690\n",
            "Epoch 2: val_loss improved from 0.09765 to 0.09134, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.1125 - accuracy: 0.9690 - val_loss: 0.0913 - val_accuracy: 0.9742 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "2181/2189 [============================>.] - ETA: 0s - loss: 0.1088 - accuracy: 0.9703\n",
            "Epoch 3: val_loss did not improve from 0.09134\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.1087 - accuracy: 0.9704 - val_loss: 0.0951 - val_accuracy: 0.9745 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1039 - accuracy: 0.9714\n",
            "Epoch 4: val_loss improved from 0.09134 to 0.08341, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.1039 - accuracy: 0.9714 - val_loss: 0.0834 - val_accuracy: 0.9760 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.1013 - accuracy: 0.9719\n",
            "Epoch 5: val_loss improved from 0.08341 to 0.08326, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.1013 - accuracy: 0.9719 - val_loss: 0.0833 - val_accuracy: 0.9767 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "2184/2189 [============================>.] - ETA: 0s - loss: 0.1007 - accuracy: 0.9725\n",
            "Epoch 6: val_loss did not improve from 0.08326\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.1007 - accuracy: 0.9725 - val_loss: 0.0953 - val_accuracy: 0.9744 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "2184/2189 [============================>.] - ETA: 0s - loss: 0.0963 - accuracy: 0.9738\n",
            "Epoch 7: val_loss improved from 0.08326 to 0.08261, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0965 - accuracy: 0.9738 - val_loss: 0.0826 - val_accuracy: 0.9788 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "2181/2189 [============================>.] - ETA: 0s - loss: 0.0938 - accuracy: 0.9745\n",
            "Epoch 8: val_loss improved from 0.08261 to 0.08088, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0936 - accuracy: 0.9746 - val_loss: 0.0809 - val_accuracy: 0.9772 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "2182/2189 [============================>.] - ETA: 0s - loss: 0.0895 - accuracy: 0.9755\n",
            "Epoch 9: val_loss improved from 0.08088 to 0.07768, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0894 - accuracy: 0.9756 - val_loss: 0.0777 - val_accuracy: 0.9788 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "2182/2189 [============================>.] - ETA: 0s - loss: 0.0897 - accuracy: 0.9745\n",
            "Epoch 10: val_loss improved from 0.07768 to 0.07697, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0896 - accuracy: 0.9745 - val_loss: 0.0770 - val_accuracy: 0.9788 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.0873 - accuracy: 0.9765\n",
            "Epoch 11: val_loss improved from 0.07697 to 0.06820, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0872 - accuracy: 0.9765 - val_loss: 0.0682 - val_accuracy: 0.9804 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.0850 - accuracy: 0.9765\n",
            "Epoch 12: val_loss did not improve from 0.06820\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0852 - accuracy: 0.9765 - val_loss: 0.0767 - val_accuracy: 0.9782 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "2183/2189 [============================>.] - ETA: 0s - loss: 0.0824 - accuracy: 0.9771\n",
            "Epoch 13: val_loss did not improve from 0.06820\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0825 - accuracy: 0.9771 - val_loss: 0.0728 - val_accuracy: 0.9804 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "2185/2189 [============================>.] - ETA: 0s - loss: 0.0820 - accuracy: 0.9776\n",
            "Epoch 14: val_loss did not improve from 0.06820\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0820 - accuracy: 0.9776 - val_loss: 0.0709 - val_accuracy: 0.9806 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "2180/2189 [============================>.] - ETA: 0s - loss: 0.0792 - accuracy: 0.9780\n",
            "Epoch 15: val_loss did not improve from 0.06820\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0791 - accuracy: 0.9781 - val_loss: 0.0720 - val_accuracy: 0.9809 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "2177/2189 [============================>.] - ETA: 0s - loss: 0.0789 - accuracy: 0.9781\n",
            "Epoch 16: val_loss improved from 0.06820 to 0.06663, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0790 - accuracy: 0.9780 - val_loss: 0.0666 - val_accuracy: 0.9814 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "2177/2189 [============================>.] - ETA: 0s - loss: 0.0751 - accuracy: 0.9790\n",
            "Epoch 17: val_loss did not improve from 0.06663\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0753 - accuracy: 0.9790 - val_loss: 0.0670 - val_accuracy: 0.9825 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "2182/2189 [============================>.] - ETA: 0s - loss: 0.0757 - accuracy: 0.9790\n",
            "Epoch 18: val_loss did not improve from 0.06663\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0757 - accuracy: 0.9790 - val_loss: 0.0690 - val_accuracy: 0.9798 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "2177/2189 [============================>.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9794\n",
            "Epoch 19: val_loss did not improve from 0.06663\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0733 - accuracy: 0.9794 - val_loss: 0.0702 - val_accuracy: 0.9804 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "2180/2189 [============================>.] - ETA: 0s - loss: 0.0714 - accuracy: 0.9801\n",
            "Epoch 20: val_loss improved from 0.06663 to 0.06609, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0714 - accuracy: 0.9800 - val_loss: 0.0661 - val_accuracy: 0.9829 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "2182/2189 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.9803\n",
            "Epoch 21: val_loss did not improve from 0.06609\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0713 - accuracy: 0.9803 - val_loss: 0.0708 - val_accuracy: 0.9825 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "2180/2189 [============================>.] - ETA: 0s - loss: 0.0703 - accuracy: 0.9802\n",
            "Epoch 22: val_loss did not improve from 0.06609\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0702 - accuracy: 0.9803 - val_loss: 0.0671 - val_accuracy: 0.9817 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0685 - accuracy: 0.9810\n",
            "Epoch 23: val_loss did not improve from 0.06609\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0685 - accuracy: 0.9810 - val_loss: 0.0679 - val_accuracy: 0.9821 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.9803\n",
            "Epoch 24: val_loss did not improve from 0.06609\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0681 - accuracy: 0.9803 - val_loss: 0.0683 - val_accuracy: 0.9817 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "2185/2189 [============================>.] - ETA: 0s - loss: 0.0672 - accuracy: 0.9809\n",
            "Epoch 25: val_loss did not improve from 0.06609\n",
            "2189/2189 [==============================] - 10s 4ms/step - loss: 0.0671 - accuracy: 0.9809 - val_loss: 0.0668 - val_accuracy: 0.9825 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "2180/2189 [============================>.] - ETA: 0s - loss: 0.0662 - accuracy: 0.9812\n",
            "Epoch 26: val_loss did not improve from 0.06609\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0662 - accuracy: 0.9812 - val_loss: 0.0701 - val_accuracy: 0.9823 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0644 - accuracy: 0.9813\n",
            "Epoch 27: val_loss did not improve from 0.06609\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0643 - accuracy: 0.9814 - val_loss: 0.0724 - val_accuracy: 0.9817 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "2182/2189 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.9818\n",
            "Epoch 28: val_loss improved from 0.06609 to 0.06451, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0637 - accuracy: 0.9818 - val_loss: 0.0645 - val_accuracy: 0.9824 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "2185/2189 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.9819\n",
            "Epoch 29: val_loss improved from 0.06451 to 0.06232, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0636 - accuracy: 0.9819 - val_loss: 0.0623 - val_accuracy: 0.9837 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "2180/2189 [============================>.] - ETA: 0s - loss: 0.0628 - accuracy: 0.9820\n",
            "Epoch 30: val_loss did not improve from 0.06232\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0627 - accuracy: 0.9820 - val_loss: 0.0671 - val_accuracy: 0.9829 - lr: 0.0010\n",
            "Epoch 31/100\n",
            "2180/2189 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9824\n",
            "Epoch 31: val_loss did not improve from 0.06232\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0619 - accuracy: 0.9824 - val_loss: 0.0670 - val_accuracy: 0.9826 - lr: 0.0010\n",
            "Epoch 32/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0601 - accuracy: 0.9825\n",
            "Epoch 32: val_loss did not improve from 0.06232\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0601 - accuracy: 0.9825 - val_loss: 0.0663 - val_accuracy: 0.9826 - lr: 0.0010\n",
            "Epoch 33/100\n",
            "2181/2189 [============================>.] - ETA: 0s - loss: 0.0608 - accuracy: 0.9822\n",
            "Epoch 33: val_loss did not improve from 0.06232\n",
            "2189/2189 [==============================] - 10s 4ms/step - loss: 0.0608 - accuracy: 0.9822 - val_loss: 0.0634 - val_accuracy: 0.9840 - lr: 0.0010\n",
            "Epoch 34/100\n",
            "2178/2189 [============================>.] - ETA: 0s - loss: 0.0585 - accuracy: 0.9835\n",
            "Epoch 34: val_loss did not improve from 0.06232\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0585 - accuracy: 0.9834 - val_loss: 0.0654 - val_accuracy: 0.9821 - lr: 0.0010\n",
            "Epoch 35/100\n",
            "2181/2189 [============================>.] - ETA: 0s - loss: 0.0589 - accuracy: 0.9831\n",
            "Epoch 35: val_loss did not improve from 0.06232\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0589 - accuracy: 0.9831 - val_loss: 0.0627 - val_accuracy: 0.9830 - lr: 0.0010\n",
            "Epoch 36/100\n",
            "2181/2189 [============================>.] - ETA: 0s - loss: 0.0589 - accuracy: 0.9830\n",
            "Epoch 36: val_loss did not improve from 0.06232\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0590 - accuracy: 0.9830 - val_loss: 0.0635 - val_accuracy: 0.9832 - lr: 0.0010\n",
            "Epoch 37/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.0562 - accuracy: 0.9832\n",
            "Epoch 37: val_loss did not improve from 0.06232\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0562 - accuracy: 0.9832 - val_loss: 0.0647 - val_accuracy: 0.9832 - lr: 0.0010\n",
            "Epoch 38/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.0570 - accuracy: 0.9832\n",
            "Epoch 38: val_loss did not improve from 0.06232\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0569 - accuracy: 0.9832 - val_loss: 0.0667 - val_accuracy: 0.9835 - lr: 0.0010\n",
            "Epoch 39/100\n",
            "2185/2189 [============================>.] - ETA: 0s - loss: 0.0571 - accuracy: 0.9829Restoring model weights from the end of the best epoch: 29.\n",
            "\n",
            "Epoch 39: val_loss did not improve from 0.06232\n",
            "2189/2189 [==============================] - 10s 5ms/step - loss: 0.0571 - accuracy: 0.9829 - val_loss: 0.0666 - val_accuracy: 0.9824 - lr: 0.0010\n",
            "Epoch 39: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dense, Dropout, BatchNormalization\n",
        "\n",
        "# Assuming your input shape is (timesteps, features)\n",
        "input_shape = (187, 1)  # Adjusted input shape to match your dataset\n",
        "\n",
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=input_shape),\n",
        "    BatchNormalization(),  # Batch Normalization layer after Convolution\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.1),\n",
        "\n",
        "    Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
        "    BatchNormalization(),  # Another Batch Normalization layer\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.1),\n",
        "\n",
        "    Conv1D(filters=256, kernel_size=3, activation='relu'),\n",
        "    BatchNormalization(),  # And another one\n",
        "    GlobalAveragePooling1D(),\n",
        "\n",
        "    Dense(100, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(5, activation='softmax')  # Adjusted for 5 classes\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZJcPnKch3GC9",
        "outputId": "42ab9379-57ca-4920-f09a-b759f3ceadca"
      },
      "id": "ZJcPnKch3GC9",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_3 (Conv1D)           (None, 183, 64)           384       \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 183, 64)           256       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPoolin  (None, 91, 64)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 91, 64)            0         \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 89, 128)           24704     \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 89, 128)           512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPoolin  (None, 44, 128)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 44, 128)           0         \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 42, 256)           98560     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 42, 256)           1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " global_average_pooling1d_1  (None, 256)               0         \n",
            "  (GlobalAveragePooling1D)                                       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 100)               25700     \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 5)                 505       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 151645 (592.36 KB)\n",
            "Trainable params: 150749 (588.86 KB)\n",
            "Non-trainable params: 896 (3.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# Assuming fazeli_mitbih_train_df is your DataFrame and the last column is the label\n",
        "X = fazeli_mitbih_train_df.iloc[:, :-1].values\n",
        "y = fazeli_mitbih_train_df.iloc[:, -1].values\n",
        "\n",
        "# Reshape X to fit the Conv1D input requirements and convert y to categorical\n",
        "X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "y_categorical = to_categorical(y)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the ModelCheckpoint callback\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'best_model.h5',  # Path where to save the model\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,  # Only save the best model\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Define the early stopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Define the ReduceLROnPlateau callback\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=5,\n",
        "    min_lr=0.001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model with the callbacks\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=100,  # Adjust the number of epochs if necessary\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, reduce_lr, model_checkpoint]  # Add ModelCheckpoint to the callbacks list\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uXN32U4WAonf",
        "outputId": "95147839-c363-4236-f205-259c99fa8cf1"
      },
      "id": "uXN32U4WAonf",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2183/2189 [============================>.] - ETA: 0s - loss: 0.2579 - accuracy: 0.9287\n",
            "Epoch 1: val_loss improved from inf to 0.15190, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 18s 7ms/step - loss: 0.2580 - accuracy: 0.9287 - val_loss: 0.1519 - val_accuracy: 0.9611 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "  19/2189 [..............................] - ETA: 13s - loss: 0.1197 - accuracy: 0.9655"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2185/2189 [============================>.] - ETA: 0s - loss: 0.1517 - accuracy: 0.9583\n",
            "Epoch 2: val_loss did not improve from 0.15190\n",
            "2189/2189 [==============================] - 14s 6ms/step - loss: 0.1516 - accuracy: 0.9584 - val_loss: 0.1894 - val_accuracy: 0.9445 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.1279 - accuracy: 0.9650\n",
            "Epoch 3: val_loss improved from 0.15190 to 0.10350, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.1280 - accuracy: 0.9650 - val_loss: 0.1035 - val_accuracy: 0.9707 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "2182/2189 [============================>.] - ETA: 0s - loss: 0.1175 - accuracy: 0.9677\n",
            "Epoch 4: val_loss improved from 0.10350 to 0.09959, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.1175 - accuracy: 0.9677 - val_loss: 0.0996 - val_accuracy: 0.9750 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.1080 - accuracy: 0.9701\n",
            "Epoch 5: val_loss improved from 0.09959 to 0.08106, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.1080 - accuracy: 0.9701 - val_loss: 0.0811 - val_accuracy: 0.9772 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0986 - accuracy: 0.9725\n",
            "Epoch 6: val_loss did not improve from 0.08106\n",
            "2189/2189 [==============================] - 14s 7ms/step - loss: 0.0985 - accuracy: 0.9725 - val_loss: 0.1212 - val_accuracy: 0.9621 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0946 - accuracy: 0.9743\n",
            "Epoch 7: val_loss did not improve from 0.08106\n",
            "2189/2189 [==============================] - 14s 7ms/step - loss: 0.0945 - accuracy: 0.9743 - val_loss: 0.1010 - val_accuracy: 0.9721 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "2182/2189 [============================>.] - ETA: 0s - loss: 0.0909 - accuracy: 0.9748\n",
            "Epoch 8: val_loss improved from 0.08106 to 0.07551, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0909 - accuracy: 0.9748 - val_loss: 0.0755 - val_accuracy: 0.9770 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9762\n",
            "Epoch 9: val_loss improved from 0.07551 to 0.06947, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0871 - accuracy: 0.9763 - val_loss: 0.0695 - val_accuracy: 0.9822 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "2181/2189 [============================>.] - ETA: 0s - loss: 0.0833 - accuracy: 0.9773\n",
            "Epoch 10: val_loss improved from 0.06947 to 0.06794, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0832 - accuracy: 0.9773 - val_loss: 0.0679 - val_accuracy: 0.9814 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "2182/2189 [============================>.] - ETA: 0s - loss: 0.0822 - accuracy: 0.9770\n",
            "Epoch 11: val_loss did not improve from 0.06794\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0821 - accuracy: 0.9770 - val_loss: 0.0767 - val_accuracy: 0.9786 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "2182/2189 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.9783\n",
            "Epoch 12: val_loss improved from 0.06794 to 0.06559, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0780 - accuracy: 0.9783 - val_loss: 0.0656 - val_accuracy: 0.9819 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9788\n",
            "Epoch 13: val_loss did not improve from 0.06559\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0767 - accuracy: 0.9788 - val_loss: 0.0749 - val_accuracy: 0.9791 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0752 - accuracy: 0.9794\n",
            "Epoch 14: val_loss did not improve from 0.06559\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0753 - accuracy: 0.9794 - val_loss: 0.2018 - val_accuracy: 0.9464 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "2184/2189 [============================>.] - ETA: 0s - loss: 0.0717 - accuracy: 0.9802\n",
            "Epoch 15: val_loss improved from 0.06559 to 0.06092, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0716 - accuracy: 0.9803 - val_loss: 0.0609 - val_accuracy: 0.9826 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "2184/2189 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9798\n",
            "Epoch 16: val_loss did not improve from 0.06092\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0720 - accuracy: 0.9799 - val_loss: 0.0658 - val_accuracy: 0.9818 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.0693 - accuracy: 0.9803\n",
            "Epoch 17: val_loss did not improve from 0.06092\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0693 - accuracy: 0.9803 - val_loss: 0.0694 - val_accuracy: 0.9809 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0665 - accuracy: 0.9810\n",
            "Epoch 18: val_loss did not improve from 0.06092\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0664 - accuracy: 0.9811 - val_loss: 0.0786 - val_accuracy: 0.9805 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9811\n",
            "Epoch 19: val_loss did not improve from 0.06092\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0658 - accuracy: 0.9811 - val_loss: 0.0740 - val_accuracy: 0.9805 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "2182/2189 [============================>.] - ETA: 0s - loss: 0.0650 - accuracy: 0.9810\n",
            "Epoch 20: val_loss did not improve from 0.06092\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0650 - accuracy: 0.9810 - val_loss: 0.0670 - val_accuracy: 0.9824 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "2185/2189 [============================>.] - ETA: 0s - loss: 0.0641 - accuracy: 0.9816\n",
            "Epoch 21: val_loss did not improve from 0.06092\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0641 - accuracy: 0.9816 - val_loss: 0.0702 - val_accuracy: 0.9816 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "2181/2189 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 0.9820\n",
            "Epoch 22: val_loss did not improve from 0.06092\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0621 - accuracy: 0.9820 - val_loss: 0.1484 - val_accuracy: 0.9689 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "2183/2189 [============================>.] - ETA: 0s - loss: 0.0615 - accuracy: 0.9823\n",
            "Epoch 23: val_loss did not improve from 0.06092\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0616 - accuracy: 0.9823 - val_loss: 0.0732 - val_accuracy: 0.9812 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "2182/2189 [============================>.] - ETA: 0s - loss: 0.0609 - accuracy: 0.9819\n",
            "Epoch 24: val_loss improved from 0.06092 to 0.05970, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 14s 7ms/step - loss: 0.0608 - accuracy: 0.9820 - val_loss: 0.0597 - val_accuracy: 0.9829 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "2183/2189 [============================>.] - ETA: 0s - loss: 0.0595 - accuracy: 0.9830\n",
            "Epoch 25: val_loss did not improve from 0.05970\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0595 - accuracy: 0.9830 - val_loss: 0.1028 - val_accuracy: 0.9702 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9827\n",
            "Epoch 26: val_loss did not improve from 0.05970\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0595 - accuracy: 0.9827 - val_loss: 0.0667 - val_accuracy: 0.9841 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "2184/2189 [============================>.] - ETA: 0s - loss: 0.0572 - accuracy: 0.9839\n",
            "Epoch 27: val_loss did not improve from 0.05970\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0573 - accuracy: 0.9839 - val_loss: 0.0640 - val_accuracy: 0.9834 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0562 - accuracy: 0.9833\n",
            "Epoch 28: val_loss improved from 0.05970 to 0.05830, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0562 - accuracy: 0.9833 - val_loss: 0.0583 - val_accuracy: 0.9843 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0573 - accuracy: 0.9831\n",
            "Epoch 29: val_loss did not improve from 0.05830\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0573 - accuracy: 0.9831 - val_loss: 0.0692 - val_accuracy: 0.9829 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "2185/2189 [============================>.] - ETA: 0s - loss: 0.0554 - accuracy: 0.9839\n",
            "Epoch 30: val_loss improved from 0.05830 to 0.05776, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0555 - accuracy: 0.9839 - val_loss: 0.0578 - val_accuracy: 0.9856 - lr: 0.0010\n",
            "Epoch 31/100\n",
            "2184/2189 [============================>.] - ETA: 0s - loss: 0.0542 - accuracy: 0.9838\n",
            "Epoch 31: val_loss did not improve from 0.05776\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0542 - accuracy: 0.9838 - val_loss: 0.0591 - val_accuracy: 0.9848 - lr: 0.0010\n",
            "Epoch 32/100\n",
            "2181/2189 [============================>.] - ETA: 0s - loss: 0.0546 - accuracy: 0.9841\n",
            "Epoch 32: val_loss did not improve from 0.05776\n",
            "2189/2189 [==============================] - 14s 7ms/step - loss: 0.0547 - accuracy: 0.9841 - val_loss: 0.0595 - val_accuracy: 0.9849 - lr: 0.0010\n",
            "Epoch 33/100\n",
            "2182/2189 [============================>.] - ETA: 0s - loss: 0.0523 - accuracy: 0.9845\n",
            "Epoch 33: val_loss did not improve from 0.05776\n",
            "2189/2189 [==============================] - 14s 7ms/step - loss: 0.0523 - accuracy: 0.9845 - val_loss: 0.0692 - val_accuracy: 0.9834 - lr: 0.0010\n",
            "Epoch 34/100\n",
            "2182/2189 [============================>.] - ETA: 0s - loss: 0.0534 - accuracy: 0.9839\n",
            "Epoch 34: val_loss did not improve from 0.05776\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0534 - accuracy: 0.9839 - val_loss: 0.0670 - val_accuracy: 0.9809 - lr: 0.0010\n",
            "Epoch 35/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0529 - accuracy: 0.9841\n",
            "Epoch 35: val_loss did not improve from 0.05776\n",
            "2189/2189 [==============================] - 14s 7ms/step - loss: 0.0529 - accuracy: 0.9841 - val_loss: 0.0586 - val_accuracy: 0.9846 - lr: 0.0010\n",
            "Epoch 36/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0520 - accuracy: 0.9851\n",
            "Epoch 36: val_loss improved from 0.05776 to 0.05427, saving model to best_model.h5\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0519 - accuracy: 0.9851 - val_loss: 0.0543 - val_accuracy: 0.9857 - lr: 0.0010\n",
            "Epoch 37/100\n",
            "2182/2189 [============================>.] - ETA: 0s - loss: 0.0514 - accuracy: 0.9852\n",
            "Epoch 37: val_loss did not improve from 0.05427\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0513 - accuracy: 0.9853 - val_loss: 0.0578 - val_accuracy: 0.9849 - lr: 0.0010\n",
            "Epoch 38/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0508 - accuracy: 0.9845\n",
            "Epoch 38: val_loss did not improve from 0.05427\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0508 - accuracy: 0.9845 - val_loss: 0.0604 - val_accuracy: 0.9834 - lr: 0.0010\n",
            "Epoch 39/100\n",
            "2184/2189 [============================>.] - ETA: 0s - loss: 0.0504 - accuracy: 0.9854\n",
            "Epoch 39: val_loss did not improve from 0.05427\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0504 - accuracy: 0.9854 - val_loss: 0.0665 - val_accuracy: 0.9820 - lr: 0.0010\n",
            "Epoch 40/100\n",
            "2183/2189 [============================>.] - ETA: 0s - loss: 0.0494 - accuracy: 0.9853\n",
            "Epoch 40: val_loss did not improve from 0.05427\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0494 - accuracy: 0.9853 - val_loss: 0.0653 - val_accuracy: 0.9827 - lr: 0.0010\n",
            "Epoch 41/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.0509 - accuracy: 0.9852\n",
            "Epoch 41: val_loss did not improve from 0.05427\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0508 - accuracy: 0.9852 - val_loss: 0.0548 - val_accuracy: 0.9850 - lr: 0.0010\n",
            "Epoch 42/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9856\n",
            "Epoch 42: val_loss did not improve from 0.05427\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0487 - accuracy: 0.9856 - val_loss: 0.0596 - val_accuracy: 0.9849 - lr: 0.0010\n",
            "Epoch 43/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9860\n",
            "Epoch 43: val_loss did not improve from 0.05427\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0482 - accuracy: 0.9860 - val_loss: 0.0576 - val_accuracy: 0.9854 - lr: 0.0010\n",
            "Epoch 44/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 0.9860\n",
            "Epoch 44: val_loss did not improve from 0.05427\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0455 - accuracy: 0.9860 - val_loss: 0.0578 - val_accuracy: 0.9858 - lr: 0.0010\n",
            "Epoch 45/100\n",
            "2183/2189 [============================>.] - ETA: 0s - loss: 0.0463 - accuracy: 0.9862\n",
            "Epoch 45: val_loss did not improve from 0.05427\n",
            "2189/2189 [==============================] - 15s 7ms/step - loss: 0.0465 - accuracy: 0.9862 - val_loss: 0.0615 - val_accuracy: 0.9847 - lr: 0.0010\n",
            "Epoch 46/100\n",
            "2185/2189 [============================>.] - ETA: 0s - loss: 0.0468 - accuracy: 0.9860Restoring model weights from the end of the best epoch: 36.\n",
            "\n",
            "Epoch 46: val_loss did not improve from 0.05427\n",
            "2189/2189 [==============================] - 14s 7ms/step - loss: 0.0468 - accuracy: 0.9860 - val_loss: 0.0676 - val_accuracy: 0.9844 - lr: 0.0010\n",
            "Epoch 46: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Assuming the last column is the label\n",
        "X_test_df = fazeli_mitbih_test_df.iloc[:, :-1].values\n",
        "y_test_df = fazeli_mitbih_test_df.iloc[:, -1].values\n",
        "\n",
        "# Reshape X to fit the Conv1D input requirements\n",
        "X_test_reshaped = X_test_df.reshape((X_test_df.shape[0], X_test_df.shape[1], 1))\n",
        "\n",
        "# Convert y to categorical if your model's output is categorical\n",
        "y_test_categorical = to_categorical(y_test_df)\n",
        "\n",
        "# Evaluate the model on the separate test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test_categorical)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9vmNFaTVAtFE",
        "outputId": "678280ab-9f3f-4479-f1c9-b8c07137ce06"
      },
      "id": "9vmNFaTVAtFE",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "685/685 [==============================] - 2s 3ms/step - loss: 0.0649 - accuracy: 0.9846\n",
            "Test Loss: 0.06490693241357803\n",
            "Test Accuracy: 0.9845605492591858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras_tuner import HyperModel\n",
        "\n",
        "class CNNHyperModel(HyperModel):\n",
        "    def __init__(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "        model.add(Conv1D(\n",
        "            filters=hp.Int('filters_1', min_value=32, max_value=128, step=32),\n",
        "            kernel_size=hp.Choice('kernel_size_1', values=[3, 5]),\n",
        "            activation='relu', input_shape=self.input_shape))\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_1', min_value=0.0, max_value=0.5, default=0.25, step=0.05)))\n",
        "\n",
        "        model.add(Conv1D(\n",
        "            filters=hp.Int('filters_2', min_value=64, max_value=256, step=32),\n",
        "            kernel_size=hp.Choice('kernel_size_2', values=[3, 5]),\n",
        "            activation='relu'))\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_2', min_value=0.0, max_value=0.5, default=0.25, step=0.05)))\n",
        "\n",
        "        model.add(GlobalAveragePooling1D())\n",
        "\n",
        "        model.add(Dense(\n",
        "            units=hp.Int('units', min_value=50, max_value=150, step=50),\n",
        "            activation='relu'))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_3', min_value=0.0, max_value=0.5, default=0.25, step=0.05)))\n",
        "        model.add(Dense(5, activation='softmax'))  # Assuming 5 classes\n",
        "\n",
        "        model.compile(optimizer=Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        return model\n"
      ],
      "metadata": {
        "id": "GICFqu-CFd4R"
      },
      "id": "GICFqu-CFd4R",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras_tuner import HyperModel, Hyperband\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# Assuming fazeli_mitbih_train_df is your DataFrame and the last column is the label\n",
        "X = fazeli_mitbih_train_df.iloc[:, :-1].values\n",
        "y = fazeli_mitbih_train_df.iloc[:, -1].values\n",
        "\n",
        "# Reshape X to fit the Conv1D input requirements and convert y to categorical\n",
        "X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "y_categorical = to_categorical(y)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define callbacks (assuming they're already defined in your script)\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001, verbose=1)\n",
        "\n",
        "class CNNHyperModel(HyperModel):\n",
        "    def __init__(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "        model.add(Conv1D(\n",
        "            filters=hp.Int('filters_1', min_value=32, max_value=128, step=32),\n",
        "            kernel_size=hp.Choice('kernel_size_1', values=[3, 5]),\n",
        "            activation='relu', input_shape=self.input_shape))\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_1', min_value=0.0, max_value=0.5, default=0.25, step=0.05)))\n",
        "\n",
        "        model.add(Conv1D(\n",
        "            filters=hp.Int('filters_2', min_value=64, max_value=256, step=32),\n",
        "            kernel_size=hp.Choice('kernel_size_2', values=[3, 5]),\n",
        "            activation='relu'))\n",
        "        model.add(MaxPooling1D(pool_size=2))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_2', min_value=0.0, max_value=0.5, default=0.25, step=0.05)))\n",
        "\n",
        "        model.add(GlobalAveragePooling1D())\n",
        "\n",
        "        model.add(Dense(\n",
        "            units=hp.Int('units', min_value=50, max_value=150, step=50),\n",
        "            activation='relu'))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_3', min_value=0.0, max_value=0.5, default=0.25, step=0.05)))\n",
        "        model.add(Dense(5, activation='softmax'))  # Assuming 5 classes\n",
        "\n",
        "        model.compile(optimizer=Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "# Instantiate and configure the hypermodel\n",
        "hypermodel = CNNHyperModel(input_shape=(187, 1))\n",
        "\n",
        "# Instantiate the tuner\n",
        "tuner = Hyperband(\n",
        "    hypermodel,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=10,\n",
        "    directory='hyperband',\n",
        "    project_name='mitbih_classification'\n",
        ")\n",
        "\n",
        "# Start the search for the best hyperparameter configuration\n",
        "tuner.search(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=10, callbacks=[early_stopping, reduce_lr, model_checkpoint])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MchXCoPRra83",
        "outputId": "76846892-900a-404c-90ac-307567f069d2"
      },
      "id": "MchXCoPRra83",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 01m 38s]\n",
            "val_accuracy: 0.9037176370620728\n",
            "\n",
            "Best val_accuracy So Far: 0.9696761965751648\n",
            "Total elapsed time: 00h 20m 10s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3M_PhHJr2QAa"
      },
      "id": "3M_PhHJr2QAa"
    },
    {
      "cell_type": "code",
      "source": [
        "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
        "\n",
        "# Print each hyperparameter and its value\n",
        "print(\"Best Hyperparameters:\")\n",
        "for hp in best_hyperparameters.space:\n",
        "    print(f\"{hp.name}: {best_hyperparameters.get(hp.name)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jIRMntY8rsl5",
        "outputId": "56ced88e-9c3e-4ee7-b420-4461e7ab7548"
      },
      "id": "jIRMntY8rsl5",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "filters_1: 128\n",
            "kernel_size_1: 5\n",
            "dropout_1: 0.1\n",
            "filters_2: 128\n",
            "kernel_size_2: 5\n",
            "dropout_2: 0.05\n",
            "units: 150\n",
            "dropout_3: 0.30000000000000004\n",
            "learning_rate: 0.0018203143877091666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_with_best_hyperparameters():\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(\n",
        "        filters=best_hyperparameters.get('filters_1'),\n",
        "        kernel_size=best_hyperparameters.get('kernel_size_1'),\n",
        "        activation='relu', input_shape=(187, 1)))  # Adjust input_shape if necessary\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Dropout(rate=best_hyperparameters.get('dropout_1')))\n",
        "\n",
        "    model.add(Conv1D(\n",
        "        filters=best_hyperparameters.get('filters_2'),\n",
        "        kernel_size=best_hyperparameters.get('kernel_size_2'),\n",
        "        activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Dropout(rate=best_hyperparameters.get('dropout_2')))\n",
        "\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "\n",
        "    model.add(Dense(\n",
        "        units=best_hyperparameters.get('units'),\n",
        "        activation='relu'))\n",
        "    model.add(Dropout(rate=best_hyperparameters.get('dropout_3')))\n",
        "    model.add(Dense(5, activation='softmax'))  # Adjust the number of classes if necessary\n",
        "\n",
        "    model.compile(optimizer=Adam(best_hyperparameters.get('learning_rate')),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create model with the best hyperparameters\n",
        "model = create_model_with_best_hyperparameters()\n"
      ],
      "metadata": {
        "id": "G86dNDPC2RcG"
      },
      "id": "G86dNDPC2RcG",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_hyperparameters = {\n",
        "    'filters_1': 128,\n",
        "    'kernel_size_1': 5,\n",
        "    'dropout_1': 0.1,\n",
        "    'filters_2': 128,\n",
        "    'kernel_size_2': 5,\n",
        "    'dropout_2': 0.05,\n",
        "    'units': 150,\n",
        "    'dropout_3': 0.3,\n",
        "    'learning_rate': 0.0018203143877091666\n",
        "}\n"
      ],
      "metadata": {
        "id": "6AV9pS47QxGR"
      },
      "id": "6AV9pS47QxGR",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_model_with_best_hyperparameters(hp):\n",
        "    model = Sequential([\n",
        "        Conv1D(\n",
        "            filters=hp['filters_1'],\n",
        "            kernel_size=hp['kernel_size_1'],\n",
        "            activation='relu', input_shape=(187, 1)),  # Adjust input_shape if necessary\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Dropout(rate=hp['dropout_1']),\n",
        "\n",
        "        Conv1D(\n",
        "            filters=hp['filters_2'],\n",
        "            kernel_size=hp['kernel_size_2'],\n",
        "            activation='relu'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Dropout(rate=hp['dropout_2']),\n",
        "\n",
        "        GlobalAveragePooling1D(),\n",
        "\n",
        "        Dense(\n",
        "            units=hp['units'],\n",
        "            activation='relu'),\n",
        "        Dropout(rate=hp['dropout_3']),\n",
        "        Dense(5, activation='softmax')  # Adjust the number of classes if necessary\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=hp['learning_rate']),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the model with the best hyperparameters\n",
        "model_with_best_hyperparameters = create_model_with_best_hyperparameters(best_hyperparameters)\n"
      ],
      "metadata": {
        "id": "CsZqW8e-Q1yV"
      },
      "id": "CsZqW8e-Q1yV",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_model_with_best_hyperparameters():\n",
        "    input_shape = (187, 1)  # Assuming this is your input shape based on the dataset\n",
        "    model = Sequential([\n",
        "        Conv1D(filters=128, kernel_size=5, activation='relu', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Dropout(0.1),\n",
        "\n",
        "        Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Dropout(0.05),\n",
        "\n",
        "        GlobalAveragePooling1D(),\n",
        "\n",
        "        Dense(150, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(5, activation='softmax')  # Adjusted for 5 classes\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0018203143877091666), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the model with the best hyperparameters\n",
        "model_with_best_hyperparameters = create_model_with_best_hyperparameters()\n"
      ],
      "metadata": {
        "id": "g1hHWTEtd8gV"
      },
      "id": "g1hHWTEtd8gV",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model  # Assuming you might want to load the best model\n",
        "\n",
        "# Assuming fazeli_mitbih_train_df is your DataFrame and the last column is the label\n",
        "X = fazeli_mitbih_train_df.iloc[:, :-1].values\n",
        "y = fazeli_mitbih_train_df.iloc[:, -1].values\n",
        "\n",
        "# Reshape X to fit the Conv1D input requirements and convert y to categorical\n",
        "X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "y_categorical = to_categorical(y)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the ModelCheckpoint callback\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'best_model_with_hyperparameters.h5',  # Update the filename to reflect it's the model with best hyperparameters\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Define the early stopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Define the ReduceLROnPlateau callback\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=5,\n",
        "    min_lr=0.001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Assuming you have the best hyperparameters already, create the model with them\n",
        "model_with_best_hyperparameters\n",
        "\n",
        "# Train the model with the callbacks\n",
        "history_with_best_hyperparameters = model_with_best_hyperparameters.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=100,  # Adjust the number of epochs if necessary\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, reduce_lr, model_checkpoint]\n",
        ")\n",
        "\n",
        "# Optionally, load the best model saved during training\n",
        "# best_model = load_model('best_model_with_hyperparameters.h5')\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predictions\n",
        "y_pred = model_with_best_hyperparameters.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Evaluation on Test Data\n",
        "test_loss, test_accuracy = model_with_best_hyperparameters.evaluate(X_test, y_test, verbose=2)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[0, 1, 2, 3, 4], yticklabels=[0, 1, 2, 3, 4])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.ylabel(\"Actual Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3928
        },
        "id": "MEcMg1tz4FE_",
        "outputId": "f29fdbd4-03af-4dcc-9f05-8ba354d5fbcb"
      },
      "id": "MEcMg1tz4FE_",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2182/2189 [============================>.] - ETA: 0s - loss: 0.2317 - accuracy: 0.9336\n",
            "Epoch 1: val_loss improved from inf to 0.20866, saving model to best_model_with_hyperparameters.h5\n",
            "2189/2189 [==============================] - 17s 6ms/step - loss: 0.2314 - accuracy: 0.9337 - val_loss: 0.2087 - val_accuracy: 0.9398 - lr: 0.0018\n",
            "Epoch 2/100\n",
            "  19/2189 [..............................] - ETA: 12s - loss: 0.1397 - accuracy: 0.9572"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.1496 - accuracy: 0.9575\n",
            "Epoch 2: val_loss improved from 0.20866 to 0.11767, saving model to best_model_with_hyperparameters.h5\n",
            "2189/2189 [==============================] - 14s 6ms/step - loss: 0.1496 - accuracy: 0.9575 - val_loss: 0.1177 - val_accuracy: 0.9663 - lr: 0.0018\n",
            "Epoch 3/100\n",
            "2181/2189 [============================>.] - ETA: 0s - loss: 0.1323 - accuracy: 0.9630\n",
            "Epoch 3: val_loss improved from 0.11767 to 0.11243, saving model to best_model_with_hyperparameters.h5\n",
            "2189/2189 [==============================] - 14s 6ms/step - loss: 0.1323 - accuracy: 0.9630 - val_loss: 0.1124 - val_accuracy: 0.9670 - lr: 0.0018\n",
            "Epoch 4/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1194 - accuracy: 0.9665\n",
            "Epoch 4: val_loss did not improve from 0.11243\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.1194 - accuracy: 0.9665 - val_loss: 0.1299 - val_accuracy: 0.9619 - lr: 0.0018\n",
            "Epoch 5/100\n",
            "2185/2189 [============================>.] - ETA: 0s - loss: 0.1143 - accuracy: 0.9681\n",
            "Epoch 5: val_loss improved from 0.11243 to 0.09381, saving model to best_model_with_hyperparameters.h5\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.1144 - accuracy: 0.9680 - val_loss: 0.0938 - val_accuracy: 0.9739 - lr: 0.0018\n",
            "Epoch 6/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.1074 - accuracy: 0.9700\n",
            "Epoch 6: val_loss improved from 0.09381 to 0.08900, saving model to best_model_with_hyperparameters.h5\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.1074 - accuracy: 0.9699 - val_loss: 0.0890 - val_accuracy: 0.9756 - lr: 0.0018\n",
            "Epoch 7/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.1023 - accuracy: 0.9714\n",
            "Epoch 7: val_loss did not improve from 0.08900\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.1023 - accuracy: 0.9714 - val_loss: 0.1340 - val_accuracy: 0.9629 - lr: 0.0018\n",
            "Epoch 8/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0979 - accuracy: 0.9727\n",
            "Epoch 8: val_loss did not improve from 0.08900\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0979 - accuracy: 0.9727 - val_loss: 0.0988 - val_accuracy: 0.9720 - lr: 0.0018\n",
            "Epoch 9/100\n",
            "2185/2189 [============================>.] - ETA: 0s - loss: 0.0964 - accuracy: 0.9723\n",
            "Epoch 9: val_loss did not improve from 0.08900\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0967 - accuracy: 0.9722 - val_loss: 0.0895 - val_accuracy: 0.9765 - lr: 0.0018\n",
            "Epoch 10/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0943 - accuracy: 0.9731\n",
            "Epoch 10: val_loss improved from 0.08900 to 0.08069, saving model to best_model_with_hyperparameters.h5\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0942 - accuracy: 0.9731 - val_loss: 0.0807 - val_accuracy: 0.9773 - lr: 0.0018\n",
            "Epoch 11/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0895 - accuracy: 0.9745\n",
            "Epoch 11: val_loss did not improve from 0.08069\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0895 - accuracy: 0.9745 - val_loss: 0.0893 - val_accuracy: 0.9741 - lr: 0.0018\n",
            "Epoch 12/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.0889 - accuracy: 0.9748\n",
            "Epoch 12: val_loss did not improve from 0.08069\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0889 - accuracy: 0.9748 - val_loss: 0.1079 - val_accuracy: 0.9744 - lr: 0.0018\n",
            "Epoch 13/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0856 - accuracy: 0.9756\n",
            "Epoch 13: val_loss improved from 0.08069 to 0.07813, saving model to best_model_with_hyperparameters.h5\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0856 - accuracy: 0.9756 - val_loss: 0.0781 - val_accuracy: 0.9781 - lr: 0.0018\n",
            "Epoch 14/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0838 - accuracy: 0.9761\n",
            "Epoch 14: val_loss did not improve from 0.07813\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0838 - accuracy: 0.9761 - val_loss: 0.0815 - val_accuracy: 0.9789 - lr: 0.0018\n",
            "Epoch 15/100\n",
            "2183/2189 [============================>.] - ETA: 0s - loss: 0.0811 - accuracy: 0.9769\n",
            "Epoch 15: val_loss improved from 0.07813 to 0.07318, saving model to best_model_with_hyperparameters.h5\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0812 - accuracy: 0.9769 - val_loss: 0.0732 - val_accuracy: 0.9804 - lr: 0.0018\n",
            "Epoch 16/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0815 - accuracy: 0.9767\n",
            "Epoch 16: val_loss did not improve from 0.07318\n",
            "2189/2189 [==============================] - 14s 6ms/step - loss: 0.0815 - accuracy: 0.9767 - val_loss: 0.0941 - val_accuracy: 0.9755 - lr: 0.0018\n",
            "Epoch 17/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0796 - accuracy: 0.9774\n",
            "Epoch 17: val_loss did not improve from 0.07318\n",
            "2189/2189 [==============================] - 14s 6ms/step - loss: 0.0796 - accuracy: 0.9774 - val_loss: 0.0743 - val_accuracy: 0.9798 - lr: 0.0018\n",
            "Epoch 18/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0791 - accuracy: 0.9778\n",
            "Epoch 18: val_loss did not improve from 0.07318\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0791 - accuracy: 0.9778 - val_loss: 0.0790 - val_accuracy: 0.9775 - lr: 0.0018\n",
            "Epoch 19/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.0786 - accuracy: 0.9769\n",
            "Epoch 19: val_loss did not improve from 0.07318\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0785 - accuracy: 0.9769 - val_loss: 0.0894 - val_accuracy: 0.9797 - lr: 0.0018\n",
            "Epoch 20/100\n",
            "2185/2189 [============================>.] - ETA: 0s - loss: 0.0780 - accuracy: 0.9781\n",
            "Epoch 20: val_loss improved from 0.07318 to 0.07229, saving model to best_model_with_hyperparameters.h5\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0779 - accuracy: 0.9781 - val_loss: 0.0723 - val_accuracy: 0.9805 - lr: 0.0018\n",
            "Epoch 21/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0762 - accuracy: 0.9791\n",
            "Epoch 21: val_loss improved from 0.07229 to 0.07009, saving model to best_model_with_hyperparameters.h5\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0762 - accuracy: 0.9791 - val_loss: 0.0701 - val_accuracy: 0.9796 - lr: 0.0018\n",
            "Epoch 22/100\n",
            "2180/2189 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9785\n",
            "Epoch 22: val_loss did not improve from 0.07009\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0738 - accuracy: 0.9785 - val_loss: 0.0805 - val_accuracy: 0.9784 - lr: 0.0018\n",
            "Epoch 23/100\n",
            "2184/2189 [============================>.] - ETA: 0s - loss: 0.0742 - accuracy: 0.9784\n",
            "Epoch 23: val_loss did not improve from 0.07009\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0742 - accuracy: 0.9784 - val_loss: 0.0968 - val_accuracy: 0.9780 - lr: 0.0018\n",
            "Epoch 24/100\n",
            "2184/2189 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9791\n",
            "Epoch 24: val_loss did not improve from 0.07009\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0731 - accuracy: 0.9791 - val_loss: 0.1121 - val_accuracy: 0.9708 - lr: 0.0018\n",
            "Epoch 25/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9796\n",
            "Epoch 25: val_loss did not improve from 0.07009\n",
            "2189/2189 [==============================] - 14s 6ms/step - loss: 0.0725 - accuracy: 0.9796 - val_loss: 0.0764 - val_accuracy: 0.9805 - lr: 0.0018\n",
            "Epoch 26/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9795\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "\n",
            "Epoch 26: val_loss did not improve from 0.07009\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0732 - accuracy: 0.9795 - val_loss: 0.0802 - val_accuracy: 0.9792 - lr: 0.0018\n",
            "Epoch 27/100\n",
            "2183/2189 [============================>.] - ETA: 0s - loss: 0.0640 - accuracy: 0.9817\n",
            "Epoch 27: val_loss improved from 0.07009 to 0.06287, saving model to best_model_with_hyperparameters.h5\n",
            "2189/2189 [==============================] - 12s 6ms/step - loss: 0.0639 - accuracy: 0.9817 - val_loss: 0.0629 - val_accuracy: 0.9833 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "2180/2189 [============================>.] - ETA: 0s - loss: 0.0610 - accuracy: 0.9826\n",
            "Epoch 28: val_loss did not improve from 0.06287\n",
            "2189/2189 [==============================] - 12s 6ms/step - loss: 0.0611 - accuracy: 0.9825 - val_loss: 0.0746 - val_accuracy: 0.9803 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9829\n",
            "Epoch 29: val_loss did not improve from 0.06287\n",
            "2189/2189 [==============================] - 12s 6ms/step - loss: 0.0593 - accuracy: 0.9829 - val_loss: 0.0641 - val_accuracy: 0.9829 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0589 - accuracy: 0.9825\n",
            "Epoch 30: val_loss did not improve from 0.06287\n",
            "2189/2189 [==============================] - 12s 6ms/step - loss: 0.0589 - accuracy: 0.9826 - val_loss: 0.0664 - val_accuracy: 0.9839 - lr: 0.0010\n",
            "Epoch 31/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0571 - accuracy: 0.9831\n",
            "Epoch 31: val_loss did not improve from 0.06287\n",
            "2189/2189 [==============================] - 12s 6ms/step - loss: 0.0571 - accuracy: 0.9831 - val_loss: 0.0664 - val_accuracy: 0.9837 - lr: 0.0010\n",
            "Epoch 32/100\n",
            "2181/2189 [============================>.] - ETA: 0s - loss: 0.0581 - accuracy: 0.9830\n",
            "Epoch 32: val_loss did not improve from 0.06287\n",
            "2189/2189 [==============================] - 12s 5ms/step - loss: 0.0582 - accuracy: 0.9830 - val_loss: 0.0923 - val_accuracy: 0.9775 - lr: 0.0010\n",
            "Epoch 33/100\n",
            "2184/2189 [============================>.] - ETA: 0s - loss: 0.0560 - accuracy: 0.9837\n",
            "Epoch 33: val_loss did not improve from 0.06287\n",
            "2189/2189 [==============================] - 12s 6ms/step - loss: 0.0560 - accuracy: 0.9837 - val_loss: 0.0701 - val_accuracy: 0.9822 - lr: 0.0010\n",
            "Epoch 34/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0564 - accuracy: 0.9831\n",
            "Epoch 34: val_loss improved from 0.06287 to 0.06131, saving model to best_model_with_hyperparameters.h5\n",
            "2189/2189 [==============================] - 12s 6ms/step - loss: 0.0564 - accuracy: 0.9831 - val_loss: 0.0613 - val_accuracy: 0.9841 - lr: 0.0010\n",
            "Epoch 35/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0555 - accuracy: 0.9835\n",
            "Epoch 35: val_loss did not improve from 0.06131\n",
            "2189/2189 [==============================] - 12s 6ms/step - loss: 0.0555 - accuracy: 0.9835 - val_loss: 0.0629 - val_accuracy: 0.9841 - lr: 0.0010\n",
            "Epoch 36/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.0558 - accuracy: 0.9833\n",
            "Epoch 36: val_loss did not improve from 0.06131\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0558 - accuracy: 0.9833 - val_loss: 0.0707 - val_accuracy: 0.9819 - lr: 0.0010\n",
            "Epoch 37/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0571 - accuracy: 0.9833\n",
            "Epoch 37: val_loss did not improve from 0.06131\n",
            "2189/2189 [==============================] - 12s 6ms/step - loss: 0.0570 - accuracy: 0.9833 - val_loss: 0.0640 - val_accuracy: 0.9837 - lr: 0.0010\n",
            "Epoch 38/100\n",
            "2183/2189 [============================>.] - ETA: 0s - loss: 0.0560 - accuracy: 0.9832\n",
            "Epoch 38: val_loss did not improve from 0.06131\n",
            "2189/2189 [==============================] - 12s 6ms/step - loss: 0.0562 - accuracy: 0.9832 - val_loss: 0.0684 - val_accuracy: 0.9828 - lr: 0.0010\n",
            "Epoch 39/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0557 - accuracy: 0.9839\n",
            "Epoch 39: val_loss did not improve from 0.06131\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0557 - accuracy: 0.9839 - val_loss: 0.0699 - val_accuracy: 0.9830 - lr: 0.0010\n",
            "Epoch 40/100\n",
            "2184/2189 [============================>.] - ETA: 0s - loss: 0.0533 - accuracy: 0.9844\n",
            "Epoch 40: val_loss did not improve from 0.06131\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0533 - accuracy: 0.9844 - val_loss: 0.0654 - val_accuracy: 0.9836 - lr: 0.0010\n",
            "Epoch 41/100\n",
            "2180/2189 [============================>.] - ETA: 0s - loss: 0.0535 - accuracy: 0.9843\n",
            "Epoch 41: val_loss did not improve from 0.06131\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0534 - accuracy: 0.9843 - val_loss: 0.0694 - val_accuracy: 0.9824 - lr: 0.0010\n",
            "Epoch 42/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.0529 - accuracy: 0.9841\n",
            "Epoch 42: val_loss did not improve from 0.06131\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0529 - accuracy: 0.9841 - val_loss: 0.0622 - val_accuracy: 0.9840 - lr: 0.0010\n",
            "Epoch 43/100\n",
            "2185/2189 [============================>.] - ETA: 0s - loss: 0.0523 - accuracy: 0.9846\n",
            "Epoch 43: val_loss did not improve from 0.06131\n",
            "2189/2189 [==============================] - 13s 6ms/step - loss: 0.0522 - accuracy: 0.9846 - val_loss: 0.0654 - val_accuracy: 0.9833 - lr: 0.0010\n",
            "Epoch 44/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9840Restoring model weights from the end of the best epoch: 34.\n",
            "\n",
            "Epoch 44: val_loss did not improve from 0.06131\n",
            "2189/2189 [==============================] - 14s 6ms/step - loss: 0.0538 - accuracy: 0.9840 - val_loss: 0.0671 - val_accuracy: 0.9842 - lr: 0.0010\n",
            "Epoch 44: early stopping\n",
            "548/548 [==============================] - 1s 2ms/step\n",
            "548/548 - 1s - loss: 0.0613 - accuracy: 0.9841 - 1s/epoch - 2ms/step\n",
            "Test Loss: 0.06131098046898842\n",
            "Test Accuracy: 0.9840671420097351\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99     14579\n",
            "           1       0.94      0.69      0.79       426\n",
            "           2       0.97      0.94      0.96      1112\n",
            "           3       0.90      0.79      0.84       145\n",
            "           4       0.99      0.99      0.99      1249\n",
            "\n",
            "    accuracy                           0.98     17511\n",
            "   macro avg       0.96      0.88      0.91     17511\n",
            "weighted avg       0.98      0.98      0.98     17511\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyYAAAK9CAYAAADR4XgGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB0QElEQVR4nO3dd3RU1dfG8WcCJKEloYdIVTD03kJHIihIERQRpEkRTJAuoFIFAkiXJoqCCIKoIEXBCEJUWghGioANpCb0FkJImfcPXuY3YwIkkOSkfD+suxZz7pl7951hSPbsc+6xWK1WqwAAAADAICfTAQAAAAAAiQkAAAAA40hMAAAAABhHYgIAAADAOBITAAAAAMaRmAAAAAAwjsQEAAAAgHEkJgAAAACMIzEBAAAAYByJCQAk4M8//1SzZs3k7u4ui8WitWvXJuvxjx8/LovFoiVLliTrcdOzxo0bq3HjxqbDAAAYQmICIM36+++/9dprr+nxxx+Xq6ur3NzcVK9ePc2ePVuRkZEpeu5u3brpwIEDmjhxopYtW6YaNWqk6PlSU/fu3WWxWOTm5pbg6/jnn3/KYrHIYrFo2rRpST7+mTNnNHbsWIWGhiZDtACAzCKr6QAAICEbN27Uiy++KBcXF3Xt2lUVKlTQ7du39fPPP2vYsGE6dOiQFi1alCLnjoyM1M6dO/X222/L398/Rc5RvHhxRUZGKlu2bCly/AfJmjWrbt68qfXr16tDhw4O+5YvXy5XV1fdunXroY595swZjRs3TiVKlFCVKlUS/bzvv//+oc4HAMgYSEwApDnHjh1Tx44dVbx4cW3dulWFCxe27fPz89Nff/2ljRs3ptj5z58/L0ny8PBIsXNYLBa5urqm2PEfxMXFRfXq1dPnn38eLzFZsWKFWrZsqa+++ipVYrl586Zy5MghZ2fnVDkfACBtYigXgDRn6tSpunHjhhYvXuyQlNxVqlQpDRgwwPY4JiZG7777rp544gm5uLioRIkSeuuttxQVFeXwvBIlSui5557Tzz//rFq1asnV1VWPP/64Pv30U1ufsWPHqnjx4pKkYcOGyWKxqESJEpLuDIG6+3d7Y8eOlcVicWgLDAxU/fr15eHhoVy5csnb21tvvfWWbf+95phs3bpVDRo0UM6cOeXh4aE2bdro8OHDCZ7vr7/+Uvfu3eXh4SF3d3f16NFDN2/evPcL+x+dOnXSd999pytXrtjagoOD9eeff6pTp07x+l+6dElDhw5VxYoVlStXLrm5uenZZ5/Vb7/9Zuuzbds21axZU5LUo0cP25Cwu9fZuHFjVahQQSEhIWrYsKFy5Mhhe13+O8ekW7ducnV1jXf9zZs3V548eXTmzJlEXysAIO0jMQGQ5qxfv16PP/646tatm6j+vXr10ujRo1WtWjXNnDlTjRo1UkBAgDp27Biv719//aUXXnhBTz/9tKZPn648efKoe/fuOnTokCSpXbt2mjlzpiTp5Zdf1rJlyzRr1qwkxX/o0CE999xzioqK0vjx4zV9+nS1bt1av/zyy32f98MPP6h58+Y6d+6cxo4dq8GDB2vHjh2qV6+ejh8/Hq9/hw4ddP36dQUEBKhDhw5asmSJxo0bl+g427VrJ4vFoq+//trWtmLFCpUpU0bVqlWL1/+ff/7R2rVr9dxzz2nGjBkaNmyYDhw4oEaNGtmShLJly2r8+PGSpD59+mjZsmVatmyZGjZsaDvOxYsX9eyzz6pKlSqaNWuWmjRpkmB8s2fPVoECBdStWzfFxsZKkj744AN9//33ev/99+Xl5ZXoawUApANWAEhDrl69apVkbdOmTaL6h4aGWiVZe/Xq5dA+dOhQqyTr1q1bbW3Fixe3SrIGBQXZ2s6dO2d1cXGxDhkyxNZ27NgxqyTre++953DMbt26WYsXLx4vhjFjxljt/zudOXOmVZL1/Pnz94z77jk++eQTW1uVKlWsBQsWtF68eNHW9ttvv1mdnJysXbt2jXe+V1991eGYzz//vDVfvnz3PKf9deTMmdNqtVqtL7zwgrVp06ZWq9VqjY2NtXp6elrHjRuX4Gtw69Yta2xsbLzrcHFxsY4fP97WFhwcHO/a7mrUqJFVknXhwoUJ7mvUqJFD2+bNm62SrBMmTLD+888/1ly5clnbtm37wGsEAKQ/VEwApCnXrl2TJOXOnTtR/b/99ltJ0uDBgx3ahwwZIknx5qKUK1dODRo0sD0uUKCAvL299c8//zx0zP91d27KN998o7i4uEQ95+zZswoNDVX37t2VN29eW3ulSpX09NNP267TXt++fR0eN2jQQBcvXrS9honRqVMnbdu2TWFhYdq6davCwsISHMYl3ZmX4uR058dGbGysLl68aBumtm/fvkSf08XFRT169EhU32bNmum1117T+PHj1a5dO7m6uuqDDz5I9LkAAOkHiQmANMXNzU2SdP369UT1//fff+Xk5KRSpUo5tHt6esrDw0P//vuvQ3uxYsXiHSNPnjy6fPnyQ0Yc30svvaR69eqpV69eKlSokDp27KgvvvjivknK3Ti9vb3j7StbtqwuXLigiIgIh/b/XkuePHkkKUnX0qJFC+XOnVurVq3S8uXLVbNmzXiv5V1xcXGaOXOmSpcuLRcXF+XPn18FChTQ/v37dfXq1USf87HHHkvSRPdp06Ypb968Cg0N1Zw5c1SwYMFEPxcAkH6QmABIU9zc3OTl5aWDBw8m6Xn/nXx+L1myZEmw3Wq1PvQ57s5/uCt79uwKCgrSDz/8oC5dumj//v166aWX9PTTT8fr+yge5VrucnFxUbt27bR06VKtWbPmntUSSZo0aZIGDx6shg0b6rPPPtPmzZsVGBio8uXLJ7oyJN15fZLi119/1blz5yRJBw4cSNJzAQDpB4kJgDTnueee099//62dO3c+sG/x4sUVFxenP//806E9PDxcV65csd1hKznkyZPH4Q5Wd/23KiNJTk5Oatq0qWbMmKHff/9dEydO1NatW/Xjjz8meOy7cR49ejTeviNHjih//vzKmTPno13APXTq1Em//vqrrl+/nuANA+768ssv1aRJEy1evFgdO3ZUs2bN5OvrG+81SWySmBgRERHq0aOHypUrpz59+mjq1KkKDg5OtuMDANIOEhMAac6bb76pnDlzqlevXgoPD4+3/++//9bs2bMl3RmKJCnenbNmzJghSWrZsmWyxfXEE0/o6tWr2r9/v63t7NmzWrNmjUO/S5cuxXvu3YUG/3sL47sKFy6sKlWqaOnSpQ6/6B88eFDff/+97TpTQpMmTfTuu+9q7ty58vT0vGe/LFmyxKvGrF69WqdPn3Zou5tAJZTEJdXw4cN14sQJLV26VDNmzFCJEiXUrVu3e76OAID0iwUWAaQ5TzzxhFasWKGXXnpJZcuWdVj5fceOHVq9erW6d+8uSapcubK6deumRYsW6cqVK2rUqJH27NmjpUuXqm3btve8Fe3D6Nixo4YPH67nn39eb7zxhm7evKkFCxboySefdJj8PX78eAUFBally5YqXry4zp07p/nz56tIkSKqX7/+PY//3nvv6dlnn5WPj4969uypyMhIvf/++3J3d9fYsWOT7Tr+y8nJSe+8884D+z333HMaP368evToobp16+rAgQNavny5Hn/8cYd+TzzxhDw8PLRw4ULlzp1bOXPmVO3atVWyZMkkxbV161bNnz9fY8aMsd2++JNPPlHjxo01atQoTZ06NUnHAwCkbVRMAKRJrVu31v79+/XCCy/om2++kZ+fn0aMGKHjx49r+vTpmjNnjq3vRx99pHHjxik4OFgDBw7U1q1bNXLkSK1cuTJZY8qXL5/WrFmjHDly6M0339TSpUsVEBCgVq1axYu9WLFi+vjjj+Xn56d58+apYcOG2rp1q9zd3e95fF9fX23atEn58uXT6NGjNW3aNNWpU0e//PJLkn+pTwlvvfWWhgwZos2bN2vAgAHat2+fNm7cqKJFizr0y5Ytm5YuXaosWbKob9++evnll7V9+/Yknev69et69dVXVbVqVb399tu29gYNGmjAgAGaPn26du3alSzXBQBIGyzWpMySBAAAAIAUQMUEAAAAgHEkJgAAAACMIzEBAAAAYByJCQAAAADjSEwAAAAAGEdiAgAAAMA4EhMAAAAAxmXIld+zV/U3HQJS0eXguaZDAAAASeSahn8LNfm7ZOSvmff3GiomAAAAAIxLw7kqAAAAYICF7+5N4FUHAAAAYByJCQAAAADjGMoFAAAA2LNYTEeQKVExAQAAAGAcFRMAAADAHpPfjeBVBwAAAGAcFRMAAADAHnNMjKBiAgAAAMA4EhMAAAAAxjGUCwAAALDH5HcjeNUBAAAAGEfFBAAAALDH5HcjqJgAAAAAMI7EBAAAAIBxDOUCAAAA7DH53QhedQAAAADGkZgAAAAA9iwWc1sSBAUFqVWrVvLy8pLFYtHatWvv2bdv376yWCyaNWuWQ/ulS5fUuXNnubm5ycPDQz179tSNGzcc+uzfv18NGjSQq6urihYtqqlTp8Y7/urVq1WmTBm5urqqYsWK+vbbb5N0LRKJCQAAAJAuRUREqHLlypo3b959+61Zs0a7du2Sl5dXvH2dO3fWoUOHFBgYqA0bNigoKEh9+vSx7b927ZqaNWum4sWLKyQkRO+9957Gjh2rRYsW2frs2LFDL7/8snr27Klff/1Vbdu2Vdu2bXXw4MEkXY/FarVak/SMdCB7VX/TISAVXQ6eazoEAACQRK5peKZz9rpvGTv3lR/HKCoqyqHNxcVFLi4u932exWLRmjVr1LZtW4f206dPq3bt2tq8ebNatmypgQMHauDAgZKkw4cPq1y5cgoODlaNGjUkSZs2bVKLFi106tQpeXl5acGCBXr77bcVFhYmZ2dnSdKIESO0du1aHTlyRJL00ksvKSIiQhs2bLCdt06dOqpSpYoWLlyY6GunYgIAAACkEQEBAXJ3d3fYAgICHupYcXFx6tKli4YNG6by5cvH279z5055eHjYkhJJ8vX1lZOTk3bv3m3r07BhQ1tSIknNmzfX0aNHdfnyZVsfX19fh2M3b95cO3fuTFK8aThXBQAAADKXkSNHavDgwQ5tD6qW3MuUKVOUNWtWvfHGGwnuDwsLU8GCBR3asmbNqrx58yosLMzWp2TJkg59ChUqZNuXJ08ehYWF2drs+9w9RmKRmAAAAAD2DK78nphhW4kREhKi2bNna9++fbKkk5XsGcoFAAAAZDA//fSTzp07p2LFiilr1qzKmjWr/v33Xw0ZMkQlSpSQJHl6eurcuXMOz4uJidGlS5fk6elp6xMeHu7Q5+7jB/W5uz+xSEwAAAAAexYnc1sy6dKli/bv36/Q0FDb5uXlpWHDhmnz5s2SJB8fH125ckUhISG2523dulVxcXGqXbu2rU9QUJCio6NtfQIDA+Xt7a08efLY+mzZssXh/IGBgfLx8UlSzAzlAgAAANKhGzdu6K+//rI9PnbsmEJDQ5U3b14VK1ZM+fLlc+ifLVs2eXp6ytvbW5JUtmxZPfPMM+rdu7cWLlyo6Oho+fv7q2PHjrZbC3fq1Enjxo1Tz549NXz4cB08eFCzZ8/WzJkzbccdMGCAGjVqpOnTp6tly5ZauXKl9u7d63BL4cSgYgIAAACkQ3v37lXVqlVVtWpVSdLgwYNVtWpVjR49OtHHWL58ucqUKaOmTZuqRYsWql+/vkNC4e7uru+//17Hjh1T9erVNWTIEI0ePdphrZO6detqxYoVWrRokSpXrqwvv/xSa9euVYUKFZJ0PaxjgnSPdUwAAEh/0vQ6Jg0S/4t9cov8abyxc5tGxQQAAACAcWk4VwUAAAAMSMZJ6Eg8XnUAAAAAxlExAQAAAOxRMTGCVx0AAACAcSQmAAAAAIxjKBcAAABgz8liOoJMiYoJAAAAAOOomAAAAAD2mPxuBK86AAAAAONITAAAAAAYx1AuAAAAwJ6Fye8mUDEBAAAAYBwVEwAAAMAek9+N4FUHAAAAYBwVEwAAAMAec0yMoGICAAAAwDgSEwAAAADGMZQLAAAAsMfkdyN41QEAAAAYR8UEAAAAsMfkdyOomAAAAAAwjsQEAAAAgHEM5QIAAADsMfndCF51AAAAAMZRMQEAAADsMfndCComBtWr9oS+nPWa/vl+oiJ/natWjSvds++ctzsq8te58u/U2KH9yMZxivx1rsM2tMfTtv2lixfUpkVv6PgPk3R510z9vn6sxrz+nLJm/d9bv/nDAfGOEfnrXH09p2+yXzOSJmRvsPq/3le+jeurcnlvbd3yg8P+UW+NUOXy3g5bvz49DUWLR/Wg99veu+NGq3J5b3326ZLUCxApavGHH6hTh/byqVlVjRv4aGD/13X82D+mw0IyedDn+4fA7/Va71fVsG5tVS7vrSOHDxuKFDCHiolBObO76MAfp/XpNzu1akafe/Zr3aSSalUsoTPnriS4f9z8Dfrk619sj69HRNn+Hh0Tq+Ub9ij0yEldvX5TFZ8sonmjXpaTk0Vj5q6XJHUc8qGcs2WxPSeve07tWTVSXwf++ohXiEcVGXlT3t7eatuuvQYP8E+wT736DTR+QoDtsbOzc2qFh2SWmPdbkrb8EKgDv/2mAgULpmJ0SGl7g/fopZc7q3zFioqNidX7s2eob++e+nrdRuXIkcN0eHhED/p8R0beVNWq1dS8+bMaN+YdAxHCAXNMjCAxMej7X37X97/8ft8+XgXcNWP4i2r1+jyteb9fgn1uRNxS+MXrCe47fvqijp++aHt84uxlNaxRWvWqPmFru3ztpsNzXmxeXTdv3SYxSQPqN2ik+g0a3bePs7Oz8hcokEoRISUl5v0ODw/X5EnvasGixerf77VUigypYcGixQ6Px0+crCYNfHT490OqXqOmoaiQXB70+W7Vuq0k6fTpU6kUEZD2GE1MLly4oI8//lg7d+5UWFiYJMnT01N169ZV9+7dVSCT/7JlsVi0eEJXzVy6RYf/CbtnvyE9mmlE72d1MuySvvhur+Ys/1GxsXEJ9n28aH49Xbesvtny2z2P161tXa3evE83b91+5GtAytsbvEeNG/jIzc1NtWrXkf8bA+Xhkcd0WEgBcXFxenvEMHXv0VOlSpU2HQ5S2I3rd75wcnN3NxwJAKQOY4lJcHCwmjdvrhw5csjX11dPPvmkpDvfBs6ZM0eTJ0/W5s2bVaNGjfseJyoqSlFRUQ5t1rhYWZyy3OMZ6ceQHk8rJjZO8z7fds8+8z/frl8Pn9TlaxGqU/lxje/fWp4F3DV8+tcO/X5cMlhVyhSVq0s2ffTlzxq/YGOCx6tRvrgqlPZSv3HLk/NSkELq1m+gpr5P67EiRXTy5Em9P2uGXn+tt5atWKUsWdL/ZwCOPln8obJkzapOr3Q1HQpSWFxcnKZOmaQqVaupdOknTYcDZD5MfjfCWGLSv39/vfjii1q4cKEs/3nzrVar+vbtq/79+2vnzp33PU5AQIDGjRvn0JalUE1lK1wr2WNOTVXLFpXfy41Vt9OU+/ab89lW298P/nlGt6NjNPftlzVqzjrdjo6x7esy/GPlyumqSk8+pkkD22pQ16aasTT+xNpubX104I/T2nvo3+S7GKSYZ1u0tP299JPeevJJb7V8xld7g/eodh0fg5Ehuf1+6KCWL/tUK7/8Ot7/mch4Jk0Yp7///FNLlq0wHQoApBpjM3t+++03DRo0KMEfsBaLRYMGDVJoaOgDjzNy5EhdvXrVYctaqHoKRJy66lV9QgXz5tIf347X9eDZuh48W8W98mny4HY6snHcPZ8XfOC4smXLouJeeR3aT4Vf0ZF/wvTFphC9M2ed3n6thZycHF/7HK7OerF5dS1de/9kEGlXkaJFlSdPHp04QWKZ0ewL2atLly7qGd8mqlapnKpVKqczZ05r+ntT9OzTT5kOD8lo0oTxCtq+TR9+slSFPD1NhwNkThYnc1smZqxi4unpqT179qhMmTIJ7t+zZ48KFSr0wOO4uLjIxcXFoS0jDONasTFYW3cfdWhbP99PKzbu0aff7Lrn8yp7F1FsbJzOX0p4MrwkOTlZlC1rFjk5WRQXZ7W1t3u6qlycs+rzb4Mf/QJgRHhYmK5cuaIC+TP3/KyM6LnWbVTbp65DW78+PfVcqzZq+3w7Q1EhOVmtVgVMfFdbtwRq8ZJlKlKkqOmQACBVGUtMhg4dqj59+igkJERNmza1JSHh4eHasmWLPvzwQ02bNs1UeKkiZ3ZnPVH0f79Alngsnyo9+ZguX7upk2GXdelqhEP/6JhYhV+4pj//PSdJql2ppGpWKK7te//U9YhbqlOppKYMba/Pvw3WleuRkqSOz9ZQdEysDv51RlG3Y1S9XDG927+1vvw+RDExjhPku7f10fpt++OdF+bcjIjQiRMnbI9PnzqlI4cPy93dXe7u7lq4YK58n26ufPnz69TJk5o5/T0VLVZcdes3MBg1Htb93u/CXl7xbmqQLWs25c+fXyVKPp7aoSIFTHp3nL77doNmvT9fOXPk1IXz5yVJuXLnlqurq+Ho8Kge9Pm+euWKzp49q/Pn7/yMP378mCQpf/783HkRmYaxxMTPz0/58+fXzJkzNX/+fMXGxkqSsmTJourVq2vJkiXq0KGDqfBSRbVyxfX9RwNsj6cObS9JWrZul/qM+eyBz4+6Ha0Xm1fX231byCVbVh0/c1HvL/9Rc5b9b95JTGycBnd/WqWLF5TFYtGJs5e0YFWQ3rebmyLdWYixXrVSatl3bjJdHZLDoUMH1avH/yY6T5t6Z72S1m2e19ujx+qPo39o3Tdrdf3adRUsWFA+devJr/8A1jJJp+73fr87abKpsJBKvlj1uSSpZ/cuDu3jJwSoDVWxdO9Bn+9tP27V6HdG2vYPHzpIktT3dX/18+ufusEi0w+pMsVitVqtD+6WsqKjo3XhwgVJd74ZyJYt2yMdL3vVey9MhozncjDJFAAA6Y1rGl5NL3ur+cbOHbn+dWPnNi1N/JPIli2bChcubDoMAAAAgNsFG0KdCgAAAIBxJCYAAAAAjEsTQ7kAAACANIPJ70bwqgMAAAAwjooJAAAAYI/J70ZQMQEAAABgHBUTAAAAwB5zTIzgVQcAAABgHIkJAAAAAOMYygUAAADYY/K7EVRMAAAAABhHxQQAAACwY6FiYgQVEwAAAADGkZgAAAAAMI6hXAAAAIAdhnKZQcUEAAAAgHFUTAAAAAB7FEyMoGICAAAAwDgqJgAAAIAd5piYQcUEAAAAgHEkJgAAAACMYygXAAAAYIehXGZQMQEAAABgHBUTAAAAwA4VEzOomAAAAAAwjsQEAAAAgHEM5QIAAADsMJTLDComAAAAAIyjYgIAAADYo2BiBBUTAAAAAMZRMQEAAADsMMfEDComAAAAAIwjMQEAAABgHEO5AAAAADsM5TKDigkAAAAA40hMAAAAADsWi8XYlhRBQUFq1aqVvLy8ZLFYtHbtWtu+6OhoDR8+XBUrVlTOnDnl5eWlrl276syZMw7HuHTpkjp37iw3Nzd5eHioZ8+eunHjhkOf/fv3q0GDBnJ1dVXRokU1derUeLGsXr1aZcqUkaurqypWrKhvv/02SdcikZgAAAAA6VJERIQqV66sefPmxdt38+ZN7du3T6NGjdK+ffv09ddf6+jRo2rdurVDv86dO+vQoUMKDAzUhg0bFBQUpD59+tj2X7t2Tc2aNVPx4sUVEhKi9957T2PHjtWiRYtsfXbs2KGXX35ZPXv21K+//qq2bduqbdu2OnjwYJKux2K1Wq1JfA3SvOxV/U2HgFR0OXiu6RAAAEASuabhmc55u6wwdu5Lyzo91PMsFovWrFmjtm3b3rNPcHCwatWqpX///VfFihXT4cOHVa5cOQUHB6tGjRqSpE2bNqlFixY6deqUvLy8tGDBAr399tsKCwuTs7OzJGnEiBFau3atjhw5Ikl66aWXFBERoQ0bNtjOVadOHVWpUkULFy5M9DVQMQEAAADsmBzKFRUVpWvXrjlsUVFRyXJdV69elcVikYeHhyRp586d8vDwsCUlkuTr6ysnJyft3r3b1qdhw4a2pESSmjdvrqNHj+ry5cu2Pr6+vg7nat68uXbu3Jmk+EhMAAAAgDQiICBA7u7uDltAQMAjH/fWrVsaPny4Xn75Zbm5uUmSwsLCVLBgQYd+WbNmVd68eRUWFmbrU6hQIYc+dx8/qM/d/YmVhotoAAAAgAEG7xY8cuRIDR482KHNxcXlkY4ZHR2tDh06yGq1asGCBY90rJREYgIAAACkES4uLo+ciNi7m5T8+++/2rp1q61aIkmenp46d+6cQ/+YmBhdunRJnp6etj7h4eEOfe4+flCfu/sTi6FcAAAAgJ30crvgB7mblPz555/64YcflC9fPof9Pj4+unLlikJCQmxtW7duVVxcnGrXrm3rExQUpOjoaFufwMBAeXt7K0+ePLY+W7ZscTh2YGCgfHx8khQviQkAAACQDt24cUOhoaEKDQ2VJB07dkyhoaE6ceKEoqOj9cILL2jv3r1avny5YmNjFRYWprCwMN2+fVuSVLZsWT3zzDPq3bu39uzZo19++UX+/v7q2LGjvLy8JEmdOnWSs7OzevbsqUOHDmnVqlWaPXu2w3CzAQMGaNOmTZo+fbqOHDmisWPHau/evfL3T9qdcrldMNI9bhcMAED6k5ZvF5y/+0pj576wpGOi+27btk1NmjSJ196tWzeNHTtWJUuWTPB5P/74oxo3bizpzgKL/v7+Wr9+vZycnNS+fXvNmTNHuXLlsvXfv3+//Pz8FBwcrPz586t///4aPny4wzFXr16td955R8ePH1fp0qU1depUtWjRItHXIpGYIAMgMQEAIP1Jy4lJgR6rjJ37/CcvGTu3aQzlAgAAAGBcGs5VAQAAgNSX3JPQkThUTAAAAAAYR2ICAAAAwDiGcgEAAAD2GMllBBUTAAAAAMZRMQEAAADsMPndDComAAAAAIyjYgIAAADYoWJiRoZMTC7ued90CEhFMbFW0yEgFWXNwg8LAAAyIoZyAQAAADAuQ1ZMAAAAgIfFUC4zqJgAAAAAMI6KCQAAAGCHiokZVEwAAAAAGEdiAgAAAMA4hnIBAAAA9hjJZQQVEwAAAADGUTEBAAAA7DD53QwqJgAAAACMo2ICAAAA2KFiYgYVEwAAAADGkZgAAAAAMI6hXAAAAIAdhnKZQcUEAAAAgHFUTAAAAAB7FEyMoGICAAAAwDgSEwAAAADGMZQLAAAAsMPkdzOomAAAAAAwjooJAAAAYIeKiRlUTAAAAAAYR2ICAAAAwDiGcgEAAAB2GMplBhUTAAAAAMZRMQEAAADsUDExg4oJAAAAAOOomAAAAAD2KJgYQcUEAAAAgHEkJgAAAACMYygXAAAAYIfJ72ZQMQEAAABgHBUTAAAAwA4VEzOomAAAAAAwjsQEAAAAgHEM5QIAAADsMJLLDComAAAAAIyjYgIAAADYYfK7GVRMAAAAABhHxQQAAACwQ8HEDComAAAAAIwjMQEAAABgHEO5AAAAADtMfjeDigkAAAAA46iYAAAAAHYomJhBxQQAAACAcSQmAAAAAIxjKBcAAABgx8mJsVwmUDEBAAAAYBwVEwAAAMAOk9/NoGICAAAAwDgqJgAAAIAdFlg0g4oJAAAAAONITAAAAAAYx1AuAAAAwA4jucygYpLGhewN1gC/vnq6SQNVrVBGP275wbYvOjpas2dM04vPt5JPzap6ukkDvTNyuM6dC3c4xr/Hj2lg/9fVpH4d1a9dXT26dFLwnl2pfSl4gI8/+kBdXn5BDepUk2+juho8wE/Hj/3j0OfkyRMaMtBfTRv5qKFPdQ0fOlAXL16w7T9z+pTGj3lbrZ5pqro1K6t1i6e1cN4cRUffTu3LQTIJDw/XyOFD1bBubdWqVknt27bSoYMHTIeFFLRyxXI9+/RTqlm1ojp3fFEH9u83HRJSEO838D8kJmlcZGSknvQuo5Fvj46379atWzr8++/q/drr+vyLrzR91vt3khD/1x36veHXV7Exsfpg8VIt/+IrPeldRm/49dOFC+dT6zKQCPv2BuvFjp205LNVmr/oY8XExMivby9F3rwpSYq8eVN+r/WUxWLRwg+XaPHSFYqOjtag/v0UFxcnSTp+7Jji4uL01uhx+mLNBg0ZNlJfrV6lubNnmrw0PKRrV6+q+ysvK2vWbJq38EN9vW6jhgwbLjc3d9OhIYVs+u5bTZsaoNde99PK1Wvk7V1G/V7rqYsXL5oODSmA9zvtslgsxrbMzGK1Wq2mg0huN6Mz3CVJkqpWKKMZs+eqSVPfe/Y5dOCAXnn5RX0buFWFC3vp8uXLeqqBjxYv/UzVqteQJEVE3FD92jW04MOPVcenbmqFn2L+/3fyDOfypUvybVxXH368TNVq1NTOHT/rjdf76Mef9yhXrlySpOvXr6tJ/Vqa98Fi1a6T8Hv56SeL9eUXn2vddz8kuD+9yZol8/ynPWvGNIX+uk9Llq0wHQpSSeeOL6p8hYp66507X0bFxcWpWdNGerlTF/Xs3cdwdEhumf39dk3DEwoqjTb3M3P/+Hv/npfRUTHJYK7fuC6LxaLcud0kSR4eHipRsqQ2rPtGkTdvKiYmRl99sUp58+ZTuXLlDUeL+7lx47okyc39zrfj0bdvy2KxyNnZ2dbHxcVFTk5OCt0Xct/j3D0G0pftP25V+fIVNHTQG2rcwEcd2rfVV6u/MB0WUkj07ds6/Pshhy+MnJycVKdOXe3/7VeDkSEl8H4D8ZGYZCBRUVGaM3OanmnR0vaN+p1hP5/oyOHfVa92ddWpXlnLPl2ieR98yC+raVhcXJymTZ2kylWrqVTpJyVJFStVkWv27Jozc5oiIyMVefOmZk2fotjY2HsOyzt54l+t/PwztXvhpdQMH8nk1KmT+mLV5ypWvIQWLFqsDi+9rCkBE7Ru7RrToSEFXL5yWbGxscqXL59De758+XThwoV7PAvpFe932sZQLjPSdGJy8uRJvfrqq/ftExUVpWvXrjlsUVFRqRRh2hEdHa03hwyU1Sq9NWqsrd1qtSpg4njlzZdPHy9drmWff6EmT/lqgH8/nT9/zlzAuK/JE8fr77/+VMCUGba2PHnzasq0WQra/qMa1KmmRvVq6vr16ypTtpwslvgf5XPh4fLv11u+Tz+jdi90SM3wkUzi4qwqW6683hg4WGXLltMLHV5Suxc6aPUXK02HBgBAskvTicmlS5e0dOnS+/YJCAiQu7u7wzZtSkAqRZg2REdHa/iQQTp75owWfLjYVi2RpD27d+mn7ds0+b0ZqlKtmsqWK6+3Ro2Ri4ur1n+z1lzQuKcpk8br56Bt+uCjT1XI09Nhn0/d+lr3baACt+3Qlu079e6kqTp/7pyKFCnq0O/8uXC91qurKleuqnfGjE/N8JGMChQooMefeMKh7fHHH9fZs2cMRYSUlMcjj7JkyRJv4vPFixeVP39+Q1EhpfB+p20Wi7ktMzM67WjdunX33f/PP//cd78kjRw5UoMHD3Zoi3VyvkfvjOduUnLixL9a9PFSeXjkcdh/61akJMnJyfFfupOTRdaMOms8nbJarZoa8K5+3PqDFi3+VI8VKXLPvnny3Hmf9+zepUuXLqph4ya2fefC7yQlZcuW15h3J8nJKU1//4D7qFK1mo4fO+bQ9u/x4/LyesxQREhJ2ZydVbZcee3etVNP/f9NTuLi4rR79051fPkVw9EhufF+A/EZTUzatm0ri8Wi+90Y7EFj7VxcXOTi4uLQlpHuynXzZoROnjhhe3z69CkdPXJYbu7uyp+/gIYNHqAjv/+u2fMWKi7uf3MN3N3dlS2bsypVrio3NzeNemuE+vT1k6uri77+crVOnzqt+g0bG7oqJGTyxPHa9N0GzZg9Tzly5rS9l7ly5Zarq6skad3ar1Sy5BPyyJtXB34L1bQpE9WpSzeVKPm4pDtJSZ+eXVW4sJcGDhmuy5cv2Y6fP3+B1L8oPJJXunZTt1de1keLFqpZ82d18MB+ffnlFxo9lipYRtWlWw+Nemu4ypevoAoVK+mzZUsVGRmpts+3Mx0aUgDvd9qV2ed6mGL0dsGPPfaY5s+frzZt2iS4PzQ0VNWrV1dsbGySjpuREpO9e3ar96vd4rW3atNWfV/3V8vmCd9S7sOPl6pGrdqSpEMHD2jenFn6/dBBxcTE6PFSpdSnr5/qN2iYorGnloxS+KleqUyC7WPenaTWbe78kJoza7o2fLNGV69elddjXmr/Ykd17tLd9h/oum++1rhRbyV4nJD9R1Im8FSWmW4XLEnbt/2oObNm6MS/x/VYkSLq0rWH2r/InKGM7PPln2npJ4t14cJ5eZcpq+FvvaNKlSqbDgspJDO/32n5dsFVx201du5fxzyV6L5BQUF67733FBISorNnz2rNmjVq27atbb/VatWYMWP04Ycf6sqVK6pXr54WLFig0qVL2/pcunRJ/fv31/r16+Xk5KT27dtr9uzZDlMD9u/fLz8/PwUHB6tAgQLq37+/3nzzTYdYVq9erVGjRun48eMqXbq0pkyZohYtWiTp2o0mJq1bt1aVKlU0fnzC3/799ttvqlq1qm3xuMTKSIkJHiyjJCZInMyWmABARkVikrCkJCbfffedfvnlF1WvXl3t2rWLl5hMmTJFAQEBWrp0qUqWLKlRo0bpwIED+v33322jMZ599lmdPXtWH3zwgaKjo9WjRw/VrFlTK1bcWUPr2rVrevLJJ+Xr66uRI0fqwIEDevXVVzVr1iz16XNnvZ0dO3aoYcOGCggI0HPPPacVK1ZoypQp2rdvnypUqJDo6zGamPz000+KiIjQM888k+D+iIgI7d27V40aNUrScUlMMhcSk8yFxAQAMoa0nJhUG28uMdk3OvGJiT2LxeKQmFitVnl5eWnIkCEaOnSoJOnq1asqVKiQlixZoo4dO+rw4cMqV66cgoODVaPGnYW4N23apBYtWujUqVPy8vLSggUL9PbbbyssLMy2ltqIESO0du1aHTlyZzTGSy+9pIiICG3YsMEWT506dVSlShUtXLgw0ddgdFZsgwYN7pmUSFLOnDmTnJQAAAAA6VVyLYVx7NgxhYWFydf3f8P+3d3dVbt2be3cuVOStHPnTnl4eNiSEkny9fWVk5OTdu/ebevTsGFDhwWemzdvrqNHj+ry5cu2Pvbnudvn7nkSi9v1AAAAAHZMLrCY0FIYAQFJXwojLCxMklSoUCGH9kKFCtn2hYWFqWDBgg77s2bNqrx58zr0SegY9ue4V5+7+xMrDRfRAAAAgMwloaUw/nsH2oyKxAQAAABIIxJaCuNheP7/Is3h4eEqXLiwrT08PFxVqlSx9Tl37pzD82JiYnTp0iXb8z09PRUeHu7Q5+7jB/Xx/M9C0Q/CUC4AAADATkZY+b1kyZLy9PTUli1bbG3Xrl3T7t275ePjI0ny8fHRlStXFBISYuuzdetWxcXFqXbt2rY+QUFBio6OtvUJDAyUt7e3bcFnHx8fh/Pc7XP3PIlFYgIAAACkQzdu3FBoaKhCQ0Ml3ZnwHhoaqhMnTshisWjgwIGaMGGC1q1bpwMHDqhr167y8vKy3bmrbNmyeuaZZ9S7d2/t2bNHv/zyi/z9/dWxY0d5eXlJkjp16iRnZ2f17NlThw4d0qpVqzR79myH4WYDBgzQpk2bNH36dB05ckRjx47V3r175e/vn6TrMXq74JTC7YIzF24XnLlwu2AAyBjS8u2Ca07cZuzcwW83TnTfbdu2qUmTJvHau3XrpiVLltgWWFy0aJGuXLmi+vXra/78+XryySdtfS9duiR/f3+HBRbnzJlzzwUW8+fPr/79+2v48OEO51y9erXeeecd2wKLU6dOTV8LLKYUEpPMhcQkcyExAYCMgcQkYUlJTDKaNPxPAgAAAEh9yTnXA4nHHBMAAAAAxpGYAAAAADCOoVwAAACAHQtjuYygYgIAAADAOComAAAAgB0KJmZQMQEAAABgHIkJAAAAAOMYygUAAADYYfK7GVRMAAAAABhHxQQAAACwQ8HEDComAAAAAIyjYgIAAADYYY6JGVRMAAAAABhHYgIAAADAOIZyAQAAAHYYyWUGFRMAAAAAxlExAQAAAOww+d0MKiYAAAAAjCMxAQAAAGAcQ7kAAAAAOwzlMoOKCQAAAADjqJgAAAAAdiiYmEHFBAAAAIBxJCYAAAAAjGMoFwAAAGCHye9mUDEBAAAAYBwVEwAAAMAOBRMzqJgAAAAAMI6KCQAAAGCHOSZmUDEBAAAAYByJCQAAAADjGMoFAAAA2GEklxlUTAAAAAAYR8UEAAAAsONEycQIKiYAAAAAjCMxAQAAAGAcQ7kAAAAAO4zkMoOKCQAAAADjqJgAAAAAdlj53QwqJgAAAACMo2ICAAAA2HGiYGIEFRMAAAAAxpGYAAAAADCOoVwAAACAHSa/m0HFBAAAAIBxVEwAAAAAOxRMzMiQiYkT/5oyFacspiNAaroWGW06BKQit+zZTIcAAEglDOUCAAAAYFyGrJgAAAAAD8siRt+YQMUEAAAAgHFUTAAAAAA7rPxuBhUTAAAAAMZRMQEAAADssMCiGVRMAAAAABhHYgIAAADAOIZyAQAAAHYYyWUGFRMAAAAAxlExAQAAAOw4UTIxgooJAAAAAONITAAAAAAYx1AuAAAAwA4jucygYgIAAADAOComAAAAgB1WfjeDigkAAAAA46iYAAAAAHYomJhBxQQAAACAcSQmAAAAAIxL1FCudevWJfqArVu3fuhgAAAAANNY+d2MRCUmbdu2TdTBLBaLYmNjHyUeAAAAAJlQohKTuLi4lI4DAAAASBOol5jxSHNMbt26lVxxAAAAAMjEkpyYxMbG6t1339Vjjz2mXLly6Z9//pEkjRo1SosXL072AAEAAABkfElOTCZOnKglS5Zo6tSpcnZ2trVXqFBBH330UbIGBwAAAKQ2i8VibMvMkpyYfPrpp1q0aJE6d+6sLFmy2NorV66sI0eOJGtwAAAAABIWGxurUaNGqWTJksqePbueeOIJvfvuu7JarbY+VqtVo0ePVuHChZU9e3b5+vrqzz//dDjOpUuX1LlzZ7m5ucnDw0M9e/bUjRs3HPrs379fDRo0kKurq4oWLaqpU6cm+/UkOTE5ffq0SpUqFa89Li5O0dHRyRIUAAAAYIqTxdyWFFOmTNGCBQs0d+5cHT58WFOmTNHUqVP1/vvv2/pMnTpVc+bM0cKFC7V7927lzJlTzZs3d5gr3rlzZx06dEiBgYHasGGDgoKC1KdPH9v+a9euqVmzZipevLhCQkL03nvvaezYsVq0aNEjv9b2EnVXLnvlypXTTz/9pOLFizu0f/nll6patWqyBQYAAADg3nbs2KE2bdqoZcuWkqQSJUro888/1549eyTdqZbMmjVL77zzjtq0aSPpzuinQoUKae3aterYsaMOHz6sTZs2KTg4WDVq1JAkvf/++2rRooWmTZsmLy8vLV++XLdv39bHH38sZ2dnlS9fXqGhoZoxY4ZDAvOoklwxGT16tPz9/TVlyhTFxcXp66+/Vu/evTVx4kSNHj062QIDAAAATDA5xyQqKkrXrl1z2KKiohKMs27dutqyZYv++OMPSdJvv/2mn3/+Wc8++6wk6dixYwoLC5Ovr6/tOe7u7qpdu7Z27twpSdq5c6c8PDxsSYkk+fr6ysnJSbt377b1adiwocP88ubNm+vo0aO6fPlysr3uSU5M2rRpo/Xr1+uHH35Qzpw5NXr0aB0+fFjr16/X008/nWyBAQAAAJlNQECA3N3dHbaAgIAE+44YMUIdO3ZUmTJllC1bNlWtWlUDBw5U586dJUlhYWGSpEKFCjk8r1ChQrZ9YWFhKliwoMP+rFmzKm/evA59EjqG/TmSQ5KHcklSgwYNFBgYmGxBAAAAAJBGjhypwYMHO7S5uLgk2PeLL77Q8uXLtWLFCtvwqoEDB8rLy0vdunVLjXCT1UMlJpK0d+9eHT58WNKdeSfVq1dPtqAAAAAAU0zetdfFxeWeich/DRs2zFY1kaSKFSvq33//VUBAgLp16yZPT09JUnh4uAoXLmx7Xnh4uKpUqSJJ8vT01Llz5xyOGxMTo0uXLtme7+npqfDwcIc+dx/f7ZMckjyU69SpU2rQoIFq1aqlAQMGaMCAAapZs6bq16+vU6dOJVtgAAAAAO7t5s2bcnJy/HU+S5YsiouLkySVLFlSnp6e2rJli23/tWvXtHv3bvn4+EiSfHx8dOXKFYWEhNj6bN26VXFxcapdu7atT1BQkMMdeAMDA+Xt7a08efIk2/UkOTHp1auXoqOjdfjwYV26dEmXLl3S4cOHFRcXp169eiVbYAAAAIAJ6WWBxVatWmnixInauHGjjh8/rjVr1mjGjBl6/vnnbdcxcOBATZgwQevWrdOBAwfUtWtXeXl5qW3btpKksmXL6plnnlHv3r21Z88e/fLLL/L391fHjh3l5eUlSerUqZOcnZ3Vs2dPHTp0SKtWrdLs2bPjDTl75Nfdar8CSyJkz55dO3bsiHdr4JCQEDVo0EA3b95M1gAfxq0Y0xEASCnXIlkvKTNxy57NdAgAUojrQ08oSHldV+w3du5PO1VKdN/r169r1KhRWrNmjc6dOycvLy+9/PLLGj16tO0OWlarVWPGjNGiRYt05coV1a9fX/Pnz9eTTz5pO86lS5fk7++v9evXy8nJSe3bt9ecOXOUK1cuW5/9+/fLz89PwcHByp8/v/r376/hw4cn34XrIRKTJ598Up999plq1arl0L5nzx516tRJf/31V7IG+DBITICMi8QkcyExATIuEpOEJSUxyWiSPJTrvffeU//+/bV3715b2969ezVgwABNmzYtWYMDAAAAUlt6Wfk9o0lUxSRPnjwOY94iIiIUExOjrFnvpLp3/54zZ05dunQp5aJNJComQMZFxSRzoWICZFxpuWLS/XNzFZMlL2feikmi/knMmjUrhcMAAAAA0oakTkJH8khUYpIeF2gBAAAAkH48UhHt1q1bun37tkObm5vbIwUEAAAAmES9xIwkT36PiIiQv7+/ChYsqJw5cypPnjwOGwAAAAAkVZITkzfffFNbt27VggUL5OLioo8++kjjxo2Tl5eXPv3005SIEQAAAEAGl+ShXOvXr9enn36qxo0bq0ePHmrQoIFKlSql4sWLa/ny5ercuXNKxAkAAACkCicmvxuR5IrJpUuX9Pjjj0u6M5/k7u2B69evr6CgoOSNDgAAAECmkOTE5PHHH9exY8ckSWXKlNEXX3wh6U4lxcPDI1mDAwAAAFKbxWJuy8ySnJj06NFDv/32myRpxIgRmjdvnlxdXTVo0CANGzYs2QMEAAAAkPElauX3+/n3338VEhKiUqVKqVKltLFSJSu/AxkXK79nLqz8DmRcaXnl995fHDR27g87VDB2btOSXDH5r+LFi6tdu3bKmzev+vTpkxwxAQAAAMZYLBZjW2b2yInJXRcvXtTixYuT63AAAAAAMpE0XEQDAAAAUl8mL1wYk2wVEwAAAAB4WCQmAAAAAIxL9FCudu3a3Xf/lStXHjUWAAAAwDhWfjcj0RUTd3f3+27FixdX165dUzJW/MfiDxepcnlvTQ2Y6ND+W+iv6tWjq2rXqKK6taqpR9fOunXrlqEokZxC9gar/+t95du4viqX99bWLT+YDgmJFLpvr4YP8lPbZ5qoQY0KCtq2xWG/1WrVRwvnqk3zxmpar7oGvt5LJ0/869DnxVbN1KBGBYftsyUfOfT568+j8uvVVU3rVlP7lk21fOnHKX5teDgP+jz/EPi9Xuv9qhrWra3K5b115PBhQ5EiNdzrZzqQmSS6YvLJJ5+kZBxIooMH9uvL1Sv15JPeDu2/hf6q11/rpVd7vaYRb49S1ixZdPToETk5MWovI4iMvClvb2+1bddegwf4mw4HSXArMlKlSnurZevn9fawgfH2r1j6sb5auVxvjZ2owo89psUL5mpI/9e07Itv5OLiYuvXs6+/WrV9wfY4R84ctr9H3LihIf59VL1WHQ0dOVp///WHJo8frdy5c6t1uxdT9PqQdA/6PEdG3lTVqtXUvPmzGjfmHQMRIrXc62c6zKFgYgZ35UqHbkZEaOTwYRozboI+/GCBw773pgTo5c5d1LP3/9aUKVHy8dQOESmkfoNGqt+gkekw8BDq1GugOvUaJLjParXqi8+XqWvPPmrQ+ClJ0tvjJ6lNs0b6adsW+TZvYeubI0dO5cufP8HjfL9pg6KjozVy9ARly5ZNJZ8opb/+OKpVyz8lMUmDHvR5btW6rSTp9OlTqRQRTLjfz3Qgs+Fr9HRo0oTxatiwker41HVov3jxog7s/0158+VT184d1aRhXb3a7RXtC9lrKFIAiXH29CldunhBNWr52Npy5cqtshUq6dCB3xz6Ll/6kVo2radXO72gFZ9+rJiYGNu+Q/t/U+WqNZQt2/9WS6/lU08n/j2m69eupvyFAEiye/1Mh1kssGgGFZN05rtvN+rw4d+1YtWX8fadPnVSkrRw3lwNHvamvMuU1YZv1qpPz+766psNKl68RCpHCyAxLl68IEnKky+fQ3vevPl06f/3SVL7lzrLu0xZ5XZ318HfQvXBvNm6eOGC+g9+U5J06eIFFfYq4nCMPHnz2c6R2809JS8DQBLd72c6kBkZT0wiIyMVEhKivHnzqly5cg77bt26pS+++OK+k+qjoqIUFRXl0GbN4uIwJjujCDt7VlMnT9QHH36c4PXFxcVJkl7o8JLaPt9eklS2bDnt3r1Ta7/+SgMGDUnVeAEkr46vdLP9vVRpb2XLlk3vTRqv1/wHytnZ2WBkAJLqQT/TgczI6FCuP/74Q2XLllXDhg1VsWJFNWrUSGfPnrXtv3r1qnr06HHfYwQEBMS7Q9h7UwJSOnQjfv/9kC5dvKiOL7ZTtUrlVK1SOe0N3qMVy5epWqVyypfvzrjzx594wuF5JR9/QmFnz5gIGUAi3P3sXr540aH90qWLypsv4fkkklSuQiXFxsYo7MxpSVLefPl16ZLjMS7//+N89zkOgNT3oJ/psbGxpkPM1JwMbplZoiom69atS/QBW7dunei+w4cPV4UKFbR3715duXJFAwcOVL169bRt2zYVK1YsUccYOXKkBg8e7NBmzZIxv3moXaeOvly73qFtzNsjVeLxx9WjZ28VKVpUBQoW1PFjxxz6/Hv8uOo3aJiaoQJIgsKPFVHefPkVErxLpb3LSLpzh63DB/erbfsO93zen3/cueNenrx5JUnlK1XWh/PnKCYmWlmz3plnErx7h4oVL8kwLiCNedDP9CxZshiKDDAnUYlJ27ZtE3Uwi8WSpAx/x44d+uGHH5Q/f37lz59f69ev1+uvv64GDRroxx9/VM6cOR94DBeX+MO2bsXco3M6lzNnLpUu/aRDW/YcOeTh7mFr796jpxbMe1/e3mXkXaas1n2zRseP/aPpM+eYCBnJ7GZEhE6cOGF7fPrUKR05fFju7u4q7OVlMDI8yM2bN3X65P/eu7OnT+vPo0fk5u6uQp6F1eHlLlq6eJGKFC2uwo89po8WzFW+AgXVoHFTSdLB/aH6/eABVatRUzly5NTBA7/p/RlT1ezZ52xJx9PPtNSSDxdo8vjR6tytp/75+099+fly2xwUpC0P+jxfvXJFZ8+e1fnz5yRJx4/f+dIpf/78yl+ggJGYkXwS8zMd5mT2SeimJCoxuTt3IblFRkYqa9b/hWCxWLRgwQL5+/urUaNGWrFiRYqcNyN7pWt3RUXd1ntTA3T16lV5e5fRwg8/VtFEVqCQth06dFC9evxvztW0qXeGLbZu87zenTTZVFhIhKO/H9QbfV+1PZ47c6ok6Znn2ujtsRPVqdurirwVqfcmjdWN69dVsUo1TZuz0PbFSzZnZ235/jt9smi+bkffVmGvx9ShUxe91Pl/805y5cqt6XMXaeaUierVpYPcPfKoe6++3Co4jXrQ53nbj1s1+p2Rtv3Dhw6SJPV93V/9/PqnbrAAkAosVqvVaurktWrVUv/+/dWlS5d4+/z9/bV8+XJdu3YtyeMsM2rFBIB0LTLadAhIRW7Zsz24E4B0ydX4LZju7Y21R4yde07bMsbObdpD/ZOIiIjQ9u3bdeLECd2+fdth3xtvvJHo4zz//PP6/PPPE0xM5s6dq7i4OC1cuPBhQgQAAAAeihMjuYxIcsXk119/VYsWLXTz5k1FREQob968unDhgnLkyKGCBQvqn3/+SalYE42KCZBxUTHJXKiYABlXWq6YDPzGXMVkVpvMWzFJ8l3JBg0apFatWuny5cvKnj27du3apX///VfVq1fXtGnTUiJGAAAAINU4WcxtmVmSE5PQ0FANGTJETk5OypIli6KiolS0aFFNnTpVb731VkrECAAAACCDS3Jiki1bNjk53XlawYIFbbc6dHd318mTJ5M3OgAAACCVWSwWY1tmluTRfVWrVlVwcLBKly6tRo0aafTo0bpw4YKWLVumChUqpESMAAAAADK4JFdMJk2apMKFC0uSJk6cqDx58qhfv346f/68Fi1alOwBAgAAAMj4klwxqVGjhu3vBQsW1KZNm5I1IAAAAMCkzD4J3ZQkV0wAAAAAILkluWJSsmTJ+07MSQvrmAAAAAAPK5PPQTcmyYnJwIEDHR5HR0fr119/1aZNmzRs2LDkigsAAABAJpLkxGTAgAEJts+bN0979+595IAAAAAAZD7JNsfk2Wef1VdffZVchwMAAACMcLJYjG2ZWbIlJl9++aXy5s2bXIcDAAAAkIk81AKL9pPfrVarwsLCdP78ec2fPz9ZgwMAAABSG7etNSPJiUmbNm0cEhMnJycVKFBAjRs3VpkyZZI1OAAAAACZQ5ITk7Fjx6ZAGAAAAEDakMmnehiT5EpVlixZdO7cuXjtFy9eVJYsWZIlKAAAAACZS5ITE6vVmmB7VFSUnJ2dHzkgAAAAAJlPoodyzZkzR5JksVj00UcfKVeuXLZ9sbGxCgoKYo4JAAAA0r3MftteUxKdmMycOVPSnYrJwoULHYZtOTs7q0SJElq4cGHyRwgAAAAgw0t0YnLs2DFJUpMmTfT1118rT548KRYUAAAAYAoFEzOSfFeuH3/8MSXiAAAAAJCJJXnye/v27TVlypR47VOnTtWLL76YLEEBAAAAyFySnJgEBQWpRYsW8dqfffZZBQUFJUtQAAAAgClOFnNbZpbkxOTGjRsJ3hY4W7ZsunbtWrIEBQAAACBzSXJiUrFiRa1atSpe+8qVK1WuXLlkCQoAAAAwxcliMbZlZkme/D5q1Ci1a9dOf//9t5566ilJ0pYtW/T5559r9erVyR4gAAAAgIwvyYlJq1attHbtWk2aNElffvmlsmfPrkqVKumHH35Qo0aNUiJGAAAAINVk8sKFMUlOTCSpZcuWatmyZbz2gwcPqkKFCo8cFAAAAIDMJclzTP7r+vXrWrRokWrVqqXKlSsnR0wAAAAAMpmHTkyCgoLUtWtXFS5cWNOmTdNTTz2lXbt2JWdsAAAAQKrjdsFmJGkoV1hYmJYsWaLFixfr2rVr6tChg6KiorR27VruyAUAAADgoSW6YtKqVSt5e3tr//79mjVrls6cOaP3338/JWMDAAAAUp3F4J/MLNEVk++++05vvPGG+vXrp9KlS6dkTAAAAAAymURXTH7++Wddv35d1atXV+3atTV37lxduHAhJWMDAAAAkEkkOjGpU6eOPvzwQ509e1avvfaaVq5cKS8vL8XFxSkwMFDXr19PyTgBAACAVMHkdzOSfFeunDlz6tVXX9XPP/+sAwcOaMiQIZo8ebIKFiyo1q1bp0SMAAAAADK4R1rHxNvbW1OnTtWpU6f0+eefJ1dMAAAAgDFUTMx45AUWJSlLlixq27at1q1blxyHAwAAAJDJJGkdEwAAACCjs1gyeenCkGSpmAAAAADAoyAxAQAAAGAcQ7kAAAAAO5l9EropVEwAAAAAGEdiAgAAANixWMxtSXX69Gm98sorypcvn7Jnz66KFStq7969tv1Wq1WjR49W4cKFlT17dvn6+urPP/90OMalS5fUuXNnubm5ycPDQz179tSNGzcc+uzfv18NGjSQq6urihYtqqlTpz7Ua3s/JCYAAABAOnT58mXVq1dP2bJl03fffafff/9d06dPV548eWx9pk6dqjlz5mjhwoXavXu3cubMqebNm+vWrVu2Pp07d9ahQ4cUGBioDRs2KCgoSH369LHtv3btmpo1a6bixYsrJCRE7733nsaOHatFixYl6/VYrFarNVmPmAbcijEdAYCUci0y2nQISEVu2bOZDgFACnFNwzOdZwT9Y+zcfrUfU1RUlEObi4uLXFxc4vUdMWKEfvnlF/30008JHstqtcrLy0tDhgzR0KFDJUlXr15VoUKFtGTJEnXs2FGHDx9WuXLlFBwcrBo1akiSNm3apBYtWujUqVPy8vLSggUL9PbbbyssLEzOzs62c69du1ZHjhxJtmunYgIAAADYcbJYjG0BAQFyd3d32AICAhKMc926dapRo4ZefPFFFSxYUFWrVtWHH35o23/s2DGFhYXJ19fX1ubu7q7atWtr586dkqSdO3fKw8PDlpRIkq+vr5ycnLR7925bn4YNG9qSEklq3ry5jh49qsuXLyff655sRwIAAADwSEaOHKmrV686bCNHjkyw7z///KMFCxaodOnS2rx5s/r166c33nhDS5culSSFhYVJkgoVKuTwvEKFCtn2hYWFqWDBgg77s2bNqrx58zr0SegY9udIDmm4iAYAAACkPpO3C77XsK2ExMXFqUaNGpo0aZIkqWrVqjp48KAWLlyobt26pWSYKYKKCQAAAJAOFS5cWOXKlXNoK1u2rE6cOCFJ8vT0lCSFh4c79AkPD7ft8/T01Llz5xz2x8TE6NKlSw59EjqG/TmSA4kJAAAAYCe93C64Xr16Onr0qEPbH3/8oeLFi0uSSpYsKU9PT23ZssW2/9q1a9q9e7d8fHwkST4+Prpy5YpCQkJsfbZu3aq4uDjVrl3b1icoKEjR0f+7AU1gYKC8vb0d7gD2qEhMAAAAgHRo0KBB2rVrlyZNmqS//vpLK1as0KJFi+Tn5ydJslgsGjhwoCZMmKB169bpwIED6tq1q7y8vNS2bVtJdyoszzzzjHr37q09e/bol19+kb+/vzp27CgvLy9JUqdOneTs7KyePXvq0KFDWrVqlWbPnq3Bgwcn6/UwxwQAAABIh2rWrKk1a9Zo5MiRGj9+vEqWLKlZs2apc+fOtj5vvvmmIiIi1KdPH125ckX169fXpk2b5OrqauuzfPly+fv7q2nTpnJyclL79u01Z84c2353d3d9//338vPzU/Xq1ZU/f36NHj3aYa2T5MA6JgDSFdYxyVxYxwTIuNLyOibzfjlu7Nx+9UoYO7dpafifBADExy+qmUtcxvvuDPfhlNQB9gAyFBITAAAAwA45shlMfgcAAABgHIkJAAAAAOMYygUAAADYMbnye2ZGxQQAAACAcVRMAAAAADvcIc4MKiYAAAAAjCMxAQAAAGAcQ7kAAAAAO4zkMoOKCQAAAADjqJgAAAAAdpj8bgYVEwAAAADGUTEBAAAA7FAwMYOKCQAAAADjSEwAAAAAGMdQLgAAAMAO39ybwesOAAAAwDgqJgAAAIAdC7PfjaBiAgAAAMA4EhMAAAAAxjGUCwAAALDDQC4zqJgAAAAAMI6KCQAAAGDHicnvRlAxAQAAAGAcFRMAAADADvUSM6iYAAAAADCOxAQAAACAcQzlAgAAAOww990MKiYAAAAAjKNiAgAAANixUDIxgooJAAAAAONITAAAAAAYx1AuAAAAwA7f3JvB6w4AAADAOComAAAAgB0mv5tBxQQAAACAcVRMAAAAADvUS8ygYgIAAADAOBITAAAAAMYxlAsAAACww+R3M6iYAAAAADCOigkAAABgh2/uzeB1BwAAAGAciQkAAAAA4xjKBQAAANhh8rsZVEwAAAAAGEfFBAAAALBDvcQMKiYAAAAAjKNiAgAAANhhiokZVEwAAAAAGEdiAgAAAMA4hnIBAAAAdpyY/m4EFRMAAAAAxlExAQAAAOww+d0MKiYAAAAAjCMxAQAAAGAciUk6E7I3WP1f7yvfxvVVuby3tm75wWH/zYgITZowXk8/1VC1qlXS861a6ItVnxuKFslt8YcfqFOH9vKpWVWNG/hoYP/XdfzYP6bDQjJ50Of7h8Dv9VrvV9Wwbm1VLu+tI4cPG4oUDyNkb7AG+PXV000aqGqFMvrxP+/vlsDv1a/3q2pcr7aqViijo0fu/f5arVb59e2d4HGQvqxcsVzPPv2UalatqM4dX9SB/ftNhwRJFoN/MjMSk3QmMvKmvL29NfKdMQnunzZ1snb8/JMmTX5Pa9Z/q85dumnyxHe1beuWVI4UKWFv8B699HJnLfv8C33w4SeKiYlR3949dfPmTdOhIRk86PMdGXlTVatW08DBQ1M5MiSHyMhIPeldRiPfHn3P/VWqVdcbgx78/i5ftlQWBsGne5u++1bTpgbotdf9tHL1Gnl7l1G/13rq4sWLpkMDjGDyezpTv0Ej1W/Q6J77Q0N/Vas2bVWzVm1J0gsdXtKXq1fp4IH9avxU09QKEylkwaLFDo/HT5ysJg18dPj3Q6peo6ahqJBcHvT5btW6rSTp9OlTqRQRklP9Bg1Vv0HDe+5/rnUbSdKZB7y/R48c1rKln2j5qi/1dOMGyRojUteypZ+o3Qsd1Pb59pKkd8aMU1DQNq39+iv17N3HcHSZG3m/GVRMMpgqVapq+49bFR4eLqvVqj27d+nf48fkU6++6dCQAm5cvy5JcnN3NxwJgNQQGRmpkW8O1Yi3Ryt//gKmw8EjiL59W4d/P6Q6PnVtbU5OTqpTp672//arwcgAc4xXTA4fPqxdu3bJx8dHZcqU0ZEjRzR79mxFRUXplVde0VNPPXXf50dFRSkqKsqhzZrFRS4uLikZdpo14u1RGj9mlJo91VBZs2aVxWLRmHET+DY9A4qLi9PUKZNUpWo1lS79pOlwAKSC6VMDVLlKVTWhAp7uXb5yWbGxscqXL59De758+XSMuYPGscCiGUYrJps2bVKVKlU0dOhQVa1aVZs2bVLDhg31119/6d9//1WzZs20devW+x4jICBA7u7uDtt7UwJS6QrSns+XL9P+/aGaPXeBPv/iKw0ZNkKTJozTrp07TIeGZDZpwjj9/eefmjptpulQAKSCbT9u1Z7duzVsxEjToQBAijBaMRk/fryGDRumCRMmaOXKlerUqZP69euniRMnSpJGjhypyZMn37dqMnLkSA0ePNihzZolc1ZLbt26pTmzZmrmnLlq2KixJOlJ7zI6evSwln6y2KFcjPRt0oTxCtq+TR8v/UyFPD1NhwMgFQTv3qVTJ0+ooU8th/ahg95Q1WrV9dGSZYYiw8PI45FHWbJkiTfR/eLFi8qfP7+hqACzjCYmhw4d0qeffipJ6tChg7p06aIXXnjBtr9z58765JNP7nsMF5f4w7ZuxSR/rOlBTEyMYmKi5eTkWH50csqiOKvVUFRITlarVQET39XWLYFavGSZihQpajokAKmkR6/eer79Cw5tLz7fWkPeHKFGje8/7BlpTzZnZ5UtV167d+3UU019Jd0Zort79051fPkVw9GBye9mGJ9jcvd2h05OTnJ1dZW73STe3Llz6+rVq6ZCS5NuRkToxIkTtsenT53SkcOH5e7ursJeXqpRs5ZmTHtPLi6uKuzlpZDgYG1Yt1ZD3xxhMGokl0nvjtN3327QrPfnK2eOnLpw/rwkKVfu3HJ1dTUcHR7Vgz7fV69c0dmzZ3X+/DlJ0vHjxyRJ+fPnV/4CTIRO627ejNBJ+/f39CkdPXJYbu7uKlzYS1evXlHY2bM6d+7/399jd97ffPnzK3/+ArbtvwoX9tJjRYqkzkUgWXXp1kOj3hqu8uUrqELFSvps2VJFRkaq7fPtTIcGGGGxWs19lV65cmVNmTJFzzzzjCTp4MGDKlOmjLJmvZMv/fTTT+rWrZv++Sdpk8AycsUkeM9u9erRNV576zbP691Jk3Xh/HnNnjVDO3f8rGtXr6qwl5fav/CSunTrzj3vM4DK5b0TbB8/IUBt+EGW7j3o8/3Nmq81+p348wv6vu6vfn79UyPEVJeRqr179+xW71e7xWtv1aatxk+crHVrv9aYd96Kt/+1fn7qe4/3t2qFMpoxe66a/P837umdUyb8OfX58s+09JPFunDhvLzLlNXwt95RpUqVTYeVKlyNfz1+b98fPm/s3M3KZt4vmowmJgsXLlTRokXVsmXLBPe/9dZbOnfunD766KMkHTcjJyYAkJlkpMQED5YZE5PMjMQkYSQmGQyJCQBkDCQmmQuJSeZCYpKwzJyYpOF/EgAAAEDqs7COiRGs/A4AAADAOComAAAAgB0nCiZGUDEBAAAAYByJCQAAAGDHYvDPw5o8ebIsFosGDhxoa7t165b8/PyUL18+5cqVS+3bt1d4eLjD806cOKGWLVsqR44cKliwoIYNG6aYGMc7SW3btk3VqlWTi4uLSpUqpSVLljx0nPdDYgIAAACkY8HBwfrggw9UqVIlh/ZBgwZp/fr1Wr16tbZv364zZ86oXbv/rXsWGxurli1b6vbt29qxY4eWLl2qJUuWaPTo0bY+x44dU8uWLdWkSROFhoZq4MCB6tWrlzZv3pzs18HtggEAaRa3C85cuF1w5pKWbxe89chFY+d+qky+JPW/ceOGqlWrpvnz52vChAmqUqWKZs2apatXr6pAgQJasWKFXnjhBUnSkSNHVLZsWe3cuVN16tTRd999p+eee05nzpxRoUKFJN1ZZ3D48OE6f/68nJ2dNXz4cG3cuFEHDx60nbNjx466cuWKNm3alHwXLiomAAAAgAOLxdwWFRWla9euOWxRUVH3jNXPz08tW7aUr6+vQ3tISIiio6Md2suUKaNixYpp586dkqSdO3eqYsWKtqREkpo3b65r167p0KFDtj7/PXbz5s1tx0hOJCYAAABAGhEQECB3d3eHLSAgIMG+K1eu1L59+xLcHxYWJmdnZ3l4eDi0FypUSGFhYbY+9knJ3f13992vz7Vr1xQZGflQ13gvabiIBgAAAKQ+kwssjhw5UoMHD3Zoc3Fxidfv5MmTGjBggAIDA+Xq6ppa4aUoKiYAAABAGuHi4iI3NzeHLaHEJCQkROfOnVO1atWUNWtWZc2aVdu3b9ecOXOUNWtWFSpUSLdv39aVK1ccnhceHi5PT09JkqenZ7y7dN19/KA+bm5uyp49e3JdtiQSEwAAACDdadq0qQ4cOKDQ0FDbVqNGDXXu3Nn292zZsmnLli225xw9elQnTpyQj4+PJMnHx0cHDhzQuXPnbH0CAwPl5uamcuXK2frYH+Nun7vHSE4M5QIAAADspIeV33Pnzq0KFSo4tOXMmVP58uWztffs2VODBw9W3rx55ebmpv79+8vHx0d16tSRJDVr1kzlypVTly5dNHXqVIWFhemdd96Rn5+frUrTt29fzZ07V2+++aZeffVVbd26VV988YU2btyY7NdEYgIAAABkQDNnzpSTk5Pat2+vqKgoNW/eXPPnz7ftz5IlizZs2KB+/frJx8dHOXPmVLdu3TR+/Hhbn5IlS2rjxo0aNGiQZs+erSJFiuijjz5S8+bNkz1e1jEBAKRZrGOSubCOSeaSltcx+emPy8bO3eDJPMbObRpzTAAAAAAYR2ICAAAAwLg0XEQDAAAAUh+jCs2gYgIAAADAOComAAAAgB0KJmZQMQEAAABgHBUTAAAAwA63rjaDigkAAAAA40hMAAAAABjHUC4AAADADgO5zKBiAgAAAMA4KiYAAACAPUomRlAxAQAAAGAciQkAAAAA4xjKBQAAANixMJbLCComAAAAAIyjYgIAAADYYeF3M6iYAAAAADCOigkAAABgh4KJGVRMAAAAABhHYgIAAADAOIZyAQAAAPYYy2UEFRMAAAAAxlExAQAAAOywwKIZVEwAAAAAGEdiAgAAAMA4hnIBAAAAdlj53QwqJgAAAACMo2ICAAAA2KFgYgYVEwAAAADGUTEBAAAA7FEyMYKKCQAAAADjSEwAAAAAGMdQLgAAAMAOK7+bQcUEAAAAgHFUTAAAAAA7LLBoBhUTAAAAAMaRmAAAAAAwjqFcAAAAgB1GcplBxQQAAACAcVRMAABplhMzUDOVKxHRpkNAKvJ0z2Y6hHvjvx4jqJgAAAAAMI6KCQAAAGCHBRbNoGICAAAAwDgSEwAAAADGMZQLAAAAsMN9N8ygYgIAAADAOComAAAAgB0KJmZQMQEAAABgHIkJAAAAAOMYygUAAADYYyyXEVRMAAAAABhHxQQAAACww8rvZlAxAQAAAGAcFRMAAADADgssmkHFBAAAAIBxJCYAAAAAjGMoFwAAAGCHkVxmUDEBAAAAYBwVEwAAAMAeJRMjqJgAAAAAMI7EBAAAAIBxDOUCAAAA7LDyuxlUTAAAAAAYR8UEAAAAsMPK72ZQMQEAAABgHBUTAAAAwA4FEzOomAAAAAAwjsQEAAAAgHEM5QIAAADsMZbLCComAAAAAIyjYgIAAADYYYFFM6iYAAAAADCOxAQAAACAcSQmAAAAgB2LxdyWFAEBAapZs6Zy586tggULqm3btjp69KhDn1u3bsnPz0/58uVTrly51L59e4WHhzv0OXHihFq2bKkcOXKoYMGCGjZsmGJiYhz6bNu2TdWqVZOLi4tKlSqlJUuWPMxLe18kJgAAAEA6tH37dvn5+WnXrl0KDAxUdHS0mjVrpoiICFufQYMGaf369Vq9erW2b9+uM2fOqF27drb9sbGxatmypW7fvq0dO3Zo6dKlWrJkiUaPHm3rc+zYMbVs2VJNmjRRaGioBg4cqF69emnz5s3Jej0Wq9VqTdYjpgG3Yh7cBwAApC1XIqJNh4BU5OmezXQI9/T3uUhj536iYPaHfu758+dVsGBBbd++XQ0bNtTVq1dVoEABrVixQi+88IIk6ciRIypbtqx27typOnXq6LvvvtNzzz2nM2fOqFChQpKkhQsXavjw4Tp//rycnZ01fPhwbdy4UQcPHrSdq2PHjrpy5Yo2bdr0aBdsh4oJAAAAkEZERUXp2rVrDltUVFSinnv16lVJUt68eSVJISEhio6Olq+vr61PmTJlVKxYMe3cuVOStHPnTlWsWNGWlEhS8+bNde3aNR06dMjWx/4Yd/vcPUZyITEBAAAA0oiAgAC5u7s7bAEBAQ98XlxcnAYOHKh69eqpQoUKkqSwsDA5OzvLw8PDoW+hQoUUFhZm62OflNzdf3ff/fpcu3ZNkZHJV11iHRMAAADAnsFlTEaOHKnBgwc7tLm4uDzweX5+fjp48KB+/vnnlAotxZGYAAAAAGmEi4tLohIRe/7+/tqwYYOCgoJUpEgRW7unp6du376tK1euOFRNwsPD5enpaeuzZ88eh+PdvWuXfZ//3skrPDxcbm5uyp794efE/BdDuQAAAAA7FoN/ksJqtcrf319r1qzR1q1bVbJkSYf91atXV7Zs2bRlyxZb29GjR3XixAn5+PhIknx8fHTgwAGdO3fO1icwMFBubm4qV66crY/9Me72uXuM5MJduQAAQJrAXbkyl7R8V65/zt8ydu7HC7gmuu/rr7+uFStW6JtvvpG3t7et3d3d3VbJ6Nevn7799lstWbJEbm5u6t+/vyRpx44dku7cLrhKlSry8vLS1KlTFRYWpi5duqhXr16aNGmSpDu3C65QoYL8/Pz06quvauvWrXrjjTe0ceNGNW/ePLkuncQEAACkDSQmmUtaTkyOXTCXmJTMn/jExHKPFRk/+eQTde/eXdKdBRaHDBmizz//XFFRUWrevLnmz59vG6YlSf/++6/69eunbdu2KWfOnOrWrZsmT56srFn/N+tj27ZtGjRokH7//XcVKVJEo0aNsp0juZCYAACANIHEJHMhMUlYUhKTjIY5JgAAAACM465cAAAAgB2DdwvO1KiYAAAAADCOigkAAABgj5KJEVRMAAAAABhHYgIAAADAOIZyAQAAAHaSugI7kgcVEwAAAADGUTEBAAAA7NxjQXWkMComAAAAAIyjYgIAAADYoWBiBhWTDCAi4oamBkzUM75NVKtaJXXt3FEHD+w3HRZS0MoVy/Xs00+pZtWK6tzxRR3Yz/udEYXsDVb/1/vKt3F9VS7vra1bfjAdElIBn+/057d9ezVisJ/atWiiRrUq6KdtW2z7YmKitfD9Ger+8vNq3rCm2rVoooljRurC+XMOxxg5xF8vtvLV0/Wr6flnG2vCmBHx+mwN3KSendurWYMa6tD6aX2+7ONUuT4gtZCYZABjR7+jnTt3aOLkqfpyzXr51K2n13r1UHh4uOnQkAI2ffetpk0N0Guv+2nl6jXy9i6jfq/11MWLF02HhmQWGXlT3t7eGvnOGNOhIJXw+U6fIm9FqlRpbw0c9na8fbdu3dIfR39X11df04fLvtC7U2bp5InjemuIv0O/qtVraeyk6Vq2eoPenTJTZ06d1OgRg2z7d+34SRNGj1Drdh20ZOUaDXrzHa3+fJm+/mJFil8fkFosVqvVajoIe1arVZZHnHF0KyaZgkkHbt26pbq1qmnW+/PVsFFjW3vHF9upfv0G8h8w6N5PRrrUueOLKl+hot56Z7QkKS4uTs2aNtLLnbqoZ+8+hqNDSqlc3lsz58zTU019TYeCFJTZP99XIqJNh/DIGtWqoAlTZ6tB46b37HP49wPq2/1lfbEuUIU8CyfY55egH/X2sDf0wy/7lDVrNo1/503FxMRo/OQZtj5frVquz5d9rNXrf3jk351M8HTPZjqEezp1OcrYuYvkcTF2btPSXMXExcVFhw8fNh1GuhEbG6PY2Fi5uDj+I3ZxcdGvv+4zFBVSSvTt2zr8+yHV8alra3NyclKdOnW1/7dfDUYG4FHx+c48Im7ckMViUa5cuRPcf+3qVQVu2qAKlaooa9Y7v7xHR9+Ws4uzQz8XFxedPxeusLNnUjxmIDUYm/w+ePDgBNtjY2M1efJk5cuXT5I0Y8aMBPvdFRUVpagox6zWmsUl3i/qGVXOnLlUuUpVLVo4XyUff1z58uXXd99u0P7fQlW0WDHT4SGZXb5yWbGxsbbPx1358uXTsWP/GIoKQHLg8505REVF6YO5M9W0WQvlzJXLYd/C92dozerPdetWpMpVqKzJM+bZ9tWsU0/zZk5VSMtdqlqjlk6fPKFVK5ZKki5eOK/CXo+l6nVkfOmvApURGKuYzJo1Sz/++KN+/fVXh81qterw4cP69ddfFRoa+sDjBAQEyN3d3WF7b0pAyl9AGjIxYKqsVquebtJQNatW1IrPlumZFi3l5JTmCmIAAGRaMTHRGvvWEFmtVg0ePire/o5deuijZas17f1FypLFSZPGjdTdEfet2r6g5198WSOG+Mm3XlX169lJTZ9+VpL4eY8Mw1jFZNKkSVq0aJGmT5+up556ytaeLVs2LVmyROXKlUvUcUaOHBmv+mLNkjmqJXcVLVZMHy/9TDdv3lRExA0VKFBQw4YMVJEiRU2HhmSWxyOPsmTJEm8i7MWLF5U/f35DUQFIDny+M7aYmGiNGTlE4WfPaOb8j+NVSyTJwyOPPDzyqGjxEipe4nG92MpXhw78pgqVqshisahv/8Hq/foAXbp4QR558iokeJckyeuxIql9OUCKMJZijxgxQqtWrVK/fv00dOhQRUc/3IQ3FxcXubm5OWyZZRjXf+XIkUMFChTUtatXtfOXn9W4yb0n3iF9yubsrLLlymv3rp22tri4OO3evVOVKlc1GBmAR8XnO+O6m5ScPnlCM+Z9JHcPjwc+526lJDr6tkN7lixZVKBgIWXLlk1bNn+r8hUryyNP3pQIO1OzWMxtmZnRBRZr1qypkJAQ+fn5qUaNGlq+fHm6vKuEab/8/JNktap4yZI6eeKEZk6bqhIlH1eb59uZDg0poEu3Hhr11nCVL19BFSpW0mfLlioyMlJteb8znJsRETpx4oTt8elTp3Tk8GG5u7ursJeXwciQUvh8p083b97U6VP/+6yePXNaf/5xRG5u7sqXP79GjxisP478rskz5ik2Nk4XL1yQJLm5uytbtmz6/eB+Hfn9oCpWqabcud105tRJLf7gfT1WpKjKV6wiSbpy5bK2b/leVarX1O3bt/Xd+jXatvV7zVm4xMAVAykjzdwueOXKlRo4cKDOnz+vAwcOJHooV0Iy0+2CJWnzpm81Z9YMhYeFyd3dQ02fbqb+AwYpd+6E7/aB9O/z5Z9p6SeLdeHCeXmXKavhb72jSpUqmw4LySx4z2716tE1XnvrNs/r3UmTDUSE1JCZP9/p9XbBv4bs0cB+r8Zrf6ZlG3Xv/bo6tm2e4PNmLfhYVavX0t9//aH3p0/W338e1a1bkcqbr4Bq+dRT11dfU4GChSTdSUxGDvbXsb//kNUqla9YWb36vaFyFSql6LWlpLR8u+AzV24/uFMK8fJwfnCnDCrNJCaSdOrUKYWEhMjX11c5c+Z86ONktsQEAICMIL0mJng4JCYJy8yJidGhXP9VpEgRFSnCBC4AAACYw8wCM7i/HAAAAADjSEwAAAAAGJemhnIBAAAApllY+d0IKiYAAAAAjKNiAgAAANijYGIEFRMAAAAAxpGYAAAAADCOoVwAAACAHUZymUHFBAAAAIBxVEwAAAAAO6z8bgYVEwAAAADGUTEBAAAA7LDAohlUTAAAAAAYR2ICAAAAwDiGcgEAAAD2GMllBBUTAAAAAMZRMQEAAADsUDAxg4oJAAAAAONITAAAAAAYx1AuAAAAwA4rv5tBxQQAAACAcVRMAAAAADus/G4GFRMAAAAAxlExAQAAAOwwx8QMKiYAAAAAjCMxAQAAAGAciQkAAAAA40hMAAAAABjH5HcAAADADpPfzaBiAgAAAMA4EhMAAAAAxjGUCwAAALDDyu9mUDEBAAAAYBwVEwAAAMAOk9/NoGICAAAAwDgqJgAAAIAdCiZmUDEBAAAAYByJCQAAAADjGMoFAAAA2GMslxFUTAAAAAAYR8UEAAAAsMMCi2ZQMQEAAABgHIkJAAAAAOMYygUAAADYYeV3M6iYAAAAADCOigkAAABgh4KJGVRMAAAAABhHYgIAAADAOIZyAQAAAPYYy2UEFRMAAAAAxlExAQAAAOyw8rsZVEwAAACAdGrevHkqUaKEXF1dVbt2be3Zs8d0SA+NxAQAAACwY7GY25Ji1apVGjx4sMaMGaN9+/apcuXKat68uc6dO5cyL0wKs1itVqvpIJLbrRjTEQAAgKS6EhFtOgSkIk/3bKZDuCeTv0u6JmGiRe3atVWzZk3NnTtXkhQXF6eiRYuqf//+GjFiRApFmHKomAAAAABpRFRUlK5du+awRUVFxet3+/ZthYSEyNfX19bm5OQkX19f7dy5MzVDTjYZcvJ7UjLNjCIqKkoBAQEaOXKkXFxcTIeDFMb7nbnwfmcumfn9TsvfoKeUzPx+p2Umf5ccOyFA48aNc2gbM2aMxo4d69B24cIFxcbGqlChQg7thQoV0pEjR1I6zBSRIYdyZUbXrl2Tu7u7rl69Kjc3N9PhIIXxfmcuvN+ZC+935sL7jf+KioqKVyFxcXGJl7ieOXNGjz32mHbs2CEfHx9b+5tvvqnt27dr9+7dqRJvcsqEtQUAAAAgbUooCUlI/vz5lSVLFoWHhzu0h4eHy9PTM6XCS1HMMQEAAADSGWdnZ1WvXl1btmyxtcXFxWnLli0OFZT0hIoJAAAAkA4NHjxY3bp1U40aNVSrVi3NmjVLERER6tGjh+nQHgqJSQbh4uKiMWPGMHEuk+D9zlx4vzMX3u/Mhfcbj+Kll17S+fPnNXr0aIWFhalKlSratGlTvAnx6QWT3wEAAAAYxxwTAAAAAMaRmAAAAAAwjsQEAAAAgHEkJgAAAACMIzHJIObNm6cSJUrI1dVVtWvX1p49e0yHhBQQFBSkVq1aycvLSxaLRWvXrjUdElJQQECAatasqdy5c6tgwYJq27atjh49ajospJAFCxaoUqVKcnNzk5ubm3x8fPTdd9+ZDgupZPLkybJYLBo4cKDpUABjSEwygFWrVmnw4MEaM2aM9u3bp8qVK6t58+Y6d+6c6dCQzCIiIlS5cmXNmzfPdChIBdu3b5efn5927dqlwMBARUdHq1mzZoqIiDAdGlJAkSJFNHnyZIWEhGjv3r166qmn1KZNGx06dMh0aEhhwcHB+uCDD1SpUiXToQBGcbvgDKB27dqqWbOm5s6dK+nOqp9FixZV//79NWLECMPRIaVYLBatWbNGbdu2NR0KUsn58+dVsGBBbd++XQ0bNjQdDlJB3rx59d5776lnz56mQ0EKuXHjhqpVq6b58+drwoQJqlKlimbNmmU6LMAIKibp3O3btxUSEiJfX19bm5OTk3x9fbVz506DkQFIblevXpV055dVZGyxsbFauXKlIiIi5OPjYzocpCA/Pz+1bNnS4ec4kFmx8ns6d+HCBcXGxsZb4bNQoUI6cuSIoagAJLe4uDgNHDhQ9erVU4UKFUyHgxRy4MAB+fj46NatW8qVK5fWrFmjcuXKmQ4LKWTlypXat2+fgoODTYcCpAkkJgCQDvj5+engwYP6+eefTYeCFOTt7a3Q0FBdvXpVX375pbp166bt27eTnGRAJ0+e1IABAxQYGChXV1fT4QBpAolJOpc/f35lyZJF4eHhDu3h4eHy9PQ0FBWA5OTv768NGzYoKChIRYoUMR0OUpCzs7NKlSolSapevbqCg4M1e/ZsffDBB4YjQ3ILCQnRuXPnVK1aNVtbbGysgoKCNHfuXEVFRSlLliwGIwRSH3NM0jlnZ2dVr15dW7ZssbXFxcVpy5YtjEsG0jmr1Sp/f3+tWbNGW7duVcmSJU2HhFQWFxenqKgo02EgBTRt2lQHDhxQaGiobatRo4Y6d+6s0NBQkhJkSlRMMoDBgwerW7duqlGjhmrVqqVZs2YpIiJCPXr0MB0aktmNGzf0119/2R4fO3ZMoaGhyps3r4oVK2YwMqQEPz8/rVixQt98841y586tsLAwSZK7u7uyZ89uODokt5EjR+rZZ59VsWLFdP36da1YsULbtm3T5s2bTYeGFJA7d+5488Vy5sypfPnyMY8MmRaJSQbw0ksv6fz58xo9erTCwsJUpUoVbdq0Kd6EeKR/e/fuVZMmTWyPBw8eLEnq1q2blixZYigqpJQFCxZIkho3buzQ/sknn6h79+6pHxBS1Llz59S1a1edPXtW7u7uqlSpkjZv3qynn37adGgAkCpYxwQAAACAccwxAQAAAGAciQkAAAAA40hMAAAAABhHYgIAAADAOBITAAAAAMaRmAAAAAAwjsQEAAAAgHEkJgAAAACMIzEBgEfUvXt3tW3b1va4cePGGjhwYKrHsW3bNlksFl25ciXFzvHfa30YqREnACD9ITEBkCF1795dFotFFotFzs7OKlWqlMaPH6+YmJgUP/fXX3+td999N1F9U/uX9BIlSmjWrFmpci4AAJIiq+kAACClPPPMM/rkk08UFRWlb7/9Vn5+fsqWLZtGjhwZr+/t27fl7OycLOfNmzdvshwHAIDMhIoJgAzLxcVFnp6eKl68uPr16ydfX1+tW7dO0v+GJE2cOFFeXl7y9vaWJJ08eVIdOnSQh4eH8ubNqzZt2uj48eO2Y8bGxmrw4MHy8PBQvnz59Oabb8pqtTqc979DuaKiojR8+HAVLVpULi4uKlWqlBYvXqzjx4+rSZMmkqQ8efLIYrGoe/fukqS4uDgFBASoZMmSyp49uypXrqwvv/zS4TzffvutnnzySWXPnl1NmjRxiPNhxMbGqmfPnrZzent7a/bs2Qn2HTdunAoUKCA3Nzf17dtXt2/ftu1LTOwAAPwXFRMAmUb27Nl18eJF2+MtW7bIzc1NgYGBkqTo6Gg1b95cPj4++umnn5Q1a1ZNmDBBzzzzjPbv3y9nZ2dNnz5dS5Ys0ccff6yyZctq+vTpWrNmjZ566ql7nrdr167auXOn5syZo8qVK+vYsWO6cOGCihYtqq+++krt27fX0aNH5ebmpuzZs0uSAgIC9Nlnn2nhwoUqXbq0goKC9Morr6hAgQJq1KiRTp48qXbt2snPz099+vTR3r17NWTIkEd6feLi4lSkSBGtXr1a+fLl044dO9SnTx8VLlxYHTp0cHjdXF1dtW3bNh0/flw9evRQvnz5NHHixETFDgBAgqwAkAF169bN2qZNG6vVarXGxcVZAwMDrS4uLtahQ4fa9hcqVMgaFRVle86yZcus3t7e1ri4OFtbVFSUNXv27NbNmzdbrVartXDhwtapU6fa9kdHR1uLFCliO5fVarU2atTIOmDAAKvVarUePXrUKskaGBiYYJw//vijVZL18uXLtrZbt25Zc+TIYd2xY4dD3549e1pffvllq9VqtY4cOdJarlw5h/3Dhw+Pd6z/Kl68uHXmzJn33P9ffn5+1vbt29sed+vWzZo3b15rRESErW3BggXWXLlyWWNjYxMVe0LXDAAAFRMAGdaGDRuUK1cuRUdHKy4uTp06ddLYsWNt+ytWrOgwr+S3337TX3/9pdy5czsc59atW/r777919epVnT17VrVr17bty5o1q2rUqBFvONddoaGhypIlS5IqBX/99Zdu3rypp59+2qH99u3bqlq1qiTp8OHDDnFIko+PT6LPcS/z5s3Txx9/rBMnTigyMlK3b99WlSpVHPpUrlxZOXLkcDjvjRs3dPLkSd24ceOBsQMAkBASEwAZVpMmTbRgwQI5OzvLy8tLWbM6/peXM2dOh8c3btxQ9erVtXz58njHKlCgwEPFcHdoVlLcuHFDkrRx40Y99thjDvtcXFweKo7EWLlypYYOHarp06fLx8dHuXPn1nvvvafdu3cn+himYgcApH8kJgAyrJw5c6pUqVKJ7l+tWjWtWrVKBQsWlJubW4J9ChcurN27d6thw4aSpJiYGIWEhKhatWoJ9q9YsaLi4uK0fft2+fr6xtt/t2ITGxtraytXrpxcXFx04sSJe1ZaypYta5vIf9euXbsefJH38csvv6hu3bp6/fXXbW1///13vH6//fabIiMjbUnXrl27lCtXLhUtWlR58+Z9YOwAACSEu3IBwP/r3Lmz8ufPrzZt2uinn37SsWPHtG3bNr3xxhs6deqUJGnAgAGaPHmy1q5dqyNHjuj111+/7xokJUqUULdu3fTqq69q7dq1tmN+8cUXkqTixYvLYrFow4YNOn/+vG7cuKHcuXNr6NChGjRokJYuXaq///5b+/bt0/vvv6+lS5dKkvr27as///xTw4YN09GjR7VixQotWbIkUdd5+vRphYaGOmyXL19W6dKltXfvXm3evFl//PGHRo0apeDg4HjPv337tnr27Knff/9d3377rcaMGSN/f385OTklKnYAABJCYgIA/y9HjhwKCgpSsWLF1K5dO5UtW1Y9e/bUrVu3bBWUIUOGqEuXLurWrZttuNPzzz9/3+MuWLBAL7zwgl5//XWVKVNGvXv3VkREhCTpscce07hx4zRixAgVKlRI/v7+kqR3331Xo0aNUkBAgMqWLatnnnlGGzduVMmSJSVJxYoV01dffaW1a9eqcuXKWrhwoSZNmpSo65w2bZqqVq3qsG3cuFGvvfaa2rVrp5deekm1a9fWxYsXHaondzVt2lSlS5dWw4YN9dJLL6l169YOc3ceFDsAAAmxWO81YxMAAAAAUgkVEwAAAADGkZgAAAAAMI7EBAAAAIBxJCYAAAAAjCMxAQAAAGAciQkAAAAA40hMAAAAABhHYgIAAADAOBITAAAAAMaRmAAAAAAwjsQEAAAAgHH/B5DGkknZjJzxAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now experimenting with LSTM"
      ],
      "metadata": {
        "id": "PprUtn6McY5F"
      },
      "id": "PprUtn6McY5F"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_model_LSTM():\n",
        "    input_shape = (187, 1)  # Assuming this is your input shape based on the dataset\n",
        "    model = Sequential([\n",
        "        LSTM(units=128, input_shape=input_shape, return_sequences=True),\n",
        "        Dropout(0.1),\n",
        "\n",
        "        LSTM(units=64, return_sequences=False),\n",
        "        Dropout(0.1),\n",
        "\n",
        "        Dense(units=150, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(5, activation='softmax')  # Adjusted for 5 classes\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the LSTM model\n",
        "model_LSTM = create_model_LSTM()\n",
        "\n",
        "# Now you can use model_LSTM for training just like you did with model_with_best_hyperparameters\n"
      ],
      "metadata": {
        "id": "h0JhcwQHiZMG"
      },
      "id": "h0JhcwQHiZMG",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model  # If you wish to load the best model later\n",
        "\n",
        "# Assuming fazeli_mitbih_train_df is your DataFrame and the last column is the label\n",
        "X = fazeli_mitbih_train_df.iloc[:, :-1].values\n",
        "y = fazeli_mitbih_train_df.iloc[:, -1].values\n",
        "\n",
        "# Reshape X to fit the LSTM input requirements and convert y to categorical\n",
        "X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "y_categorical = to_categorical(y)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define callbacks\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'best_model_LSTM.h5',  # Update the filename to reflect this is the LSTM model\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=5,\n",
        "    min_lr=0.001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Create the LSTM model with the best hyperparameters\n",
        "# Assuming 'create_model_LSTM()' function is defined and returns the LSTM model\n",
        "model_LSTM = create_model_LSTM()\n",
        "\n",
        "# Train the LSTM model with the callbacks\n",
        "history_LSTM = model_LSTM.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=100,  # Adjust the number of epochs as necessary\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, reduce_lr, model_checkpoint]\n",
        ")\n",
        "\n",
        "# Optionally, load the best model saved during training\n",
        "# best_model_LSTM = load_model('best_model_LSTM.h5')\n",
        "\n",
        "# Evaluation on Test Data\n",
        "test_loss, test_accuracy = model_LSTM.evaluate(X_test, y_test, verbose=2)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Predictions and Classification Report\n",
        "y_pred = model_LSTM.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4207
        },
        "id": "AHerD-9N4_dE",
        "outputId": "2ac603d8-632d-4bd3-eaa3-3652d4292d2e"
      },
      "id": "AHerD-9N4_dE",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.6737 - accuracy: 0.8264\n",
            "Epoch 1: val_loss improved from inf to 0.63237, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 40s 16ms/step - loss: 0.6737 - accuracy: 0.8264 - val_loss: 0.6324 - val_accuracy: 0.8315 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "   9/2189 [..............................] - ETA: 30s - loss: 0.6807 - accuracy: 0.8264"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2189/2189 [==============================] - ETA: 0s - loss: 0.6088 - accuracy: 0.8256\n",
            "Epoch 2: val_loss improved from 0.63237 to 0.52314, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 34s 16ms/step - loss: 0.6088 - accuracy: 0.8256 - val_loss: 0.5231 - val_accuracy: 0.8480 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.4233 - accuracy: 0.8837\n",
            "Epoch 3: val_loss improved from 0.52314 to 0.35900, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 34s 16ms/step - loss: 0.4231 - accuracy: 0.8837 - val_loss: 0.3590 - val_accuracy: 0.9077 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.3893 - accuracy: 0.8915\n",
            "Epoch 4: val_loss improved from 0.35900 to 0.32623, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 34s 16ms/step - loss: 0.3893 - accuracy: 0.8915 - val_loss: 0.3262 - val_accuracy: 0.9111 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.4020 - accuracy: 0.8864\n",
            "Epoch 5: val_loss did not improve from 0.32623\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.4020 - accuracy: 0.8864 - val_loss: 0.3552 - val_accuracy: 0.8975 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.2987 - accuracy: 0.9207\n",
            "Epoch 6: val_loss improved from 0.32623 to 0.23744, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 34s 16ms/step - loss: 0.2985 - accuracy: 0.9208 - val_loss: 0.2374 - val_accuracy: 0.9355 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.2378 - accuracy: 0.9364\n",
            "Epoch 7: val_loss improved from 0.23744 to 0.20461, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.2378 - accuracy: 0.9364 - val_loss: 0.2046 - val_accuracy: 0.9390 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1996 - accuracy: 0.9456\n",
            "Epoch 8: val_loss improved from 0.20461 to 0.17193, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 34s 16ms/step - loss: 0.1996 - accuracy: 0.9456 - val_loss: 0.1719 - val_accuracy: 0.9563 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1775 - accuracy: 0.9510\n",
            "Epoch 9: val_loss did not improve from 0.17193\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.1775 - accuracy: 0.9510 - val_loss: 0.1942 - val_accuracy: 0.9480 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.1647 - accuracy: 0.9545\n",
            "Epoch 10: val_loss improved from 0.17193 to 0.14705, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.1646 - accuracy: 0.9545 - val_loss: 0.1470 - val_accuracy: 0.9598 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1560 - accuracy: 0.9575\n",
            "Epoch 11: val_loss improved from 0.14705 to 0.12402, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.1560 - accuracy: 0.9575 - val_loss: 0.1240 - val_accuracy: 0.9653 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1348 - accuracy: 0.9630\n",
            "Epoch 12: val_loss did not improve from 0.12402\n",
            "2189/2189 [==============================] - 34s 16ms/step - loss: 0.1348 - accuracy: 0.9630 - val_loss: 0.1256 - val_accuracy: 0.9624 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1267 - accuracy: 0.9655\n",
            "Epoch 13: val_loss improved from 0.12402 to 0.11173, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.1267 - accuracy: 0.9655 - val_loss: 0.1117 - val_accuracy: 0.9701 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.1176 - accuracy: 0.9688\n",
            "Epoch 14: val_loss improved from 0.11173 to 0.10446, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.1175 - accuracy: 0.9688 - val_loss: 0.1045 - val_accuracy: 0.9702 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1120 - accuracy: 0.9697\n",
            "Epoch 15: val_loss did not improve from 0.10446\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.1120 - accuracy: 0.9697 - val_loss: 0.1277 - val_accuracy: 0.9655 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.1054 - accuracy: 0.9724\n",
            "Epoch 16: val_loss improved from 0.10446 to 0.09685, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.1054 - accuracy: 0.9724 - val_loss: 0.0968 - val_accuracy: 0.9757 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.1004 - accuracy: 0.9731\n",
            "Epoch 17: val_loss did not improve from 0.09685\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.1004 - accuracy: 0.9731 - val_loss: 0.1196 - val_accuracy: 0.9624 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0992 - accuracy: 0.9730\n",
            "Epoch 18: val_loss did not improve from 0.09685\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0992 - accuracy: 0.9730 - val_loss: 0.0975 - val_accuracy: 0.9749 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0917 - accuracy: 0.9761\n",
            "Epoch 19: val_loss did not improve from 0.09685\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0918 - accuracy: 0.9761 - val_loss: 0.0991 - val_accuracy: 0.9752 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0853 - accuracy: 0.9767\n",
            "Epoch 20: val_loss improved from 0.09685 to 0.08751, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 33s 15ms/step - loss: 0.0853 - accuracy: 0.9767 - val_loss: 0.0875 - val_accuracy: 0.9760 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.0866 - accuracy: 0.9767\n",
            "Epoch 21: val_loss improved from 0.08751 to 0.08509, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0866 - accuracy: 0.9767 - val_loss: 0.0851 - val_accuracy: 0.9774 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0838 - accuracy: 0.9768\n",
            "Epoch 22: val_loss did not improve from 0.08509\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0838 - accuracy: 0.9768 - val_loss: 0.0922 - val_accuracy: 0.9753 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9791\n",
            "Epoch 23: val_loss improved from 0.08509 to 0.08225, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0781 - accuracy: 0.9791 - val_loss: 0.0822 - val_accuracy: 0.9779 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0762 - accuracy: 0.9789\n",
            "Epoch 24: val_loss did not improve from 0.08225\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0762 - accuracy: 0.9789 - val_loss: 0.0839 - val_accuracy: 0.9780 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0746 - accuracy: 0.9789\n",
            "Epoch 25: val_loss improved from 0.08225 to 0.08057, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0745 - accuracy: 0.9789 - val_loss: 0.0806 - val_accuracy: 0.9778 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9788\n",
            "Epoch 26: val_loss improved from 0.08057 to 0.08022, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0753 - accuracy: 0.9788 - val_loss: 0.0802 - val_accuracy: 0.9768 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9804\n",
            "Epoch 27: val_loss improved from 0.08022 to 0.07321, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0691 - accuracy: 0.9804 - val_loss: 0.0732 - val_accuracy: 0.9808 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9810\n",
            "Epoch 28: val_loss improved from 0.07321 to 0.07025, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0686 - accuracy: 0.9810 - val_loss: 0.0702 - val_accuracy: 0.9808 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0656 - accuracy: 0.9814\n",
            "Epoch 29: val_loss did not improve from 0.07025\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0655 - accuracy: 0.9814 - val_loss: 0.0733 - val_accuracy: 0.9797 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0643 - accuracy: 0.9815\n",
            "Epoch 30: val_loss did not improve from 0.07025\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0643 - accuracy: 0.9815 - val_loss: 0.0718 - val_accuracy: 0.9807 - lr: 0.0010\n",
            "Epoch 31/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0597 - accuracy: 0.9827\n",
            "Epoch 31: val_loss improved from 0.07025 to 0.06771, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0597 - accuracy: 0.9827 - val_loss: 0.0677 - val_accuracy: 0.9817 - lr: 0.0010\n",
            "Epoch 32/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0625 - accuracy: 0.9822\n",
            "Epoch 32: val_loss did not improve from 0.06771\n",
            "2189/2189 [==============================] - 33s 15ms/step - loss: 0.0625 - accuracy: 0.9823 - val_loss: 0.0735 - val_accuracy: 0.9810 - lr: 0.0010\n",
            "Epoch 33/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0596 - accuracy: 0.9827\n",
            "Epoch 33: val_loss did not improve from 0.06771\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0596 - accuracy: 0.9827 - val_loss: 0.0683 - val_accuracy: 0.9819 - lr: 0.0010\n",
            "Epoch 34/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9833\n",
            "Epoch 34: val_loss did not improve from 0.06771\n",
            "2189/2189 [==============================] - 33s 15ms/step - loss: 0.0577 - accuracy: 0.9833 - val_loss: 0.0701 - val_accuracy: 0.9807 - lr: 0.0010\n",
            "Epoch 35/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0567 - accuracy: 0.9834\n",
            "Epoch 35: val_loss improved from 0.06771 to 0.06583, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0567 - accuracy: 0.9834 - val_loss: 0.0658 - val_accuracy: 0.9825 - lr: 0.0010\n",
            "Epoch 36/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0552 - accuracy: 0.9837\n",
            "Epoch 36: val_loss improved from 0.06583 to 0.06124, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0552 - accuracy: 0.9837 - val_loss: 0.0612 - val_accuracy: 0.9824 - lr: 0.0010\n",
            "Epoch 37/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0531 - accuracy: 0.9841\n",
            "Epoch 37: val_loss did not improve from 0.06124\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0531 - accuracy: 0.9841 - val_loss: 0.0688 - val_accuracy: 0.9818 - lr: 0.0010\n",
            "Epoch 38/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9835\n",
            "Epoch 38: val_loss did not improve from 0.06124\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0547 - accuracy: 0.9835 - val_loss: 0.0681 - val_accuracy: 0.9810 - lr: 0.0010\n",
            "Epoch 39/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0504 - accuracy: 0.9848\n",
            "Epoch 39: val_loss did not improve from 0.06124\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0504 - accuracy: 0.9848 - val_loss: 0.0618 - val_accuracy: 0.9845 - lr: 0.0010\n",
            "Epoch 40/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0497 - accuracy: 0.9855\n",
            "Epoch 40: val_loss did not improve from 0.06124\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0497 - accuracy: 0.9855 - val_loss: 0.0683 - val_accuracy: 0.9829 - lr: 0.0010\n",
            "Epoch 41/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0504 - accuracy: 0.9851\n",
            "Epoch 41: val_loss did not improve from 0.06124\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0504 - accuracy: 0.9851 - val_loss: 0.0657 - val_accuracy: 0.9839 - lr: 0.0010\n",
            "Epoch 42/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0484 - accuracy: 0.9856\n",
            "Epoch 42: val_loss did not improve from 0.06124\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0484 - accuracy: 0.9856 - val_loss: 0.0675 - val_accuracy: 0.9829 - lr: 0.0010\n",
            "Epoch 43/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0459 - accuracy: 0.9861\n",
            "Epoch 43: val_loss did not improve from 0.06124\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0459 - accuracy: 0.9861 - val_loss: 0.0613 - val_accuracy: 0.9823 - lr: 0.0010\n",
            "Epoch 44/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0476 - accuracy: 0.9857\n",
            "Epoch 44: val_loss improved from 0.06124 to 0.06002, saving model to best_model_LSTM.h5\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0476 - accuracy: 0.9857 - val_loss: 0.0600 - val_accuracy: 0.9844 - lr: 0.0010\n",
            "Epoch 45/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9863\n",
            "Epoch 45: val_loss did not improve from 0.06002\n",
            "2189/2189 [==============================] - 33s 15ms/step - loss: 0.0439 - accuracy: 0.9863 - val_loss: 0.0680 - val_accuracy: 0.9825 - lr: 0.0010\n",
            "Epoch 46/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9777\n",
            "Epoch 46: val_loss did not improve from 0.06002\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0784 - accuracy: 0.9777 - val_loss: 0.0643 - val_accuracy: 0.9830 - lr: 0.0010\n",
            "Epoch 47/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9861\n",
            "Epoch 47: val_loss did not improve from 0.06002\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0461 - accuracy: 0.9861 - val_loss: 0.0650 - val_accuracy: 0.9830 - lr: 0.0010\n",
            "Epoch 48/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9863\n",
            "Epoch 48: val_loss did not improve from 0.06002\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0440 - accuracy: 0.9863 - val_loss: 0.0617 - val_accuracy: 0.9832 - lr: 0.0010\n",
            "Epoch 49/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0446 - accuracy: 0.9868\n",
            "Epoch 49: val_loss did not improve from 0.06002\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0446 - accuracy: 0.9868 - val_loss: 0.0647 - val_accuracy: 0.9820 - lr: 0.0010\n",
            "Epoch 50/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9866\n",
            "Epoch 50: val_loss did not improve from 0.06002\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0422 - accuracy: 0.9866 - val_loss: 0.0672 - val_accuracy: 0.9829 - lr: 0.0010\n",
            "Epoch 51/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9875\n",
            "Epoch 51: val_loss did not improve from 0.06002\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0401 - accuracy: 0.9875 - val_loss: 0.0633 - val_accuracy: 0.9829 - lr: 0.0010\n",
            "Epoch 52/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.9878\n",
            "Epoch 52: val_loss did not improve from 0.06002\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0396 - accuracy: 0.9878 - val_loss: 0.0642 - val_accuracy: 0.9841 - lr: 0.0010\n",
            "Epoch 53/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.9879\n",
            "Epoch 53: val_loss did not improve from 0.06002\n",
            "2189/2189 [==============================] - 33s 15ms/step - loss: 0.0386 - accuracy: 0.9879 - val_loss: 0.0673 - val_accuracy: 0.9841 - lr: 0.0010\n",
            "Epoch 54/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0393 - accuracy: 0.9880Restoring model weights from the end of the best epoch: 44.\n",
            "\n",
            "Epoch 54: val_loss did not improve from 0.06002\n",
            "2189/2189 [==============================] - 34s 15ms/step - loss: 0.0393 - accuracy: 0.9880 - val_loss: 0.0624 - val_accuracy: 0.9842 - lr: 0.0010\n",
            "Epoch 54: early stopping\n",
            "548/548 - 3s - loss: 0.0600 - accuracy: 0.9844 - 3s/epoch - 6ms/step\n",
            "Test Loss: 0.0600249320268631\n",
            "Test Accuracy: 0.9844098091125488\n",
            "548/548 [==============================] - 4s 6ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99     14579\n",
            "           1       0.88      0.75      0.81       426\n",
            "           2       0.97      0.94      0.96      1112\n",
            "           3       0.86      0.79      0.82       145\n",
            "           4       0.99      0.99      0.99      1249\n",
            "\n",
            "    accuracy                           0.98     17511\n",
            "   macro avg       0.94      0.89      0.91     17511\n",
            "weighted avg       0.98      0.98      0.98     17511\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'confusion_matrix' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1add8728422f>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# Confusion Matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Blues\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying bidirectionality\n"
      ],
      "metadata": {
        "id": "pmy-STd3p8yz"
      },
      "id": "pmy-STd3p8yz"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_model_LSTM_bidirectional():\n",
        "    input_shape = (187, 1)  # Assuming this is your input shape based on the dataset\n",
        "    model = Sequential([\n",
        "        # Wrapping the LSTM layer with Bidirectional\n",
        "        Bidirectional(LSTM(units=128, return_sequences=True), input_shape=input_shape),\n",
        "        Dropout(0.1),\n",
        "\n",
        "        # Adding another Bidirectional LSTM layer\n",
        "        Bidirectional(LSTM(units=64, return_sequences=False)),\n",
        "        Dropout(0.1),\n",
        "\n",
        "        # Dense layer as before\n",
        "        Dense(units=150, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(5, activation='softmax')  # Adjusted for 5 classes\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the bidirectional LSTM model\n",
        "model_LSTM_bidirectional = create_model_LSTM_bidirectional()\n",
        "\n",
        "# Now you can use model_LSTM_bidirectional for training just like you did with the original model\n"
      ],
      "metadata": {
        "id": "ANmtVjffkqjD"
      },
      "id": "ANmtVjffkqjD",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "# No need to load the best model immediately after training since we're using callbacks to manage this\n",
        "\n",
        "# Assuming fazeli_mitbih_train_df is your DataFrame and the last column is the label\n",
        "X = fazeli_mitbih_train_df.iloc[:, :-1].values\n",
        "y = fazeli_mitbih_train_df.iloc[:, -1].values\n",
        "\n",
        "# Reshape X to fit the LSTM input requirements and convert y to categorical\n",
        "X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "y_categorical = to_categorical(y)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define callbacks\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'best_model_LSTM_bidirectional.h5',  # Updated filename to reflect it's the bidirectional LSTM model\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=5,\n",
        "    min_lr=0.001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Create the bidirectional LSTM model with the best hyperparameters\n",
        "# Assuming 'create_model_LSTM_bidirectional()' function is defined and returns the bidirectional LSTM model\n",
        "model_LSTM_bidirectional = create_model_LSTM_bidirectional()\n",
        "\n",
        "# Train the bidirectional LSTM model with the callbacks\n",
        "history_LSTM_bidirectional = model_LSTM_bidirectional.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=100,  # Adjust the number of epochs as necessary\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, reduce_lr, model_checkpoint]\n",
        ")\n",
        "\n",
        "# Optionally, load the best model saved during training\n",
        "# best_model_LSTM_bidirectional = load_model('best_model_LSTM_bidirectional.h5')\n",
        "\n",
        "# Evaluation on Test Data\n",
        "test_loss, test_accuracy = model_LSTM_bidirectional.evaluate(X_test, y_test, verbose=2)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Predictions and Classification Report\n",
        "y_pred = model_LSTM_bidirectional.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2392
        },
        "id": "c33e8BatqEJk",
        "outputId": "5c2ca63c-b680-4ceb-a6f4-747b48b23c9e"
      },
      "id": "c33e8BatqEJk",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.3221 - accuracy: 0.9170\n",
            "Epoch 1: val_loss improved from inf to 0.17689, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 80s 33ms/step - loss: 0.3221 - accuracy: 0.9170 - val_loss: 0.1769 - val_accuracy: 0.9508 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "   5/2189 [..............................] - ETA: 1:00 - loss: 0.1799 - accuracy: 0.9563"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1750 - accuracy: 0.9525\n",
            "Epoch 2: val_loss improved from 0.17689 to 0.15195, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.1750 - accuracy: 0.9525 - val_loss: 0.1519 - val_accuracy: 0.9587 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1398 - accuracy: 0.9618\n",
            "Epoch 3: val_loss improved from 0.15195 to 0.11292, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.1398 - accuracy: 0.9618 - val_loss: 0.1129 - val_accuracy: 0.9660 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1225 - accuracy: 0.9663\n",
            "Epoch 4: val_loss improved from 0.11292 to 0.10153, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.1225 - accuracy: 0.9663 - val_loss: 0.1015 - val_accuracy: 0.9718 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1098 - accuracy: 0.9692\n",
            "Epoch 5: val_loss improved from 0.10153 to 0.10116, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.1098 - accuracy: 0.9692 - val_loss: 0.1012 - val_accuracy: 0.9718 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1002 - accuracy: 0.9717\n",
            "Epoch 6: val_loss improved from 0.10116 to 0.08409, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.1002 - accuracy: 0.9717 - val_loss: 0.0841 - val_accuracy: 0.9754 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0923 - accuracy: 0.9736\n",
            "Epoch 7: val_loss improved from 0.08409 to 0.07696, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 64s 29ms/step - loss: 0.0924 - accuracy: 0.9736 - val_loss: 0.0770 - val_accuracy: 0.9772 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0863 - accuracy: 0.9756\n",
            "Epoch 8: val_loss improved from 0.07696 to 0.07347, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 64s 29ms/step - loss: 0.0863 - accuracy: 0.9756 - val_loss: 0.0735 - val_accuracy: 0.9792 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9772\n",
            "Epoch 9: val_loss did not improve from 0.07347\n",
            "2189/2189 [==============================] - 65s 30ms/step - loss: 0.0794 - accuracy: 0.9772 - val_loss: 0.0785 - val_accuracy: 0.9762 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9783\n",
            "Epoch 10: val_loss improved from 0.07347 to 0.06629, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 65s 30ms/step - loss: 0.0749 - accuracy: 0.9783 - val_loss: 0.0663 - val_accuracy: 0.9814 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9795\n",
            "Epoch 11: val_loss improved from 0.06629 to 0.06365, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 65s 30ms/step - loss: 0.0695 - accuracy: 0.9795 - val_loss: 0.0637 - val_accuracy: 0.9833 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0653 - accuracy: 0.9811\n",
            "Epoch 12: val_loss did not improve from 0.06365\n",
            "2189/2189 [==============================] - 64s 29ms/step - loss: 0.0653 - accuracy: 0.9811 - val_loss: 0.0669 - val_accuracy: 0.9805 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0615 - accuracy: 0.9812\n",
            "Epoch 13: val_loss improved from 0.06365 to 0.05858, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 65s 30ms/step - loss: 0.0615 - accuracy: 0.9812 - val_loss: 0.0586 - val_accuracy: 0.9842 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9825\n",
            "Epoch 14: val_loss did not improve from 0.05858\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0581 - accuracy: 0.9825 - val_loss: 0.0588 - val_accuracy: 0.9829 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0549 - accuracy: 0.9839\n",
            "Epoch 15: val_loss did not improve from 0.05858\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0549 - accuracy: 0.9839 - val_loss: 0.0601 - val_accuracy: 0.9824 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9840\n",
            "Epoch 16: val_loss improved from 0.05858 to 0.05480, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0526 - accuracy: 0.9840 - val_loss: 0.0548 - val_accuracy: 0.9849 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.9848\n",
            "Epoch 17: val_loss improved from 0.05480 to 0.04872, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0514 - accuracy: 0.9848 - val_loss: 0.0487 - val_accuracy: 0.9858 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9852\n",
            "Epoch 18: val_loss did not improve from 0.04872\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0482 - accuracy: 0.9852 - val_loss: 0.0517 - val_accuracy: 0.9858 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9855\n",
            "Epoch 19: val_loss did not improve from 0.04872\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0468 - accuracy: 0.9855 - val_loss: 0.0550 - val_accuracy: 0.9848 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0447 - accuracy: 0.9868\n",
            "Epoch 20: val_loss did not improve from 0.04872\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.0447 - accuracy: 0.9868 - val_loss: 0.0623 - val_accuracy: 0.9842 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9867\n",
            "Epoch 21: val_loss did not improve from 0.04872\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.0432 - accuracy: 0.9867 - val_loss: 0.0528 - val_accuracy: 0.9853 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.9874\n",
            "Epoch 22: val_loss did not improve from 0.04872\n",
            "2189/2189 [==============================] - 67s 30ms/step - loss: 0.0408 - accuracy: 0.9874 - val_loss: 0.0540 - val_accuracy: 0.9850 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9874\n",
            "Epoch 23: val_loss did not improve from 0.04872\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0404 - accuracy: 0.9874 - val_loss: 0.0589 - val_accuracy: 0.9853 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9876\n",
            "Epoch 24: val_loss did not improve from 0.04872\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0392 - accuracy: 0.9876 - val_loss: 0.0500 - val_accuracy: 0.9870 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 0.9885\n",
            "Epoch 25: val_loss did not improve from 0.04872\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0372 - accuracy: 0.9885 - val_loss: 0.0647 - val_accuracy: 0.9830 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 0.9883\n",
            "Epoch 26: val_loss did not improve from 0.04872\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0372 - accuracy: 0.9883 - val_loss: 0.0544 - val_accuracy: 0.9861 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9889Restoring model weights from the end of the best epoch: 17.\n",
            "\n",
            "Epoch 27: val_loss did not improve from 0.04872\n",
            "2189/2189 [==============================] - 67s 30ms/step - loss: 0.0355 - accuracy: 0.9889 - val_loss: 0.0542 - val_accuracy: 0.9866 - lr: 0.0010\n",
            "Epoch 27: early stopping\n",
            "548/548 - 6s - loss: 0.0487 - accuracy: 0.9858 - 6s/epoch - 11ms/step\n",
            "Test Loss: 0.04872136563062668\n",
            "Test Accuracy: 0.9857803583145142\n",
            "548/548 [==============================] - 7s 11ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99     14579\n",
            "           1       0.95      0.78      0.86       426\n",
            "           2       0.97      0.95      0.96      1112\n",
            "           3       0.83      0.76      0.79       145\n",
            "           4       0.99      0.99      0.99      1249\n",
            "\n",
            "    accuracy                           0.99     17511\n",
            "   macro avg       0.94      0.90      0.92     17511\n",
            "weighted avg       0.99      0.99      0.99     17511\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'confusion_matrix' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-952ef2396539>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# Confusion Matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Blues\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "bidirectionality with another layer"
      ],
      "metadata": {
        "id": "Z4LhiQSg5TDO"
      },
      "id": "Z4LhiQSg5TDO"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_model_LSTM_bidirectional_2():\n",
        "    input_shape = (187, 1)  # Assuming this is your input shape based on the dataset\n",
        "    model = Sequential([\n",
        "        Bidirectional(LSTM(units=128, return_sequences=True), input_shape=input_shape),\n",
        "        Dropout(0.1),\n",
        "\n",
        "        Bidirectional(LSTM(units=64, return_sequences=True)),\n",
        "        Dropout(0.1),\n",
        "\n",
        "        Bidirectional(LSTM(units=32, return_sequences=False)),  # Additional layer\n",
        "        Dropout(0.1),\n",
        "\n",
        "        Dense(units=150, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(5, activation='softmax')  # Adjusted for 5 classes\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the LSTM model with two bidirectional layers\n",
        "model_LSTM_bidirectional_2 = create_model_LSTM_bidirectional_2()\n"
      ],
      "metadata": {
        "id": "ffDZ68ERspXJ"
      },
      "id": "ffDZ68ERspXJ",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Assuming fazeli_mitbih_train_df is your DataFrame and the last column is the label\n",
        "X = fazeli_mitbih_train_df.iloc[:, :-1].values\n",
        "y = fazeli_mitbih_train_df.iloc[:, -1].values\n",
        "\n",
        "# Reshape X to fit the LSTM input requirements and convert y to categorical\n",
        "X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "y_categorical = to_categorical(y)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define callbacks\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'best_model_LSTM_bidirectional_2.h5',  # Updated filename for the new bidirectional model\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=5,\n",
        "    min_lr=0.001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Assuming 'create_model_LSTM_bidirectional_2()' function is defined and returns the bidirectional LSTM model\n",
        "model_LSTM_bidirectional_2 = create_model_LSTM_bidirectional_2()\n",
        "\n",
        "# Train the bidirectional LSTM model with the callbacks\n",
        "history_LSTM_bidirectional_2 = model_LSTM_bidirectional_2.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=100,  # Adjust the number of epochs as necessary\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, reduce_lr, model_checkpoint]\n",
        ")\n",
        "\n",
        "# Optionally, load the best model saved during training\n",
        "# best_model_LSTM_bidirectional_2 = load_model('best_model_LSTM_bidirectional_2.h5')\n",
        "\n",
        "# Evaluation on Test Data\n",
        "test_loss, test_accuracy = model_LSTM_bidirectional_2.evaluate(X_test, y_test, verbose=2)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Predictions and Classification Report\n",
        "y_pred = model_LSTM_bidirectional_2.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vk8wSUZL5av-",
        "outputId": "3af31476-2c85-4401-dca5-d632af874ac6"
      },
      "id": "vk8wSUZL5av-",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.4754 - accuracy: 0.8748\n",
            "Epoch 1: val_loss improved from inf to 0.24721, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 103s 43ms/step - loss: 0.4754 - accuracy: 0.8748 - val_loss: 0.2472 - val_accuracy: 0.9373 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "   3/2189 [..............................] - ETA: 1:21 - loss: 0.2075 - accuracy: 0.9479"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2189/2189 [==============================] - ETA: 0s - loss: 0.2653 - accuracy: 0.9314\n",
            "Epoch 2: val_loss did not improve from 0.24721\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.2653 - accuracy: 0.9314 - val_loss: 0.3246 - val_accuracy: 0.9130 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1852 - accuracy: 0.9490\n",
            "Epoch 3: val_loss improved from 0.24721 to 0.13139, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.1852 - accuracy: 0.9490 - val_loss: 0.1314 - val_accuracy: 0.9620 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1505 - accuracy: 0.9572\n",
            "Epoch 4: val_loss improved from 0.13139 to 0.11245, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 89s 41ms/step - loss: 0.1505 - accuracy: 0.9572 - val_loss: 0.1125 - val_accuracy: 0.9684 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1284 - accuracy: 0.9645\n",
            "Epoch 5: val_loss improved from 0.11245 to 0.11102, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.1284 - accuracy: 0.9645 - val_loss: 0.1110 - val_accuracy: 0.9685 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1184 - accuracy: 0.9672\n",
            "Epoch 6: val_loss improved from 0.11102 to 0.09606, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.1184 - accuracy: 0.9672 - val_loss: 0.0961 - val_accuracy: 0.9730 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1073 - accuracy: 0.9703\n",
            "Epoch 7: val_loss did not improve from 0.09606\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.1073 - accuracy: 0.9703 - val_loss: 0.1004 - val_accuracy: 0.9716 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1001 - accuracy: 0.9721\n",
            "Epoch 8: val_loss improved from 0.09606 to 0.08442, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.1001 - accuracy: 0.9721 - val_loss: 0.0844 - val_accuracy: 0.9765 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0921 - accuracy: 0.9742\n",
            "Epoch 9: val_loss improved from 0.08442 to 0.07346, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.0921 - accuracy: 0.9742 - val_loss: 0.0735 - val_accuracy: 0.9785 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0861 - accuracy: 0.9766\n",
            "Epoch 10: val_loss did not improve from 0.07346\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.0861 - accuracy: 0.9766 - val_loss: 0.0744 - val_accuracy: 0.9782 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0806 - accuracy: 0.9773\n",
            "Epoch 11: val_loss improved from 0.07346 to 0.06918, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.0806 - accuracy: 0.9773 - val_loss: 0.0692 - val_accuracy: 0.9806 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0888 - accuracy: 0.9752\n",
            "Epoch 12: val_loss did not improve from 0.06918\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.0888 - accuracy: 0.9752 - val_loss: 0.0784 - val_accuracy: 0.9781 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0810 - accuracy: 0.9775\n",
            "Epoch 13: val_loss did not improve from 0.06918\n",
            "2189/2189 [==============================] - 91s 41ms/step - loss: 0.0810 - accuracy: 0.9775 - val_loss: 0.0705 - val_accuracy: 0.9793 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9795\n",
            "Epoch 14: val_loss improved from 0.06918 to 0.06808, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.0710 - accuracy: 0.9795 - val_loss: 0.0681 - val_accuracy: 0.9798 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9809\n",
            "Epoch 15: val_loss improved from 0.06808 to 0.06598, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.0671 - accuracy: 0.9809 - val_loss: 0.0660 - val_accuracy: 0.9814 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0646 - accuracy: 0.9809\n",
            "Epoch 16: val_loss improved from 0.06598 to 0.06040, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 95s 43ms/step - loss: 0.0646 - accuracy: 0.9809 - val_loss: 0.0604 - val_accuracy: 0.9828 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9824\n",
            "Epoch 17: val_loss did not improve from 0.06040\n",
            "2189/2189 [==============================] - 95s 43ms/step - loss: 0.0613 - accuracy: 0.9824 - val_loss: 0.0607 - val_accuracy: 0.9832 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9823\n",
            "Epoch 18: val_loss improved from 0.06040 to 0.05732, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 95s 43ms/step - loss: 0.0604 - accuracy: 0.9823 - val_loss: 0.0573 - val_accuracy: 0.9834 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9831\n",
            "Epoch 19: val_loss did not improve from 0.05732\n",
            "2189/2189 [==============================] - 95s 43ms/step - loss: 0.0578 - accuracy: 0.9831 - val_loss: 0.0599 - val_accuracy: 0.9830 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0545 - accuracy: 0.9840\n",
            "Epoch 20: val_loss improved from 0.05732 to 0.05629, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 95s 43ms/step - loss: 0.0545 - accuracy: 0.9840 - val_loss: 0.0563 - val_accuracy: 0.9848 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0530 - accuracy: 0.9849\n",
            "Epoch 21: val_loss did not improve from 0.05629\n",
            "2189/2189 [==============================] - 95s 43ms/step - loss: 0.0530 - accuracy: 0.9849 - val_loss: 0.0631 - val_accuracy: 0.9824 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9844\n",
            "Epoch 22: val_loss improved from 0.05629 to 0.05616, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 95s 43ms/step - loss: 0.0520 - accuracy: 0.9844 - val_loss: 0.0562 - val_accuracy: 0.9845 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0498 - accuracy: 0.9853\n",
            "Epoch 23: val_loss did not improve from 0.05616\n",
            "2189/2189 [==============================] - 95s 43ms/step - loss: 0.0498 - accuracy: 0.9853 - val_loss: 0.0564 - val_accuracy: 0.9851 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9859\n",
            "Epoch 24: val_loss improved from 0.05616 to 0.05340, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 94s 43ms/step - loss: 0.0471 - accuracy: 0.9859 - val_loss: 0.0534 - val_accuracy: 0.9850 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9867\n",
            "Epoch 25: val_loss improved from 0.05340 to 0.05186, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 95s 43ms/step - loss: 0.0467 - accuracy: 0.9867 - val_loss: 0.0519 - val_accuracy: 0.9854 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9866\n",
            "Epoch 26: val_loss did not improve from 0.05186\n",
            "2189/2189 [==============================] - 94s 43ms/step - loss: 0.0448 - accuracy: 0.9866 - val_loss: 0.0569 - val_accuracy: 0.9840 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9870\n",
            "Epoch 27: val_loss did not improve from 0.05186\n",
            "2189/2189 [==============================] - 91s 42ms/step - loss: 0.0440 - accuracy: 0.9870 - val_loss: 0.0652 - val_accuracy: 0.9843 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0419 - accuracy: 0.9874\n",
            "Epoch 28: val_loss did not improve from 0.05186\n",
            "2189/2189 [==============================] - 91s 41ms/step - loss: 0.0419 - accuracy: 0.9874 - val_loss: 0.0590 - val_accuracy: 0.9850 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0416 - accuracy: 0.9878\n",
            "Epoch 29: val_loss did not improve from 0.05186\n",
            "2189/2189 [==============================] - 91s 41ms/step - loss: 0.0416 - accuracy: 0.9878 - val_loss: 0.0544 - val_accuracy: 0.9862 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0409 - accuracy: 0.9876\n",
            "Epoch 30: val_loss did not improve from 0.05186\n",
            "2189/2189 [==============================] - 91s 42ms/step - loss: 0.0409 - accuracy: 0.9876 - val_loss: 0.0889 - val_accuracy: 0.9732 - lr: 0.0010\n",
            "Epoch 31/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9879\n",
            "Epoch 31: val_loss did not improve from 0.05186\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.0392 - accuracy: 0.9879 - val_loss: 0.0563 - val_accuracy: 0.9846 - lr: 0.0010\n",
            "Epoch 32/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0383 - accuracy: 0.9881\n",
            "Epoch 32: val_loss did not improve from 0.05186\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.0383 - accuracy: 0.9881 - val_loss: 0.0591 - val_accuracy: 0.9845 - lr: 0.0010\n",
            "Epoch 33/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9886\n",
            "Epoch 33: val_loss did not improve from 0.05186\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.0364 - accuracy: 0.9886 - val_loss: 0.0522 - val_accuracy: 0.9866 - lr: 0.0010\n",
            "Epoch 34/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0374 - accuracy: 0.9880\n",
            "Epoch 34: val_loss did not improve from 0.05186\n",
            "2189/2189 [==============================] - 91s 42ms/step - loss: 0.0374 - accuracy: 0.9880 - val_loss: 0.0606 - val_accuracy: 0.9855 - lr: 0.0010\n",
            "Epoch 35/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9889\n",
            "Epoch 35: val_loss improved from 0.05186 to 0.04646, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 89s 40ms/step - loss: 0.0363 - accuracy: 0.9889 - val_loss: 0.0465 - val_accuracy: 0.9863 - lr: 0.0010\n",
            "Epoch 36/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0373 - accuracy: 0.9885\n",
            "Epoch 36: val_loss did not improve from 0.04646\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.0373 - accuracy: 0.9885 - val_loss: 0.0519 - val_accuracy: 0.9870 - lr: 0.0010\n",
            "Epoch 37/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9895\n",
            "Epoch 37: val_loss did not improve from 0.04646\n",
            "2189/2189 [==============================] - 88s 40ms/step - loss: 0.0338 - accuracy: 0.9895 - val_loss: 0.0568 - val_accuracy: 0.9848 - lr: 0.0010\n",
            "Epoch 38/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9898\n",
            "Epoch 38: val_loss did not improve from 0.04646\n",
            "2189/2189 [==============================] - 89s 40ms/step - loss: 0.0328 - accuracy: 0.9898 - val_loss: 0.0492 - val_accuracy: 0.9862 - lr: 0.0010\n",
            "Epoch 39/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9896\n",
            "Epoch 39: val_loss did not improve from 0.04646\n",
            "2189/2189 [==============================] - 89s 41ms/step - loss: 0.0331 - accuracy: 0.9896 - val_loss: 0.0515 - val_accuracy: 0.9861 - lr: 0.0010\n",
            "Epoch 40/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9897\n",
            "Epoch 40: val_loss did not improve from 0.04646\n",
            "2189/2189 [==============================] - 89s 41ms/step - loss: 0.0317 - accuracy: 0.9897 - val_loss: 0.0507 - val_accuracy: 0.9867 - lr: 0.0010\n",
            "Epoch 41/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9898\n",
            "Epoch 41: val_loss did not improve from 0.04646\n",
            "2189/2189 [==============================] - 88s 40ms/step - loss: 0.0310 - accuracy: 0.9898 - val_loss: 0.0526 - val_accuracy: 0.9861 - lr: 0.0010\n",
            "Epoch 42/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9900\n",
            "Epoch 42: val_loss did not improve from 0.04646\n",
            "2189/2189 [==============================] - 88s 40ms/step - loss: 0.0314 - accuracy: 0.9900 - val_loss: 0.0538 - val_accuracy: 0.9857 - lr: 0.0010\n",
            "Epoch 43/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9898\n",
            "Epoch 43: val_loss did not improve from 0.04646\n",
            "2189/2189 [==============================] - 89s 41ms/step - loss: 0.0312 - accuracy: 0.9898 - val_loss: 0.0499 - val_accuracy: 0.9863 - lr: 0.0010\n",
            "Epoch 44/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9913\n",
            "Epoch 44: val_loss did not improve from 0.04646\n",
            "2189/2189 [==============================] - 88s 40ms/step - loss: 0.0274 - accuracy: 0.9913 - val_loss: 0.0557 - val_accuracy: 0.9877 - lr: 0.0010\n",
            "Epoch 45/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9905Restoring model weights from the end of the best epoch: 35.\n",
            "\n",
            "Epoch 45: val_loss did not improve from 0.04646\n",
            "2189/2189 [==============================] - 89s 41ms/step - loss: 0.0293 - accuracy: 0.9905 - val_loss: 0.0542 - val_accuracy: 0.9871 - lr: 0.0010\n",
            "Epoch 45: early stopping\n",
            "548/548 - 8s - loss: 0.0465 - accuracy: 0.9863 - 8s/epoch - 15ms/step\n",
            "Test Loss: 0.04645908623933792\n",
            "Test Accuracy: 0.9862943291664124\n",
            "548/548 [==============================] - 10s 15ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99     14579\n",
            "           1       0.92      0.78      0.84       426\n",
            "           2       0.97      0.96      0.96      1112\n",
            "           3       0.88      0.82      0.85       145\n",
            "           4       0.99      0.99      0.99      1249\n",
            "\n",
            "    accuracy                           0.99     17511\n",
            "   macro avg       0.95      0.91      0.93     17511\n",
            "weighted avg       0.99      0.99      0.99     17511\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparamater Tuning a 2 layer Bidirectional LSTM model"
      ],
      "metadata": {
        "id": "j7jgzQNeyEvC"
      },
      "id": "j7jgzQNeyEvC"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras_tuner import HyperModel, Hyperband\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "class LSTMHyperModel(HyperModel):\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "        model.add(Bidirectional(LSTM(units=hp.Int('units_1', min_value=32, max_value=128, step=32), return_sequences=True), input_shape=self.input_shape))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_1', min_value=0.0, max_value=0.5, default=0.25, step=0.1)))\n",
        "\n",
        "        # Adding another bidirectional LSTM layer if return_sequences is set to True\n",
        "        model.add(Bidirectional(LSTM(units=hp.Int('units_2', min_value=64, max_value=256, step=32), return_sequences=False)))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_2', min_value=0.0, max_value=0.5, default=0.25, step=0.1)))\n",
        "\n",
        "        model.add(Dense(units=hp.Int('dense_units', min_value=50, max_value=150, step=50), activation='relu'))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_3', min_value=0.0, max_value=0.5, default=0.25, step=0.1)))\n",
        "\n",
        "        model.add(Dense(self.num_classes, activation='softmax'))\n",
        "\n",
        "        model.compile(optimizer=Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "# Load your data\n",
        "X = fazeli_mitbih_train_df.iloc[:, :-1].values\n",
        "y = fazeli_mitbih_train_df.iloc[:, -1].values\n",
        "\n",
        "# Preprocess the data\n",
        "X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "y_categorical = to_categorical(y)\n",
        "num_classes = y_categorical.shape[1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001, verbose=1),\n",
        "    ModelCheckpoint('best_model_LSTM_bidirectional_2.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "]\n",
        "\n",
        "# Instantiate and configure the hypermodel\n",
        "hypermodel = LSTMHyperModel(input_shape=(187, 1), num_classes=num_classes)\n",
        "\n",
        "# Instantiate the tuner\n",
        "tuner = Hyperband(\n",
        "    hypermodel,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=10,\n",
        "    directory='hyperband',\n",
        "    project_name='mitbih_lstm_classification'\n",
        ")\n",
        "\n",
        "# Start the search for the best hyperparameter configuration\n",
        "tuner.search(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=10, callbacks=callbacks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "94DJb-xN6FVc",
        "outputId": "6d3a124f-7556-4fd0-cb64-9050703a4cd6"
      },
      "id": "94DJb-xN6FVc",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 10m 53s]\n",
            "val_accuracy: 0.9740163087844849\n",
            "\n",
            "Best val_accuracy So Far: 0.9820113182067871\n",
            "Total elapsed time: 02h 18m 46s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recreate the tuner with the same configuration\n",
        "tuner = Hyperband(\n",
        "    hypermodel,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=10,\n",
        "    directory='hyperband',  # Same directory as before\n",
        "    project_name='mitbih_lstm_classification'  # Same project name as before\n",
        ")\n",
        "\n",
        "# Load the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters()[0]\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "for hp in best_hps.space:\n",
        "    print(f\"{hp.name}: {best_hps.get(hp.name)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YUfnFsdWuXkv",
        "outputId": "482c04d5-7207-49b3-9633-68059552cbcd"
      },
      "id": "YUfnFsdWuXkv",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloading Tuner from hyperband/mitbih_lstm_classification/tuner0.json\n",
            "Best Hyperparameters:\n",
            "units_1: 64\n",
            "dropout_1: 0.0\n",
            "units_2: 160\n",
            "dropout_2: 0.4\n",
            "dense_units: 100\n",
            "dropout_3: 0.2\n",
            "learning_rate: 0.002097863337902064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_model_LSTM_bidirectional_2_with_best_hps():\n",
        "    input_shape = (187, 1)  # Assuming this is your input shape based on the dataset\n",
        "    model = Sequential([\n",
        "        Bidirectional(LSTM(units=64, return_sequences=True), input_shape=input_shape),  # units_1: 64\n",
        "        Dropout(0.0),  # dropout_1: 0.0\n",
        "\n",
        "        Bidirectional(LSTM(units=160, return_sequences=False)),  # units_2: 160\n",
        "        Dropout(0.4),  # dropout_2: 0.4\n",
        "\n",
        "        Dense(units=100, activation='relu'),  # dense_units: 100\n",
        "        Dropout(0.2),  # dropout_3: 0.2\n",
        "\n",
        "        Dense(5, activation='softmax')  # Assuming 5 classes\n",
        "    ])\n",
        "\n",
        "    # learning_rate: 0.002097863337902064\n",
        "    model.compile(optimizer=Adam(learning_rate=0.002097863337902064), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the LSTM model with best hyperparameters\n",
        "model_LSTM_bidirectional_2_with_best_hps = create_model_LSTM_bidirectional_2_with_best_hps()\n"
      ],
      "metadata": {
        "id": "FcvYmJllrcwG"
      },
      "id": "FcvYmJllrcwG",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Assuming fazeli_mitbih_train_df is your DataFrame and the last column is the label\n",
        "X = fazeli_mitbih_train_df.iloc[:, :-1].values\n",
        "y = fazeli_mitbih_train_df.iloc[:, -1].values\n",
        "\n",
        "# Reshape X to fit the LSTM input requirements and convert y to categorical\n",
        "X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "y_categorical = to_categorical(y)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define callbacks\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'best_model_LSTM_bidirectional_2_with_best_hps.h5',  # Updated filename for the model with best hyperparameters\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=5,\n",
        "    min_lr=0.001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Use the model with best hyperparameters you created earlier\n",
        "model = model_LSTM_bidirectional_2_with_best_hps  # Assuming this model is already created with the function call\n",
        "\n",
        "# Train the model with the callbacks\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=100,  # Adjust the number of epochs as necessary\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, reduce_lr, model_checkpoint]\n",
        ")\n",
        "\n",
        "# Evaluation on Test Data\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Predictions and Classification Report\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes))\n",
        "\n",
        "# Confusion Matrix\n",
        "# (Add confusion matrix visualization code here if needed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jjTxpRu-riSd",
        "outputId": "902f197c-2a17-4a3c-c31b-049f23e6641e"
      },
      "id": "jjTxpRu-riSd",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.3303 - accuracy: 0.9148\n",
            "Epoch 1: val_loss improved from inf to 0.19216, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 76s 31ms/step - loss: 0.3303 - accuracy: 0.9148 - val_loss: 0.1922 - val_accuracy: 0.9460 - lr: 0.0021\n",
            "Epoch 2/100\n",
            "   5/2189 [..............................] - ETA: 1:00 - loss: 0.1781 - accuracy: 0.9563"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1739 - accuracy: 0.9529\n",
            "Epoch 2: val_loss improved from 0.19216 to 0.12433, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 65s 30ms/step - loss: 0.1739 - accuracy: 0.9529 - val_loss: 0.1243 - val_accuracy: 0.9640 - lr: 0.0021\n",
            "Epoch 3/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1412 - accuracy: 0.9607\n",
            "Epoch 3: val_loss improved from 0.12433 to 0.11655, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.1412 - accuracy: 0.9607 - val_loss: 0.1166 - val_accuracy: 0.9648 - lr: 0.0021\n",
            "Epoch 4/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1214 - accuracy: 0.9653\n",
            "Epoch 4: val_loss improved from 0.11655 to 0.09571, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.1214 - accuracy: 0.9653 - val_loss: 0.0957 - val_accuracy: 0.9717 - lr: 0.0021\n",
            "Epoch 5/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1159 - accuracy: 0.9671\n",
            "Epoch 5: val_loss improved from 0.09571 to 0.09117, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 69s 31ms/step - loss: 0.1159 - accuracy: 0.9671 - val_loss: 0.0912 - val_accuracy: 0.9726 - lr: 0.0021\n",
            "Epoch 6/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1030 - accuracy: 0.9708\n",
            "Epoch 6: val_loss did not improve from 0.09117\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.1030 - accuracy: 0.9708 - val_loss: 0.0965 - val_accuracy: 0.9701 - lr: 0.0021\n",
            "Epoch 7/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9740\n",
            "Epoch 7: val_loss improved from 0.09117 to 0.08135, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0914 - accuracy: 0.9740 - val_loss: 0.0813 - val_accuracy: 0.9770 - lr: 0.0021\n",
            "Epoch 8/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0854 - accuracy: 0.9755\n",
            "Epoch 8: val_loss improved from 0.08135 to 0.07629, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0854 - accuracy: 0.9755 - val_loss: 0.0763 - val_accuracy: 0.9780 - lr: 0.0021\n",
            "Epoch 9/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9782\n",
            "Epoch 9: val_loss improved from 0.07629 to 0.07176, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0756 - accuracy: 0.9782 - val_loss: 0.0718 - val_accuracy: 0.9804 - lr: 0.0021\n",
            "Epoch 10/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9789\n",
            "Epoch 10: val_loss improved from 0.07176 to 0.06772, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.0728 - accuracy: 0.9789 - val_loss: 0.0677 - val_accuracy: 0.9808 - lr: 0.0021\n",
            "Epoch 11/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9779\n",
            "Epoch 11: val_loss improved from 0.06772 to 0.06463, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.0754 - accuracy: 0.9779 - val_loss: 0.0646 - val_accuracy: 0.9819 - lr: 0.0021\n",
            "Epoch 12/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.9816\n",
            "Epoch 12: val_loss did not improve from 0.06463\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.0620 - accuracy: 0.9816 - val_loss: 0.0768 - val_accuracy: 0.9778 - lr: 0.0021\n",
            "Epoch 13/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9824\n",
            "Epoch 13: val_loss improved from 0.06463 to 0.05724, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.0593 - accuracy: 0.9824 - val_loss: 0.0572 - val_accuracy: 0.9841 - lr: 0.0021\n",
            "Epoch 14/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.9835\n",
            "Epoch 14: val_loss did not improve from 0.05724\n",
            "2189/2189 [==============================] - 65s 30ms/step - loss: 0.0570 - accuracy: 0.9835 - val_loss: 0.0659 - val_accuracy: 0.9827 - lr: 0.0021\n",
            "Epoch 15/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9821\n",
            "Epoch 15: val_loss did not improve from 0.05724\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.0630 - accuracy: 0.9821 - val_loss: 0.0596 - val_accuracy: 0.9825 - lr: 0.0021\n",
            "Epoch 16/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9846\n",
            "Epoch 16: val_loss did not improve from 0.05724\n",
            "2189/2189 [==============================] - 65s 30ms/step - loss: 0.0500 - accuracy: 0.9846 - val_loss: 0.0617 - val_accuracy: 0.9840 - lr: 0.0021\n",
            "Epoch 17/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9861\n",
            "Epoch 17: val_loss improved from 0.05724 to 0.05590, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.0489 - accuracy: 0.9861 - val_loss: 0.0559 - val_accuracy: 0.9868 - lr: 0.0021\n",
            "Epoch 18/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9867\n",
            "Epoch 18: val_loss improved from 0.05590 to 0.05268, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.0457 - accuracy: 0.9867 - val_loss: 0.0527 - val_accuracy: 0.9856 - lr: 0.0021\n",
            "Epoch 19/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9865\n",
            "Epoch 19: val_loss improved from 0.05268 to 0.05223, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.0456 - accuracy: 0.9865 - val_loss: 0.0522 - val_accuracy: 0.9865 - lr: 0.0021\n",
            "Epoch 20/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9875\n",
            "Epoch 20: val_loss did not improve from 0.05223\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0421 - accuracy: 0.9875 - val_loss: 0.0764 - val_accuracy: 0.9810 - lr: 0.0021\n",
            "Epoch 21/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9878\n",
            "Epoch 21: val_loss improved from 0.05223 to 0.04825, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.0404 - accuracy: 0.9878 - val_loss: 0.0482 - val_accuracy: 0.9868 - lr: 0.0021\n",
            "Epoch 22/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9884\n",
            "Epoch 22: val_loss did not improve from 0.04825\n",
            "2189/2189 [==============================] - 65s 30ms/step - loss: 0.0382 - accuracy: 0.9884 - val_loss: 0.0576 - val_accuracy: 0.9851 - lr: 0.0021\n",
            "Epoch 23/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9889\n",
            "Epoch 23: val_loss did not improve from 0.04825\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.0362 - accuracy: 0.9889 - val_loss: 0.0581 - val_accuracy: 0.9840 - lr: 0.0021\n",
            "Epoch 24/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9892\n",
            "Epoch 24: val_loss did not improve from 0.04825\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.0362 - accuracy: 0.9892 - val_loss: 0.0582 - val_accuracy: 0.9873 - lr: 0.0021\n",
            "Epoch 25/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9897\n",
            "Epoch 25: val_loss did not improve from 0.04825\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.0337 - accuracy: 0.9897 - val_loss: 0.0564 - val_accuracy: 0.9862 - lr: 0.0021\n",
            "Epoch 26/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9894\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "\n",
            "Epoch 26: val_loss did not improve from 0.04825\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0334 - accuracy: 0.9894 - val_loss: 0.0530 - val_accuracy: 0.9867 - lr: 0.0021\n",
            "Epoch 27/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9929\n",
            "Epoch 27: val_loss did not improve from 0.04825\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0222 - accuracy: 0.9929 - val_loss: 0.0525 - val_accuracy: 0.9882 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9935\n",
            "Epoch 28: val_loss did not improve from 0.04825\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.0190 - accuracy: 0.9935 - val_loss: 0.0592 - val_accuracy: 0.9887 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9940\n",
            "Epoch 29: val_loss did not improve from 0.04825\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.0174 - accuracy: 0.9940 - val_loss: 0.0674 - val_accuracy: 0.9885 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9944\n",
            "Epoch 30: val_loss did not improve from 0.04825\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0166 - accuracy: 0.9944 - val_loss: 0.0659 - val_accuracy: 0.9881 - lr: 0.0010\n",
            "Epoch 31/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9941Restoring model weights from the end of the best epoch: 21.\n",
            "\n",
            "Epoch 31: val_loss did not improve from 0.04825\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.0159 - accuracy: 0.9941 - val_loss: 0.0696 - val_accuracy: 0.9887 - lr: 0.0010\n",
            "Epoch 31: early stopping\n",
            "548/548 - 6s - loss: 0.0482 - accuracy: 0.9868 - 6s/epoch - 11ms/step\n",
            "Test Loss: 0.0482456311583519\n",
            "Test Accuracy: 0.9867511987686157\n",
            "548/548 [==============================] - 7s 11ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99     14579\n",
            "           1       0.91      0.82      0.86       426\n",
            "           2       0.95      0.97      0.96      1112\n",
            "           3       0.92      0.75      0.83       145\n",
            "           4       0.99      0.99      0.99      1249\n",
            "\n",
            "    accuracy                           0.99     17511\n",
            "   macro avg       0.95      0.90      0.93     17511\n",
            "weighted avg       0.99      0.99      0.99     17511\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparamater Tuning a 3 layer Bidirectional LSTM model"
      ],
      "metadata": {
        "id": "rAiDL4TxyXzm"
      },
      "id": "rAiDL4TxyXzm"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras_tuner import HyperModel, Hyperband\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class LSTMHyperModel3Layers(HyperModel):\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "        model.add(Bidirectional(LSTM(units=hp.Int('units_1', min_value=32, max_value=128, step=32), return_sequences=True), input_shape=self.input_shape))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_1', min_value=0.0, max_value=0.5, default=0.25, step=0.1)))\n",
        "        model.add(Bidirectional(LSTM(units=hp.Int('units_2', min_value=64, max_value=256, step=32), return_sequences=True)))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_2', min_value=0.0, max_value=0.5, default=0.25, step=0.1)))\n",
        "        # Additional bidirectional LSTM layer\n",
        "        model.add(Bidirectional(LSTM(units=hp.Int('units_3', min_value=32, max_value=128, step=32), return_sequences=False)))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_3', min_value=0.0, max_value=0.5, default=0.25, step=0.1)))\n",
        "        model.add(Dense(units=hp.Int('dense_units', min_value=50, max_value=150, step=50), activation='relu'))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_4', min_value=0.0, max_value=0.5, default=0.25, step=0.1)))\n",
        "        model.add(Dense(self.num_classes, activation='softmax'))\n",
        "        model.compile(optimizer=Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "                      loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "# Assuming you have your data loaded in X and y\n",
        "X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "y_categorical = to_categorical(y)\n",
        "num_classes = y_categorical.shape[1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Instantiate and configure the hypermodel for 3 layers\n",
        "hypermodel3Layers = LSTMHyperModel3Layers(input_shape=(187, 1), num_classes=num_classes)\n",
        "\n",
        "# Instantiate the tuner\n",
        "tuner3Layers = Hyperband(\n",
        "    hypermodel3Layers,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=10,\n",
        "    directory='hyperband_3layers',\n",
        "    project_name='mitbih_lstm_classification_3layers'\n",
        ")\n",
        "\n",
        "# Define callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001, verbose=1),\n",
        "    ModelCheckpoint('best_model_LSTM_bidirectional_3layers.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "]\n",
        "\n",
        "# Start the search for the best hyperparameter configuration\n",
        "tuner3Layers.search(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=10, callbacks=callbacks)\n"
      ],
      "metadata": {
        "id": "tBubYWAHyUAJ",
        "outputId": "eff4c0a1-f54f-47fd-9349-0f63f9122e87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "id": "tBubYWAHyUAJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 26 Complete [00h 10m 07s]\n",
            "val_accuracy: 0.9769858717918396\n",
            "\n",
            "Best val_accuracy So Far: 0.9832676649093628\n",
            "Total elapsed time: 02h 12m 59s\n",
            "\n",
            "Search: Running Trial #27\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "96                |64                |units_1\n",
            "0                 |0                 |dropout_1\n",
            "64                |160               |units_2\n",
            "0.4               |0.2               |dropout_2\n",
            "32                |96                |units_3\n",
            "0.2               |0.3               |dropout_3\n",
            "100               |150               |dense_units\n",
            "0.2               |0.1               |dropout_4\n",
            "0.00033123        |0.00083801        |learning_rate\n",
            "10                |10                |tuner/epochs\n",
            "0                 |4                 |tuner/initial_epoch\n",
            "0                 |1                 |tuner/bracket\n",
            "0                 |1                 |tuner/round\n",
            "\n",
            "Epoch 1/10\n",
            "2049/2189 [===========================>..] - ETA: 5s - loss: 0.6609 - accuracy: 0.8257"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras_tuner import HyperModel, Hyperband\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class LSTMHyperModel3Layers(HyperModel):\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "        model.add(Bidirectional(LSTM(units=hp.Int('units_1', min_value=32, max_value=128, step=32), return_sequences=True), input_shape=self.input_shape))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_1', min_value=0.0, max_value=0.5, default=0.25, step=0.1)))\n",
        "        model.add(Bidirectional(LSTM(units=hp.Int('units_2', min_value=64, max_value=256, step=32), return_sequences=True)))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_2', min_value=0.0, max_value=0.5, default=0.25, step=0.1)))\n",
        "        # Additional bidirectional LSTM layer\n",
        "        model.add(Bidirectional(LSTM(units=hp.Int('units_3', min_value=32, max_value=128, step=32), return_sequences=False)))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_3', min_value=0.0, max_value=0.5, default=0.25, step=0.1)))\n",
        "        model.add(Dense(units=hp.Int('dense_units', min_value=50, max_value=150, step=50), activation='relu'))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_4', min_value=0.0, max_value=0.5, default=0.25, step=0.1)))\n",
        "        model.add(Dense(self.num_classes, activation='softmax'))\n",
        "        model.compile(optimizer=Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "                      loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n"
      ],
      "metadata": {
        "id": "XzWs37Fiyg0-"
      },
      "id": "XzWs37Fiyg0-",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the tuner\n",
        "tuner3Layers = Hyperband(\n",
        "    hypermodel3Layers,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=10,\n",
        "    directory='hyperband_3layers',\n",
        "    project_name='mitbih_lstm_classification_3layers'\n",
        ")\n"
      ],
      "metadata": {
        "id": "RRuVcl4IAppb",
        "outputId": "7c47d67f-6bc2-488e-c084-714cd182e91d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "id": "RRuVcl4IAppb",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'hypermodel3Layers' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-24fbc57caae7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Instantiate the tuner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m tuner3Layers = Hyperband(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mhypermodel3Layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hypermodel3Layers' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the 3-layer model"
      ],
      "metadata": {
        "id": "jwvWIaDGLI0O"
      },
      "id": "jwvWIaDGLI0O"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_model_LSTM_3layer_with_best_hps():\n",
        "    input_shape = (187, 1)  # Assuming this is your input shape based on the dataset\n",
        "    model = Sequential([\n",
        "        Bidirectional(LSTM(units=64, return_sequences=True), input_shape=input_shape),  # units_1: 64, using return_sequences=True for stacking\n",
        "        Dropout(0.0),  # dropout_1: 0.0\n",
        "\n",
        "        Bidirectional(LSTM(units=160, return_sequences=True)),  # units_2: 160, adapted to return_sequences=True for the next bidirectional layer\n",
        "        Dropout(0.4),  # dropout_2: 0.4\n",
        "\n",
        "        Bidirectional(LSTM(units=96, return_sequences=False)),  # units_3: 96, final LSTM layer does not return sequences\n",
        "        Dropout(0.2),  # dropout_3: 0.2\n",
        "\n",
        "        Dense(units=150, activation='relu'),  # dense_units: 150\n",
        "        Dropout(0.1),  # dropout_4: 0.1\n",
        "\n",
        "        Dense(5, activation='softmax')  # Assuming 5 classes\n",
        "    ])\n",
        "\n",
        "    # learning_rate: 0.00083801\n",
        "    model.compile(optimizer=Adam(learning_rate=0.00083801), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the LSTM model with best hyperparameters\n",
        "model_LSTM_3layer_with_best_hps = create_model_LSTM_3layer_with_best_hps()\n"
      ],
      "metadata": {
        "id": "34Iueyy3KlZK"
      },
      "id": "34Iueyy3KlZK",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming fazeli_mitbih_train_df is your DataFrame and the last column is the label\n",
        "X = fazeli_mitbih_train_df.iloc[:, :-1].values\n",
        "y = fazeli_mitbih_train_df.iloc[:, -1].values\n",
        "\n",
        "# Reshape X to fit the LSTM input requirements and convert y to categorical\n",
        "X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "y_categorical = to_categorical(y)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "def create_model_LSTM_3layer_with_best_hps():\n",
        "    input_shape = (187, 1)  # Assuming this is your input shape based on the dataset\n",
        "    model = Sequential([\n",
        "        Bidirectional(LSTM(units=64, return_sequences=True), input_shape=input_shape),\n",
        "        Dropout(0.0),\n",
        "        Bidirectional(LSTM(units=160, return_sequences=True)),\n",
        "        Dropout(0.4),\n",
        "        Bidirectional(LSTM(units=96)),\n",
        "        Dropout(0.2),\n",
        "        Dense(units=150, activation='relu'),\n",
        "        Dropout(0.1),\n",
        "        Dense(5, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.00083801), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model_LSTM_3layer_with_best_hps = create_model_LSTM_3layer_with_best_hps()\n",
        "\n",
        "# Define callbacks\n",
        "model_checkpoint = ModelCheckpoint('best_model_LSTM_3layer_with_best_hps.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001, verbose=1)\n",
        "\n",
        "# Train the model with the callbacks\n",
        "history = model_LSTM_3layer_with_best_hps.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr, model_checkpoint])\n",
        "\n",
        "# Evaluation on Test Data\n",
        "test_loss, test_accuracy = model_LSTM_3layer_with_best_hps.evaluate(X_test, y_test, verbose=2)\n",
        "print(f\"Test Loss: {test_loss}\\nTest Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Predictions and Classification Report\n",
        "y_pred = model_LSTM_3layer_with_best_hps.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HpKS4qzgA_bD",
        "outputId": "b1b313d5-f848-4449-eedb-e0d67e7fd957",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HpKS4qzgA_bD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.2993 - accuracy: 0.9208\n",
            "Epoch 1: val_loss improved from inf to 0.18048, saving model to best_model_LSTM_3layer_with_best_hps.h5\n",
            "2189/2189 [==============================] - 112s 46ms/step - loss: 0.2993 - accuracy: 0.9208 - val_loss: 0.1805 - val_accuracy: 0.9486 - lr: 8.3801e-04\n",
            "Epoch 2/100\n",
            "   3/2189 [..............................] - ETA: 1:26 - loss: 0.1300 - accuracy: 0.9688"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1707 - accuracy: 0.9529\n",
            "Epoch 2: val_loss improved from 0.18048 to 0.13541, saving model to best_model_LSTM_3layer_with_best_hps.h5\n",
            "2189/2189 [==============================] - 98s 45ms/step - loss: 0.1707 - accuracy: 0.9529 - val_loss: 0.1354 - val_accuracy: 0.9596 - lr: 8.3801e-04\n",
            "Epoch 3/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1375 - accuracy: 0.9613"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['N', 'S', 'V', 'F', 'Q'], yticklabels=['N', 'S', 'V', 'F', 'Q'])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.ylabel(\"Actual Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oqi2ldIvLgpp"
      },
      "id": "oqi2ldIvLgpp",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:tf-gpu] *",
      "language": "python",
      "name": "conda-env-tf-gpu-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}