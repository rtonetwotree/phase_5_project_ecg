{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8a00172b-9894-4971-a537-441334e1f38d",
      "metadata": {
        "id": "8a00172b-9894-4971-a537-441334e1f38d"
      },
      "source": [
        "## First Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tsfresh\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWrZla58tcRp",
        "outputId": "8b97414e-b0b2-4209-b99d-79af14fd18aa"
      },
      "id": "RWrZla58tcRp",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tsfresh\n",
            "  Downloading tsfresh-0.20.2-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/95.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m92.2/95.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (2.31.0)\n",
            "Requirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (1.5.3)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (1.11.4)\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (0.14.1)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (0.5.6)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (4.66.2)\n",
            "Collecting stumpy>=1.7.2 (from tsfresh)\n",
            "  Downloading stumpy-1.12.0-py3-none-any.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.1/169.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from tsfresh) (2.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.0->tsfresh) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.0->tsfresh) (2023.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.4.1->tsfresh) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.1->tsfresh) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.1->tsfresh) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.1->tsfresh) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.1->tsfresh) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->tsfresh) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->tsfresh) (3.3.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13->tsfresh) (23.2)\n",
            "Requirement already satisfied: numba>=0.55.2 in /usr/local/lib/python3.10/dist-packages (from stumpy>=1.7.2->tsfresh) (0.58.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.55.2->stumpy>=1.7.2->tsfresh) (0.41.1)\n",
            "Installing collected packages: stumpy, tsfresh\n",
            "Successfully installed stumpy-1.12.0 tsfresh-0.20.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install keras-tuner\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AYlBpR4rLRL",
        "outputId": "7f6cca59-c4f4-4ffd-9e22-5eeea58f4615"
      },
      "id": "1AYlBpR4rLRL",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m122.9/129.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPQBOk7Rugm3",
        "outputId": "3dca5a24-f4bd-40e1-e087-722e695d8cbf"
      },
      "id": "BPQBOk7Rugm3",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "96435299-050b-4010-b57c-cc490b83b705",
      "metadata": {
        "id": "96435299-050b-4010-b57c-cc490b83b705"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tsfresh import extract_features\n",
        "from tsfresh.feature_extraction import ComprehensiveFCParameters\n",
        "\n",
        "# If you need to plot or visualize data later on\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# For any data preprocessing or manipulation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Depending on the models you plan to use\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38da3325-d76d-421e-94d0-0596c6b95d3b",
      "metadata": {
        "id": "38da3325-d76d-421e-94d0-0596c6b95d3b"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "37dbff3e-7b6b-4798-9190-3d0180925068",
      "metadata": {
        "id": "37dbff3e-7b6b-4798-9190-3d0180925068"
      },
      "outputs": [],
      "source": [
        "fazeli_mitbih_train_df = pd.read_csv('/content/drive/MyDrive/mitbih_train.csv', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fazeli_mitbih_test_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/mitbih_test.csv', header=None)"
      ],
      "metadata": {
        "id": "KwjAVYgmE55a"
      },
      "id": "KwjAVYgmE55a",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "74aed28a-031c-4298-a7f8-3d01d46e93be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74aed28a-031c-4298-a7f8-3d01d46e93be",
        "outputId": "cbc1b04a-a8f5-44e8-ef40-2c8b969fd72d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    72471\n",
              "4.0     6431\n",
              "2.0     5788\n",
              "1.0     2223\n",
              "3.0      641\n",
              "Name: 187, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "column_187 = fazeli_mitbih_train_df.iloc[:, 187]\n",
        "column_187.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3376e4c2-e6a3-41fc-8edd-3580bf98081c",
      "metadata": {
        "id": "3376e4c2-e6a3-41fc-8edd-3580bf98081c"
      },
      "source": [
        "# Bidirectional LSTM Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two Layers"
      ],
      "metadata": {
        "id": "pmy-STd3p8yz"
      },
      "id": "pmy-STd3p8yz"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_model_LSTM_bidirectional():\n",
        "    input_shape = (187, 1)  # input shape based on the dataset\n",
        "    model = Sequential([\n",
        "        # Wrapping the LSTM layer with Bidirectional\n",
        "        Bidirectional(LSTM(units=128, return_sequences=True), input_shape=input_shape),\n",
        "        Dropout(0.1),\n",
        "\n",
        "        # Adding another Bidirectional LSTM layer\n",
        "        Bidirectional(LSTM(units=64, return_sequences=False)),\n",
        "        Dropout(0.1),\n",
        "\n",
        "        # Dense layer\n",
        "        Dense(units=150, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(5, activation='softmax')  # Adjusted for 5 classes\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the bidirectional LSTM model\n",
        "model_LSTM_bidirectional = create_model_LSTM_bidirectional()\n",
        "\n",
        "# Now use model_LSTM_bidirectional for training\n"
      ],
      "metadata": {
        "id": "ANmtVjffkqjD"
      },
      "id": "ANmtVjffkqjD",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# fazeli_mitbih_train_df is your DataFrame and the last column is the label\n",
        "X = fazeli_mitbih_train_df.iloc[:, :-1].values\n",
        "y = fazeli_mitbih_train_df.iloc[:, -1].values\n",
        "\n",
        "# Reshape X to fit the LSTM input requirements and convert y to categorical\n",
        "X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "y_categorical = to_categorical(y)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define callbacks\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'best_model_LSTM_bidirectional.h5',  # filename to reflect it's the bidirectional LSTM model\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=5,\n",
        "    min_lr=0.001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Create the bidirectional LSTM model with the best hyperparameters\n",
        "# 'create_model_LSTM_bidirectional()' function is defined and returns the bidirectional LSTM model\n",
        "model_LSTM_bidirectional = create_model_LSTM_bidirectional()\n",
        "\n",
        "# Train the bidirectional LSTM model with the callbacks\n",
        "history_LSTM_bidirectional = model_LSTM_bidirectional.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=100,  # Adjust the number of epochs as necessary\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, reduce_lr, model_checkpoint]\n",
        ")\n",
        "\n",
        "# Evaluation on Test Data\n",
        "test_loss, test_accuracy = model_LSTM_bidirectional.evaluate(X_test, y_test, verbose=2)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Predictions and Classification Report\n",
        "y_pred = model_LSTM_bidirectional.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2392
        },
        "id": "c33e8BatqEJk",
        "outputId": "5c2ca63c-b680-4ceb-a6f4-747b48b23c9e"
      },
      "id": "c33e8BatqEJk",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.3221 - accuracy: 0.9170\n",
            "Epoch 1: val_loss improved from inf to 0.17689, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 80s 33ms/step - loss: 0.3221 - accuracy: 0.9170 - val_loss: 0.1769 - val_accuracy: 0.9508 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "   5/2189 [..............................] - ETA: 1:00 - loss: 0.1799 - accuracy: 0.9563"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1750 - accuracy: 0.9525\n",
            "Epoch 2: val_loss improved from 0.17689 to 0.15195, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.1750 - accuracy: 0.9525 - val_loss: 0.1519 - val_accuracy: 0.9587 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1398 - accuracy: 0.9618\n",
            "Epoch 3: val_loss improved from 0.15195 to 0.11292, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.1398 - accuracy: 0.9618 - val_loss: 0.1129 - val_accuracy: 0.9660 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1225 - accuracy: 0.9663\n",
            "Epoch 4: val_loss improved from 0.11292 to 0.10153, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.1225 - accuracy: 0.9663 - val_loss: 0.1015 - val_accuracy: 0.9718 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1098 - accuracy: 0.9692\n",
            "Epoch 5: val_loss improved from 0.10153 to 0.10116, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.1098 - accuracy: 0.9692 - val_loss: 0.1012 - val_accuracy: 0.9718 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1002 - accuracy: 0.9717\n",
            "Epoch 6: val_loss improved from 0.10116 to 0.08409, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.1002 - accuracy: 0.9717 - val_loss: 0.0841 - val_accuracy: 0.9754 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0923 - accuracy: 0.9736\n",
            "Epoch 7: val_loss improved from 0.08409 to 0.07696, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 64s 29ms/step - loss: 0.0924 - accuracy: 0.9736 - val_loss: 0.0770 - val_accuracy: 0.9772 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0863 - accuracy: 0.9756\n",
            "Epoch 8: val_loss improved from 0.07696 to 0.07347, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 64s 29ms/step - loss: 0.0863 - accuracy: 0.9756 - val_loss: 0.0735 - val_accuracy: 0.9792 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9772\n",
            "Epoch 9: val_loss did not improve from 0.07347\n",
            "2189/2189 [==============================] - 65s 30ms/step - loss: 0.0794 - accuracy: 0.9772 - val_loss: 0.0785 - val_accuracy: 0.9762 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9783\n",
            "Epoch 10: val_loss improved from 0.07347 to 0.06629, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 65s 30ms/step - loss: 0.0749 - accuracy: 0.9783 - val_loss: 0.0663 - val_accuracy: 0.9814 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9795\n",
            "Epoch 11: val_loss improved from 0.06629 to 0.06365, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 65s 30ms/step - loss: 0.0695 - accuracy: 0.9795 - val_loss: 0.0637 - val_accuracy: 0.9833 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0653 - accuracy: 0.9811\n",
            "Epoch 12: val_loss did not improve from 0.06365\n",
            "2189/2189 [==============================] - 64s 29ms/step - loss: 0.0653 - accuracy: 0.9811 - val_loss: 0.0669 - val_accuracy: 0.9805 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0615 - accuracy: 0.9812\n",
            "Epoch 13: val_loss improved from 0.06365 to 0.05858, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 65s 30ms/step - loss: 0.0615 - accuracy: 0.9812 - val_loss: 0.0586 - val_accuracy: 0.9842 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9825\n",
            "Epoch 14: val_loss did not improve from 0.05858\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0581 - accuracy: 0.9825 - val_loss: 0.0588 - val_accuracy: 0.9829 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0549 - accuracy: 0.9839\n",
            "Epoch 15: val_loss did not improve from 0.05858\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0549 - accuracy: 0.9839 - val_loss: 0.0601 - val_accuracy: 0.9824 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9840\n",
            "Epoch 16: val_loss improved from 0.05858 to 0.05480, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0526 - accuracy: 0.9840 - val_loss: 0.0548 - val_accuracy: 0.9849 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.9848\n",
            "Epoch 17: val_loss improved from 0.05480 to 0.04872, saving model to best_model_LSTM_bidirectional.h5\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0514 - accuracy: 0.9848 - val_loss: 0.0487 - val_accuracy: 0.9858 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9852\n",
            "Epoch 18: val_loss did not improve from 0.04872\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0482 - accuracy: 0.9852 - val_loss: 0.0517 - val_accuracy: 0.9858 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9855\n",
            "Epoch 19: val_loss did not improve from 0.04872\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0468 - accuracy: 0.9855 - val_loss: 0.0550 - val_accuracy: 0.9848 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0447 - accuracy: 0.9868\n",
            "Epoch 20: val_loss did not improve from 0.04872\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.0447 - accuracy: 0.9868 - val_loss: 0.0623 - val_accuracy: 0.9842 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9867\n",
            "Epoch 21: val_loss did not improve from 0.04872\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.0432 - accuracy: 0.9867 - val_loss: 0.0528 - val_accuracy: 0.9853 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.9874\n",
            "Epoch 22: val_loss did not improve from 0.04872\n",
            "2189/2189 [==============================] - 67s 30ms/step - loss: 0.0408 - accuracy: 0.9874 - val_loss: 0.0540 - val_accuracy: 0.9850 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9874\n",
            "Epoch 23: val_loss did not improve from 0.04872\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0404 - accuracy: 0.9874 - val_loss: 0.0589 - val_accuracy: 0.9853 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9876\n",
            "Epoch 24: val_loss did not improve from 0.04872\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0392 - accuracy: 0.9876 - val_loss: 0.0500 - val_accuracy: 0.9870 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 0.9885\n",
            "Epoch 25: val_loss did not improve from 0.04872\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0372 - accuracy: 0.9885 - val_loss: 0.0647 - val_accuracy: 0.9830 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 0.9883\n",
            "Epoch 26: val_loss did not improve from 0.04872\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0372 - accuracy: 0.9883 - val_loss: 0.0544 - val_accuracy: 0.9861 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9889Restoring model weights from the end of the best epoch: 17.\n",
            "\n",
            "Epoch 27: val_loss did not improve from 0.04872\n",
            "2189/2189 [==============================] - 67s 30ms/step - loss: 0.0355 - accuracy: 0.9889 - val_loss: 0.0542 - val_accuracy: 0.9866 - lr: 0.0010\n",
            "Epoch 27: early stopping\n",
            "548/548 - 6s - loss: 0.0487 - accuracy: 0.9858 - 6s/epoch - 11ms/step\n",
            "Test Loss: 0.04872136563062668\n",
            "Test Accuracy: 0.9857803583145142\n",
            "548/548 [==============================] - 7s 11ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99     14579\n",
            "           1       0.95      0.78      0.86       426\n",
            "           2       0.97      0.95      0.96      1112\n",
            "           3       0.83      0.76      0.79       145\n",
            "           4       0.99      0.99      0.99      1249\n",
            "\n",
            "    accuracy                           0.99     17511\n",
            "   macro avg       0.94      0.90      0.92     17511\n",
            "weighted avg       0.99      0.99      0.99     17511\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'confusion_matrix' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-952ef2396539>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# Confusion Matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Blues\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "bidirectionality with another layer"
      ],
      "metadata": {
        "id": "Z4LhiQSg5TDO"
      },
      "id": "Z4LhiQSg5TDO"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_model_LSTM_bidirectional_2():\n",
        "    input_shape = (187, 1)  # input shape based on the dataset\n",
        "    model = Sequential([\n",
        "        Bidirectional(LSTM(units=128, return_sequences=True), input_shape=input_shape),\n",
        "        Dropout(0.1),\n",
        "\n",
        "        Bidirectional(LSTM(units=64, return_sequences=True)),\n",
        "        Dropout(0.1),\n",
        "\n",
        "        Bidirectional(LSTM(units=32, return_sequences=False)),  # Additional layer\n",
        "        Dropout(0.1),\n",
        "\n",
        "        Dense(units=150, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(5, activation='softmax')  # Adjusted for 5 classes\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the LSTM model with two bidirectional layers\n",
        "model_LSTM_bidirectional_2 = create_model_LSTM_bidirectional_2()\n"
      ],
      "metadata": {
        "id": "ffDZ68ERspXJ"
      },
      "id": "ffDZ68ERspXJ",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# fazeli_mitbih_train_df is your DataFrame and the last column is the label\n",
        "X = fazeli_mitbih_train_df.iloc[:, :-1].values\n",
        "y = fazeli_mitbih_train_df.iloc[:, -1].values\n",
        "\n",
        "# Reshape X to fit the LSTM input requirements and convert y to categorical\n",
        "X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "y_categorical = to_categorical(y)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define callbacks\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'best_model_LSTM_bidirectional_2.h5',  # Updated filename for the new bidirectional model\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=5,\n",
        "    min_lr=0.001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Assuming 'create_model_LSTM_bidirectional_2()' function is defined and returns the bidirectional LSTM model\n",
        "model_LSTM_bidirectional_2 = create_model_LSTM_bidirectional_2()\n",
        "\n",
        "# Train the bidirectional LSTM model with the callbacks\n",
        "history_LSTM_bidirectional_2 = model_LSTM_bidirectional_2.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=100,  # Adjust the number of epochs as necessary\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, reduce_lr, model_checkpoint]\n",
        ")\n",
        "\n",
        "# Evaluation on Test Data\n",
        "test_loss, test_accuracy = model_LSTM_bidirectional_2.evaluate(X_test, y_test, verbose=2)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Predictions and Classification Report\n",
        "y_pred = model_LSTM_bidirectional_2.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vk8wSUZL5av-",
        "outputId": "3af31476-2c85-4401-dca5-d632af874ac6"
      },
      "id": "vk8wSUZL5av-",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.4754 - accuracy: 0.8748\n",
            "Epoch 1: val_loss improved from inf to 0.24721, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 103s 43ms/step - loss: 0.4754 - accuracy: 0.8748 - val_loss: 0.2472 - val_accuracy: 0.9373 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "   3/2189 [..............................] - ETA: 1:21 - loss: 0.2075 - accuracy: 0.9479"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2189/2189 [==============================] - ETA: 0s - loss: 0.2653 - accuracy: 0.9314\n",
            "Epoch 2: val_loss did not improve from 0.24721\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.2653 - accuracy: 0.9314 - val_loss: 0.3246 - val_accuracy: 0.9130 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1852 - accuracy: 0.9490\n",
            "Epoch 3: val_loss improved from 0.24721 to 0.13139, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.1852 - accuracy: 0.9490 - val_loss: 0.1314 - val_accuracy: 0.9620 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1505 - accuracy: 0.9572\n",
            "Epoch 4: val_loss improved from 0.13139 to 0.11245, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 89s 41ms/step - loss: 0.1505 - accuracy: 0.9572 - val_loss: 0.1125 - val_accuracy: 0.9684 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1284 - accuracy: 0.9645\n",
            "Epoch 5: val_loss improved from 0.11245 to 0.11102, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.1284 - accuracy: 0.9645 - val_loss: 0.1110 - val_accuracy: 0.9685 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1184 - accuracy: 0.9672\n",
            "Epoch 6: val_loss improved from 0.11102 to 0.09606, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.1184 - accuracy: 0.9672 - val_loss: 0.0961 - val_accuracy: 0.9730 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1073 - accuracy: 0.9703\n",
            "Epoch 7: val_loss did not improve from 0.09606\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.1073 - accuracy: 0.9703 - val_loss: 0.1004 - val_accuracy: 0.9716 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1001 - accuracy: 0.9721\n",
            "Epoch 8: val_loss improved from 0.09606 to 0.08442, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.1001 - accuracy: 0.9721 - val_loss: 0.0844 - val_accuracy: 0.9765 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0921 - accuracy: 0.9742\n",
            "Epoch 9: val_loss improved from 0.08442 to 0.07346, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.0921 - accuracy: 0.9742 - val_loss: 0.0735 - val_accuracy: 0.9785 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0861 - accuracy: 0.9766\n",
            "Epoch 10: val_loss did not improve from 0.07346\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.0861 - accuracy: 0.9766 - val_loss: 0.0744 - val_accuracy: 0.9782 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0806 - accuracy: 0.9773\n",
            "Epoch 11: val_loss improved from 0.07346 to 0.06918, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.0806 - accuracy: 0.9773 - val_loss: 0.0692 - val_accuracy: 0.9806 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0888 - accuracy: 0.9752\n",
            "Epoch 12: val_loss did not improve from 0.06918\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.0888 - accuracy: 0.9752 - val_loss: 0.0784 - val_accuracy: 0.9781 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0810 - accuracy: 0.9775\n",
            "Epoch 13: val_loss did not improve from 0.06918\n",
            "2189/2189 [==============================] - 91s 41ms/step - loss: 0.0810 - accuracy: 0.9775 - val_loss: 0.0705 - val_accuracy: 0.9793 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9795\n",
            "Epoch 14: val_loss improved from 0.06918 to 0.06808, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.0710 - accuracy: 0.9795 - val_loss: 0.0681 - val_accuracy: 0.9798 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9809\n",
            "Epoch 15: val_loss improved from 0.06808 to 0.06598, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.0671 - accuracy: 0.9809 - val_loss: 0.0660 - val_accuracy: 0.9814 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0646 - accuracy: 0.9809\n",
            "Epoch 16: val_loss improved from 0.06598 to 0.06040, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 95s 43ms/step - loss: 0.0646 - accuracy: 0.9809 - val_loss: 0.0604 - val_accuracy: 0.9828 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9824\n",
            "Epoch 17: val_loss did not improve from 0.06040\n",
            "2189/2189 [==============================] - 95s 43ms/step - loss: 0.0613 - accuracy: 0.9824 - val_loss: 0.0607 - val_accuracy: 0.9832 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9823\n",
            "Epoch 18: val_loss improved from 0.06040 to 0.05732, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 95s 43ms/step - loss: 0.0604 - accuracy: 0.9823 - val_loss: 0.0573 - val_accuracy: 0.9834 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9831\n",
            "Epoch 19: val_loss did not improve from 0.05732\n",
            "2189/2189 [==============================] - 95s 43ms/step - loss: 0.0578 - accuracy: 0.9831 - val_loss: 0.0599 - val_accuracy: 0.9830 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0545 - accuracy: 0.9840\n",
            "Epoch 20: val_loss improved from 0.05732 to 0.05629, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 95s 43ms/step - loss: 0.0545 - accuracy: 0.9840 - val_loss: 0.0563 - val_accuracy: 0.9848 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0530 - accuracy: 0.9849\n",
            "Epoch 21: val_loss did not improve from 0.05629\n",
            "2189/2189 [==============================] - 95s 43ms/step - loss: 0.0530 - accuracy: 0.9849 - val_loss: 0.0631 - val_accuracy: 0.9824 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9844\n",
            "Epoch 22: val_loss improved from 0.05629 to 0.05616, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 95s 43ms/step - loss: 0.0520 - accuracy: 0.9844 - val_loss: 0.0562 - val_accuracy: 0.9845 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0498 - accuracy: 0.9853\n",
            "Epoch 23: val_loss did not improve from 0.05616\n",
            "2189/2189 [==============================] - 95s 43ms/step - loss: 0.0498 - accuracy: 0.9853 - val_loss: 0.0564 - val_accuracy: 0.9851 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9859\n",
            "Epoch 24: val_loss improved from 0.05616 to 0.05340, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 94s 43ms/step - loss: 0.0471 - accuracy: 0.9859 - val_loss: 0.0534 - val_accuracy: 0.9850 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9867\n",
            "Epoch 25: val_loss improved from 0.05340 to 0.05186, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 95s 43ms/step - loss: 0.0467 - accuracy: 0.9867 - val_loss: 0.0519 - val_accuracy: 0.9854 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9866\n",
            "Epoch 26: val_loss did not improve from 0.05186\n",
            "2189/2189 [==============================] - 94s 43ms/step - loss: 0.0448 - accuracy: 0.9866 - val_loss: 0.0569 - val_accuracy: 0.9840 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9870\n",
            "Epoch 27: val_loss did not improve from 0.05186\n",
            "2189/2189 [==============================] - 91s 42ms/step - loss: 0.0440 - accuracy: 0.9870 - val_loss: 0.0652 - val_accuracy: 0.9843 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0419 - accuracy: 0.9874\n",
            "Epoch 28: val_loss did not improve from 0.05186\n",
            "2189/2189 [==============================] - 91s 41ms/step - loss: 0.0419 - accuracy: 0.9874 - val_loss: 0.0590 - val_accuracy: 0.9850 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0416 - accuracy: 0.9878\n",
            "Epoch 29: val_loss did not improve from 0.05186\n",
            "2189/2189 [==============================] - 91s 41ms/step - loss: 0.0416 - accuracy: 0.9878 - val_loss: 0.0544 - val_accuracy: 0.9862 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0409 - accuracy: 0.9876\n",
            "Epoch 30: val_loss did not improve from 0.05186\n",
            "2189/2189 [==============================] - 91s 42ms/step - loss: 0.0409 - accuracy: 0.9876 - val_loss: 0.0889 - val_accuracy: 0.9732 - lr: 0.0010\n",
            "Epoch 31/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9879\n",
            "Epoch 31: val_loss did not improve from 0.05186\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.0392 - accuracy: 0.9879 - val_loss: 0.0563 - val_accuracy: 0.9846 - lr: 0.0010\n",
            "Epoch 32/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0383 - accuracy: 0.9881\n",
            "Epoch 32: val_loss did not improve from 0.05186\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.0383 - accuracy: 0.9881 - val_loss: 0.0591 - val_accuracy: 0.9845 - lr: 0.0010\n",
            "Epoch 33/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9886\n",
            "Epoch 33: val_loss did not improve from 0.05186\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.0364 - accuracy: 0.9886 - val_loss: 0.0522 - val_accuracy: 0.9866 - lr: 0.0010\n",
            "Epoch 34/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0374 - accuracy: 0.9880\n",
            "Epoch 34: val_loss did not improve from 0.05186\n",
            "2189/2189 [==============================] - 91s 42ms/step - loss: 0.0374 - accuracy: 0.9880 - val_loss: 0.0606 - val_accuracy: 0.9855 - lr: 0.0010\n",
            "Epoch 35/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9889\n",
            "Epoch 35: val_loss improved from 0.05186 to 0.04646, saving model to best_model_LSTM_bidirectional_2.h5\n",
            "2189/2189 [==============================] - 89s 40ms/step - loss: 0.0363 - accuracy: 0.9889 - val_loss: 0.0465 - val_accuracy: 0.9863 - lr: 0.0010\n",
            "Epoch 36/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0373 - accuracy: 0.9885\n",
            "Epoch 36: val_loss did not improve from 0.04646\n",
            "2189/2189 [==============================] - 90s 41ms/step - loss: 0.0373 - accuracy: 0.9885 - val_loss: 0.0519 - val_accuracy: 0.9870 - lr: 0.0010\n",
            "Epoch 37/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9895\n",
            "Epoch 37: val_loss did not improve from 0.04646\n",
            "2189/2189 [==============================] - 88s 40ms/step - loss: 0.0338 - accuracy: 0.9895 - val_loss: 0.0568 - val_accuracy: 0.9848 - lr: 0.0010\n",
            "Epoch 38/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9898\n",
            "Epoch 38: val_loss did not improve from 0.04646\n",
            "2189/2189 [==============================] - 89s 40ms/step - loss: 0.0328 - accuracy: 0.9898 - val_loss: 0.0492 - val_accuracy: 0.9862 - lr: 0.0010\n",
            "Epoch 39/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9896\n",
            "Epoch 39: val_loss did not improve from 0.04646\n",
            "2189/2189 [==============================] - 89s 41ms/step - loss: 0.0331 - accuracy: 0.9896 - val_loss: 0.0515 - val_accuracy: 0.9861 - lr: 0.0010\n",
            "Epoch 40/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9897\n",
            "Epoch 40: val_loss did not improve from 0.04646\n",
            "2189/2189 [==============================] - 89s 41ms/step - loss: 0.0317 - accuracy: 0.9897 - val_loss: 0.0507 - val_accuracy: 0.9867 - lr: 0.0010\n",
            "Epoch 41/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9898\n",
            "Epoch 41: val_loss did not improve from 0.04646\n",
            "2189/2189 [==============================] - 88s 40ms/step - loss: 0.0310 - accuracy: 0.9898 - val_loss: 0.0526 - val_accuracy: 0.9861 - lr: 0.0010\n",
            "Epoch 42/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9900\n",
            "Epoch 42: val_loss did not improve from 0.04646\n",
            "2189/2189 [==============================] - 88s 40ms/step - loss: 0.0314 - accuracy: 0.9900 - val_loss: 0.0538 - val_accuracy: 0.9857 - lr: 0.0010\n",
            "Epoch 43/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9898\n",
            "Epoch 43: val_loss did not improve from 0.04646\n",
            "2189/2189 [==============================] - 89s 41ms/step - loss: 0.0312 - accuracy: 0.9898 - val_loss: 0.0499 - val_accuracy: 0.9863 - lr: 0.0010\n",
            "Epoch 44/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9913\n",
            "Epoch 44: val_loss did not improve from 0.04646\n",
            "2189/2189 [==============================] - 88s 40ms/step - loss: 0.0274 - accuracy: 0.9913 - val_loss: 0.0557 - val_accuracy: 0.9877 - lr: 0.0010\n",
            "Epoch 45/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9905Restoring model weights from the end of the best epoch: 35.\n",
            "\n",
            "Epoch 45: val_loss did not improve from 0.04646\n",
            "2189/2189 [==============================] - 89s 41ms/step - loss: 0.0293 - accuracy: 0.9905 - val_loss: 0.0542 - val_accuracy: 0.9871 - lr: 0.0010\n",
            "Epoch 45: early stopping\n",
            "548/548 - 8s - loss: 0.0465 - accuracy: 0.9863 - 8s/epoch - 15ms/step\n",
            "Test Loss: 0.04645908623933792\n",
            "Test Accuracy: 0.9862943291664124\n",
            "548/548 [==============================] - 10s 15ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99     14579\n",
            "           1       0.92      0.78      0.84       426\n",
            "           2       0.97      0.96      0.96      1112\n",
            "           3       0.88      0.82      0.85       145\n",
            "           4       0.99      0.99      0.99      1249\n",
            "\n",
            "    accuracy                           0.99     17511\n",
            "   macro avg       0.95      0.91      0.93     17511\n",
            "weighted avg       0.99      0.99      0.99     17511\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparamater Tuning a 2 layer Bidirectional LSTM model"
      ],
      "metadata": {
        "id": "j7jgzQNeyEvC"
      },
      "id": "j7jgzQNeyEvC"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras_tuner import HyperModel, Hyperband\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "class LSTMHyperModel(HyperModel):\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "        model.add(Bidirectional(LSTM(units=hp.Int('units_1', min_value=32, max_value=128, step=32), return_sequences=True), input_shape=self.input_shape))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_1', min_value=0.0, max_value=0.5, default=0.25, step=0.1)))\n",
        "\n",
        "        # Adding another bidirectional LSTM layer if return_sequences is set to True\n",
        "        model.add(Bidirectional(LSTM(units=hp.Int('units_2', min_value=64, max_value=256, step=32), return_sequences=False)))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_2', min_value=0.0, max_value=0.5, default=0.25, step=0.1)))\n",
        "\n",
        "        model.add(Dense(units=hp.Int('dense_units', min_value=50, max_value=150, step=50), activation='relu'))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_3', min_value=0.0, max_value=0.5, default=0.25, step=0.1)))\n",
        "\n",
        "        model.add(Dense(self.num_classes, activation='softmax'))\n",
        "\n",
        "        model.compile(optimizer=Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "# Load your data\n",
        "X = fazeli_mitbih_train_df.iloc[:, :-1].values\n",
        "y = fazeli_mitbih_train_df.iloc[:, -1].values\n",
        "\n",
        "# Preprocess the data\n",
        "X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "y_categorical = to_categorical(y)\n",
        "num_classes = y_categorical.shape[1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001, verbose=1),\n",
        "    ModelCheckpoint('best_model_LSTM_bidirectional_2.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "]\n",
        "\n",
        "# Instantiate and configure the hypermodel\n",
        "hypermodel = LSTMHyperModel(input_shape=(187, 1), num_classes=num_classes)\n",
        "\n",
        "# Instantiate the tuner\n",
        "tuner = Hyperband(\n",
        "    hypermodel,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=10,\n",
        "    directory='hyperband',\n",
        "    project_name='mitbih_lstm_classification'\n",
        ")\n",
        "\n",
        "# Start the search for the best hyperparameter configuration\n",
        "tuner.search(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=10, callbacks=callbacks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "94DJb-xN6FVc",
        "outputId": "6d3a124f-7556-4fd0-cb64-9050703a4cd6"
      },
      "id": "94DJb-xN6FVc",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 10m 53s]\n",
            "val_accuracy: 0.9740163087844849\n",
            "\n",
            "Best val_accuracy So Far: 0.9820113182067871\n",
            "Total elapsed time: 02h 18m 46s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recreate the tuner with the same configuration\n",
        "tuner = Hyperband(\n",
        "    hypermodel,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=10,\n",
        "    directory='hyperband',  # Same directory as before\n",
        "    project_name='mitbih_lstm_classification'  # Same project name as before\n",
        ")\n",
        "\n",
        "# Load the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters()[0]\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "for hp in best_hps.space:\n",
        "    print(f\"{hp.name}: {best_hps.get(hp.name)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YUfnFsdWuXkv",
        "outputId": "482c04d5-7207-49b3-9633-68059552cbcd"
      },
      "id": "YUfnFsdWuXkv",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloading Tuner from hyperband/mitbih_lstm_classification/tuner0.json\n",
            "Best Hyperparameters:\n",
            "units_1: 64\n",
            "dropout_1: 0.0\n",
            "units_2: 160\n",
            "dropout_2: 0.4\n",
            "dense_units: 100\n",
            "dropout_3: 0.2\n",
            "learning_rate: 0.002097863337902064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_model_LSTM_bidirectional_2_with_best_hps():\n",
        "    input_shape = (187, 1)  # input shape based on the dataset\n",
        "    model = Sequential([\n",
        "        Bidirectional(LSTM(units=64, return_sequences=True), input_shape=input_shape),  # units_1: 64\n",
        "        Dropout(0.0),  # dropout_1: 0.0\n",
        "\n",
        "        Bidirectional(LSTM(units=160, return_sequences=False)),  # units_2: 160\n",
        "        Dropout(0.4),  # dropout_2: 0.4\n",
        "\n",
        "        Dense(units=100, activation='relu'),  # dense_units: 100\n",
        "        Dropout(0.2),  # dropout_3: 0.2\n",
        "\n",
        "        Dense(5, activation='softmax')  # Assuming 5 classes\n",
        "    ])\n",
        "\n",
        "    # learning_rate: 0.002097863337902064\n",
        "    model.compile(optimizer=Adam(learning_rate=0.002097863337902064), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the LSTM model with best hyperparameters\n",
        "model_LSTM_bidirectional_2_with_best_hps = create_model_LSTM_bidirectional_2_with_best_hps()\n"
      ],
      "metadata": {
        "id": "FcvYmJllrcwG"
      },
      "id": "FcvYmJllrcwG",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# fazeli_mitbih_train_df is your DataFrame and the last column is the label\n",
        "X = fazeli_mitbih_train_df.iloc[:, :-1].values\n",
        "y = fazeli_mitbih_train_df.iloc[:, -1].values\n",
        "\n",
        "# Reshape X to fit the LSTM input requirements and convert y to categorical\n",
        "X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "y_categorical = to_categorical(y)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define callbacks\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'best_model_LSTM_bidirectional_2_with_best_hps.h5',  # Updated filename for the model with best hyperparameters\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=5,\n",
        "    min_lr=0.001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Use the model with best hyperparameters created earlier\n",
        "model = model_LSTM_bidirectional_2_with_best_hps  # this model is already created with the function call\n",
        "\n",
        "# Train the model with the callbacks\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=100,  # Adjust the number of epochs as necessary\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, reduce_lr, model_checkpoint]\n",
        ")\n",
        "\n",
        "# Evaluation on Test Data\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Predictions and Classification Report\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jjTxpRu-riSd",
        "outputId": "902f197c-2a17-4a3c-c31b-049f23e6641e"
      },
      "id": "jjTxpRu-riSd",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.3303 - accuracy: 0.9148\n",
            "Epoch 1: val_loss improved from inf to 0.19216, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 76s 31ms/step - loss: 0.3303 - accuracy: 0.9148 - val_loss: 0.1922 - val_accuracy: 0.9460 - lr: 0.0021\n",
            "Epoch 2/100\n",
            "   5/2189 [..............................] - ETA: 1:00 - loss: 0.1781 - accuracy: 0.9563"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1739 - accuracy: 0.9529\n",
            "Epoch 2: val_loss improved from 0.19216 to 0.12433, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 65s 30ms/step - loss: 0.1739 - accuracy: 0.9529 - val_loss: 0.1243 - val_accuracy: 0.9640 - lr: 0.0021\n",
            "Epoch 3/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1412 - accuracy: 0.9607\n",
            "Epoch 3: val_loss improved from 0.12433 to 0.11655, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.1412 - accuracy: 0.9607 - val_loss: 0.1166 - val_accuracy: 0.9648 - lr: 0.0021\n",
            "Epoch 4/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1214 - accuracy: 0.9653\n",
            "Epoch 4: val_loss improved from 0.11655 to 0.09571, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.1214 - accuracy: 0.9653 - val_loss: 0.0957 - val_accuracy: 0.9717 - lr: 0.0021\n",
            "Epoch 5/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1159 - accuracy: 0.9671\n",
            "Epoch 5: val_loss improved from 0.09571 to 0.09117, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 69s 31ms/step - loss: 0.1159 - accuracy: 0.9671 - val_loss: 0.0912 - val_accuracy: 0.9726 - lr: 0.0021\n",
            "Epoch 6/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1030 - accuracy: 0.9708\n",
            "Epoch 6: val_loss did not improve from 0.09117\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.1030 - accuracy: 0.9708 - val_loss: 0.0965 - val_accuracy: 0.9701 - lr: 0.0021\n",
            "Epoch 7/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9740\n",
            "Epoch 7: val_loss improved from 0.09117 to 0.08135, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0914 - accuracy: 0.9740 - val_loss: 0.0813 - val_accuracy: 0.9770 - lr: 0.0021\n",
            "Epoch 8/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0854 - accuracy: 0.9755\n",
            "Epoch 8: val_loss improved from 0.08135 to 0.07629, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0854 - accuracy: 0.9755 - val_loss: 0.0763 - val_accuracy: 0.9780 - lr: 0.0021\n",
            "Epoch 9/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9782\n",
            "Epoch 9: val_loss improved from 0.07629 to 0.07176, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0756 - accuracy: 0.9782 - val_loss: 0.0718 - val_accuracy: 0.9804 - lr: 0.0021\n",
            "Epoch 10/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9789\n",
            "Epoch 10: val_loss improved from 0.07176 to 0.06772, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.0728 - accuracy: 0.9789 - val_loss: 0.0677 - val_accuracy: 0.9808 - lr: 0.0021\n",
            "Epoch 11/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9779\n",
            "Epoch 11: val_loss improved from 0.06772 to 0.06463, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.0754 - accuracy: 0.9779 - val_loss: 0.0646 - val_accuracy: 0.9819 - lr: 0.0021\n",
            "Epoch 12/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.9816\n",
            "Epoch 12: val_loss did not improve from 0.06463\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.0620 - accuracy: 0.9816 - val_loss: 0.0768 - val_accuracy: 0.9778 - lr: 0.0021\n",
            "Epoch 13/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9824\n",
            "Epoch 13: val_loss improved from 0.06463 to 0.05724, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.0593 - accuracy: 0.9824 - val_loss: 0.0572 - val_accuracy: 0.9841 - lr: 0.0021\n",
            "Epoch 14/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.9835\n",
            "Epoch 14: val_loss did not improve from 0.05724\n",
            "2189/2189 [==============================] - 65s 30ms/step - loss: 0.0570 - accuracy: 0.9835 - val_loss: 0.0659 - val_accuracy: 0.9827 - lr: 0.0021\n",
            "Epoch 15/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9821\n",
            "Epoch 15: val_loss did not improve from 0.05724\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.0630 - accuracy: 0.9821 - val_loss: 0.0596 - val_accuracy: 0.9825 - lr: 0.0021\n",
            "Epoch 16/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9846\n",
            "Epoch 16: val_loss did not improve from 0.05724\n",
            "2189/2189 [==============================] - 65s 30ms/step - loss: 0.0500 - accuracy: 0.9846 - val_loss: 0.0617 - val_accuracy: 0.9840 - lr: 0.0021\n",
            "Epoch 17/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9861\n",
            "Epoch 17: val_loss improved from 0.05724 to 0.05590, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.0489 - accuracy: 0.9861 - val_loss: 0.0559 - val_accuracy: 0.9868 - lr: 0.0021\n",
            "Epoch 18/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9867\n",
            "Epoch 18: val_loss improved from 0.05590 to 0.05268, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.0457 - accuracy: 0.9867 - val_loss: 0.0527 - val_accuracy: 0.9856 - lr: 0.0021\n",
            "Epoch 19/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9865\n",
            "Epoch 19: val_loss improved from 0.05268 to 0.05223, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.0456 - accuracy: 0.9865 - val_loss: 0.0522 - val_accuracy: 0.9865 - lr: 0.0021\n",
            "Epoch 20/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9875\n",
            "Epoch 20: val_loss did not improve from 0.05223\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0421 - accuracy: 0.9875 - val_loss: 0.0764 - val_accuracy: 0.9810 - lr: 0.0021\n",
            "Epoch 21/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9878\n",
            "Epoch 21: val_loss improved from 0.05223 to 0.04825, saving model to best_model_LSTM_bidirectional_2_with_best_hps.h5\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.0404 - accuracy: 0.9878 - val_loss: 0.0482 - val_accuracy: 0.9868 - lr: 0.0021\n",
            "Epoch 22/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9884\n",
            "Epoch 22: val_loss did not improve from 0.04825\n",
            "2189/2189 [==============================] - 65s 30ms/step - loss: 0.0382 - accuracy: 0.9884 - val_loss: 0.0576 - val_accuracy: 0.9851 - lr: 0.0021\n",
            "Epoch 23/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9889\n",
            "Epoch 23: val_loss did not improve from 0.04825\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.0362 - accuracy: 0.9889 - val_loss: 0.0581 - val_accuracy: 0.9840 - lr: 0.0021\n",
            "Epoch 24/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9892\n",
            "Epoch 24: val_loss did not improve from 0.04825\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.0362 - accuracy: 0.9892 - val_loss: 0.0582 - val_accuracy: 0.9873 - lr: 0.0021\n",
            "Epoch 25/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9897\n",
            "Epoch 25: val_loss did not improve from 0.04825\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.0337 - accuracy: 0.9897 - val_loss: 0.0564 - val_accuracy: 0.9862 - lr: 0.0021\n",
            "Epoch 26/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9894\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "\n",
            "Epoch 26: val_loss did not improve from 0.04825\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0334 - accuracy: 0.9894 - val_loss: 0.0530 - val_accuracy: 0.9867 - lr: 0.0021\n",
            "Epoch 27/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9929\n",
            "Epoch 27: val_loss did not improve from 0.04825\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0222 - accuracy: 0.9929 - val_loss: 0.0525 - val_accuracy: 0.9882 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9935\n",
            "Epoch 28: val_loss did not improve from 0.04825\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.0190 - accuracy: 0.9935 - val_loss: 0.0592 - val_accuracy: 0.9887 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9940\n",
            "Epoch 29: val_loss did not improve from 0.04825\n",
            "2189/2189 [==============================] - 68s 31ms/step - loss: 0.0174 - accuracy: 0.9940 - val_loss: 0.0674 - val_accuracy: 0.9885 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9944\n",
            "Epoch 30: val_loss did not improve from 0.04825\n",
            "2189/2189 [==============================] - 67s 31ms/step - loss: 0.0166 - accuracy: 0.9944 - val_loss: 0.0659 - val_accuracy: 0.9881 - lr: 0.0010\n",
            "Epoch 31/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9941Restoring model weights from the end of the best epoch: 21.\n",
            "\n",
            "Epoch 31: val_loss did not improve from 0.04825\n",
            "2189/2189 [==============================] - 66s 30ms/step - loss: 0.0159 - accuracy: 0.9941 - val_loss: 0.0696 - val_accuracy: 0.9887 - lr: 0.0010\n",
            "Epoch 31: early stopping\n",
            "548/548 - 6s - loss: 0.0482 - accuracy: 0.9868 - 6s/epoch - 11ms/step\n",
            "Test Loss: 0.0482456311583519\n",
            "Test Accuracy: 0.9867511987686157\n",
            "548/548 [==============================] - 7s 11ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99     14579\n",
            "           1       0.91      0.82      0.86       426\n",
            "           2       0.95      0.97      0.96      1112\n",
            "           3       0.92      0.75      0.83       145\n",
            "           4       0.99      0.99      0.99      1249\n",
            "\n",
            "    accuracy                           0.99     17511\n",
            "   macro avg       0.95      0.90      0.93     17511\n",
            "weighted avg       0.99      0.99      0.99     17511\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Model: Hyperparamater tuning a 1-layer bidirectional LTSM\n",
        "\n",
        "it turns out 1 bidirectional LTSM layer performs about as well as a more complex 2- or 3-layered bidirectional LTSM. So go with the simpler model!"
      ],
      "metadata": {
        "id": "VgvRabgraMSR"
      },
      "id": "VgvRabgraMSR"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras_tuner import HyperModel, Hyperband\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "class LSTMHyperModel1Layer(HyperModel):\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "        model.add(Bidirectional(LSTM(units=hp.Int('units_1', min_value=32, max_value=128, step=32), return_sequences=False), input_shape=self.input_shape))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_1', min_value=0.0, max_value=0.5, default=0.25, step=0.1)))\n",
        "\n",
        "        model.add(Dense(units=hp.Int('dense_units', min_value=50, max_value=150, step=50), activation='relu'))\n",
        "        model.add(Dropout(rate=hp.Float('dropout_2', min_value=0.0, max_value=0.5, default=0.25, step=0.1)))\n",
        "\n",
        "        model.add(Dense(self.num_classes, activation='softmax'))\n",
        "\n",
        "        model.compile(optimizer=Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "\n",
        "# Load your data\n",
        "X = fazeli_mitbih_train_df.iloc[:, :-1].values\n",
        "y = fazeli_mitbih_train_df.iloc[:, -1].values\n",
        "\n",
        "# Preprocess the data\n",
        "X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "y_categorical = to_categorical(y)\n",
        "num_classes = y_categorical.shape[1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001, verbose=1),\n",
        "    ModelCheckpoint('best_model_LSTM_1layer.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "]\n",
        "\n",
        "# Instantiate and configure the hypermodel\n",
        "hypermodel = LSTMHyperModel1Layer(input_shape=(187, 1), num_classes=num_classes)\n",
        "\n",
        "# Instantiate the tuner\n",
        "tuner = Hyperband(\n",
        "    hypermodel,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=10,\n",
        "    directory='hyperband',\n",
        "    project_name='mitbih_lstm_classification'\n",
        ")\n",
        "\n",
        "# Start the search for the best hyperparameter configuration\n",
        "tuner.search(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=10, callbacks=callbacks)\n"
      ],
      "metadata": {
        "id": "jPaQNKdkaL7L"
      },
      "id": "jPaQNKdkaL7L",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "for param in best_hps.values:\n",
        "    print(f\"{param}: {best_hps.get(param)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMawztE1bOvQ",
        "outputId": "a911e3a0-8956-4b1b-dca9-088f58927a7f"
      },
      "id": "IMawztE1bOvQ",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "units_1: 128\n",
            "dropout_1: 0.1\n",
            "dense_units: 100\n",
            "dropout_2: 0.2\n",
            "learning_rate: 0.0007043882704094401\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 4\n",
            "tuner/bracket: 2\n",
            "tuner/round: 2\n",
            "tuner/trial_id: 0012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_model_LSTM_1layer_with_best_hps():\n",
        "    input_shape = (187, 1)  # Assuming this is your input shape based on the dataset\n",
        "    best_units_1 = 128  # Best units for the first LSTM layer\n",
        "    best_dropout_1 = 0.1  # Best dropout rate after the first LSTM layer\n",
        "    best_dense_units = 100  # Best units for the dense layer\n",
        "    best_dropout_2 = 0.2  # Best dropout rate after the dense layer\n",
        "    best_learning_rate = 0.0007043882704094401  # Best learning rate for the optimizer\n",
        "\n",
        "    model = Sequential([\n",
        "        Bidirectional(LSTM(units=best_units_1, return_sequences=False), input_shape=input_shape),\n",
        "        Dropout(best_dropout_1),\n",
        "\n",
        "        Dense(units=best_dense_units, activation='relu'),\n",
        "        Dropout(best_dropout_2),\n",
        "\n",
        "        Dense(5, activation='softmax')  # Assuming 5 classes\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=best_learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the LSTM model with best hyperparameters\n",
        "model_LSTM_1layer_with_best_hps = create_model_LSTM_1layer_with_best_hps()\n"
      ],
      "metadata": {
        "id": "q5ZwwwRfuN1e"
      },
      "id": "q5ZwwwRfuN1e",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Assuming fazeli_mitbih_train_df is your DataFrame and the last column is the label\n",
        "X = fazeli_mitbih_train_df.iloc[:, :-1].values\n",
        "y = fazeli_mitbih_train_df.iloc[:, -1].values\n",
        "\n",
        "# Reshape X to fit the LSTM input requirements and convert y to categorical\n",
        "X_reshaped = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "y_categorical = to_categorical(y)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define callbacks\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'best_model_LSTM_1layer_with_best_hps.h5',  # Updated filename for the model with best hyperparameters\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=5,\n",
        "    min_lr=0.001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Assuming create_model_LSTM_1layer_with_best_hps() is defined as previously shown\n",
        "model_LSTM_1layer_with_best_hps = create_model_LSTM_1layer_with_best_hps()\n",
        "\n",
        "# Train the model with the callbacks\n",
        "history = model_LSTM_1layer_with_best_hps.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=100,  # Adjust the number of epochs as necessary\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, reduce_lr, model_checkpoint]\n",
        ")\n",
        "\n",
        "# Evaluation on Test Data\n",
        "test_loss, test_accuracy = model_LSTM_1layer_with_best_hps.evaluate(X_test, y_test, verbose=2)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Predictions and Classification Report\n",
        "y_pred = model_LSTM_1layer_with_best_hps.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZtSnpBFvj1-",
        "outputId": "024a2170-7455-48f7-bee7-c6f4d16f4b47"
      },
      "id": "WZtSnpBFvj1-",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.3907 - accuracy: 0.8980\n",
            "Epoch 1: val_loss improved from inf to 0.23613, saving model to best_model_LSTM_1layer_with_best_hps.h5\n",
            "2189/2189 [==============================] - 40s 17ms/step - loss: 0.3906 - accuracy: 0.8981 - val_loss: 0.2361 - val_accuracy: 0.9394 - lr: 7.0439e-04\n",
            "Epoch 2/100\n",
            "   9/2189 [..............................] - ETA: 32s - loss: 0.2913 - accuracy: 0.9201"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.2149 - accuracy: 0.9444\n",
            "Epoch 2: val_loss improved from 0.23613 to 0.17525, saving model to best_model_LSTM_1layer_with_best_hps.h5\n",
            "2189/2189 [==============================] - 35s 16ms/step - loss: 0.2149 - accuracy: 0.9444 - val_loss: 0.1752 - val_accuracy: 0.9502 - lr: 7.0439e-04\n",
            "Epoch 3/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1701 - accuracy: 0.9545\n",
            "Epoch 3: val_loss improved from 0.17525 to 0.15352, saving model to best_model_LSTM_1layer_with_best_hps.h5\n",
            "2189/2189 [==============================] - 35s 16ms/step - loss: 0.1701 - accuracy: 0.9545 - val_loss: 0.1535 - val_accuracy: 0.9564 - lr: 7.0439e-04\n",
            "Epoch 4/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9605\n",
            "Epoch 4: val_loss improved from 0.15352 to 0.10841, saving model to best_model_LSTM_1layer_with_best_hps.h5\n",
            "2189/2189 [==============================] - 35s 16ms/step - loss: 0.1440 - accuracy: 0.9605 - val_loss: 0.1084 - val_accuracy: 0.9697 - lr: 7.0439e-04\n",
            "Epoch 5/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.1227 - accuracy: 0.9664\n",
            "Epoch 5: val_loss did not improve from 0.10841\n",
            "2189/2189 [==============================] - 35s 16ms/step - loss: 0.1227 - accuracy: 0.9664 - val_loss: 0.1089 - val_accuracy: 0.9689 - lr: 7.0439e-04\n",
            "Epoch 6/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.1115 - accuracy: 0.9689\n",
            "Epoch 6: val_loss did not improve from 0.10841\n",
            "2189/2189 [==============================] - 41s 19ms/step - loss: 0.1114 - accuracy: 0.9689 - val_loss: 0.1323 - val_accuracy: 0.9603 - lr: 7.0439e-04\n",
            "Epoch 7/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.9732\n",
            "Epoch 7: val_loss improved from 0.10841 to 0.08197, saving model to best_model_LSTM_1layer_with_best_hps.h5\n",
            "2189/2189 [==============================] - 42s 19ms/step - loss: 0.0967 - accuracy: 0.9732 - val_loss: 0.0820 - val_accuracy: 0.9760 - lr: 7.0439e-04\n",
            "Epoch 8/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0890 - accuracy: 0.9747\n",
            "Epoch 8: val_loss improved from 0.08197 to 0.07911, saving model to best_model_LSTM_1layer_with_best_hps.h5\n",
            "2189/2189 [==============================] - 42s 19ms/step - loss: 0.0889 - accuracy: 0.9747 - val_loss: 0.0791 - val_accuracy: 0.9778 - lr: 7.0439e-04\n",
            "Epoch 9/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0812 - accuracy: 0.9769\n",
            "Epoch 9: val_loss improved from 0.07911 to 0.06951, saving model to best_model_LSTM_1layer_with_best_hps.h5\n",
            "2189/2189 [==============================] - 42s 19ms/step - loss: 0.0812 - accuracy: 0.9769 - val_loss: 0.0695 - val_accuracy: 0.9799 - lr: 7.0439e-04\n",
            "Epoch 10/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0763 - accuracy: 0.9782\n",
            "Epoch 10: val_loss improved from 0.06951 to 0.06641, saving model to best_model_LSTM_1layer_with_best_hps.h5\n",
            "2189/2189 [==============================] - 42s 19ms/step - loss: 0.0764 - accuracy: 0.9782 - val_loss: 0.0664 - val_accuracy: 0.9812 - lr: 7.0439e-04\n",
            "Epoch 11/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.9799\n",
            "Epoch 11: val_loss did not improve from 0.06641\n",
            "2189/2189 [==============================] - 41s 19ms/step - loss: 0.0714 - accuracy: 0.9798 - val_loss: 0.0685 - val_accuracy: 0.9794 - lr: 7.0439e-04\n",
            "Epoch 12/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0665 - accuracy: 0.9810\n",
            "Epoch 12: val_loss improved from 0.06641 to 0.06588, saving model to best_model_LSTM_1layer_with_best_hps.h5\n",
            "2189/2189 [==============================] - 43s 20ms/step - loss: 0.0664 - accuracy: 0.9810 - val_loss: 0.0659 - val_accuracy: 0.9809 - lr: 7.0439e-04\n",
            "Epoch 13/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9806\n",
            "Epoch 13: val_loss improved from 0.06588 to 0.05540, saving model to best_model_LSTM_1layer_with_best_hps.h5\n",
            "2189/2189 [==============================] - 44s 20ms/step - loss: 0.0661 - accuracy: 0.9806 - val_loss: 0.0554 - val_accuracy: 0.9848 - lr: 7.0439e-04\n",
            "Epoch 14/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0640 - accuracy: 0.9815\n",
            "Epoch 14: val_loss did not improve from 0.05540\n",
            "2189/2189 [==============================] - 43s 20ms/step - loss: 0.0641 - accuracy: 0.9815 - val_loss: 0.0640 - val_accuracy: 0.9814 - lr: 7.0439e-04\n",
            "Epoch 15/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0569 - accuracy: 0.9830\n",
            "Epoch 15: val_loss did not improve from 0.05540\n",
            "2189/2189 [==============================] - 43s 20ms/step - loss: 0.0569 - accuracy: 0.9830 - val_loss: 0.0613 - val_accuracy: 0.9817 - lr: 7.0439e-04\n",
            "Epoch 16/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0649 - accuracy: 0.9812\n",
            "Epoch 16: val_loss did not improve from 0.05540\n",
            "2189/2189 [==============================] - 42s 19ms/step - loss: 0.0649 - accuracy: 0.9812 - val_loss: 0.0573 - val_accuracy: 0.9829 - lr: 7.0439e-04\n",
            "Epoch 17/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0577 - accuracy: 0.9829\n",
            "Epoch 17: val_loss did not improve from 0.05540\n",
            "2189/2189 [==============================] - 42s 19ms/step - loss: 0.0578 - accuracy: 0.9829 - val_loss: 0.0570 - val_accuracy: 0.9838 - lr: 7.0439e-04\n",
            "Epoch 18/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0566 - accuracy: 0.9838\n",
            "Epoch 18: val_loss improved from 0.05540 to 0.05428, saving model to best_model_LSTM_1layer_with_best_hps.h5\n",
            "2189/2189 [==============================] - 42s 19ms/step - loss: 0.0566 - accuracy: 0.9838 - val_loss: 0.0543 - val_accuracy: 0.9840 - lr: 7.0439e-04\n",
            "Epoch 19/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 0.9834\n",
            "Epoch 19: val_loss improved from 0.05428 to 0.05246, saving model to best_model_LSTM_1layer_with_best_hps.h5\n",
            "2189/2189 [==============================] - 42s 19ms/step - loss: 0.0564 - accuracy: 0.9834 - val_loss: 0.0525 - val_accuracy: 0.9847 - lr: 7.0439e-04\n",
            "Epoch 20/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0498 - accuracy: 0.9855\n",
            "Epoch 20: val_loss improved from 0.05246 to 0.05227, saving model to best_model_LSTM_1layer_with_best_hps.h5\n",
            "2189/2189 [==============================] - 41s 19ms/step - loss: 0.0498 - accuracy: 0.9855 - val_loss: 0.0523 - val_accuracy: 0.9853 - lr: 7.0439e-04\n",
            "Epoch 21/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0485 - accuracy: 0.9856\n",
            "Epoch 21: val_loss did not improve from 0.05227\n",
            "2189/2189 [==============================] - 42s 19ms/step - loss: 0.0485 - accuracy: 0.9856 - val_loss: 0.0615 - val_accuracy: 0.9820 - lr: 7.0439e-04\n",
            "Epoch 22/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0470 - accuracy: 0.9860\n",
            "Epoch 22: val_loss improved from 0.05227 to 0.04845, saving model to best_model_LSTM_1layer_with_best_hps.h5\n",
            "2189/2189 [==============================] - 44s 20ms/step - loss: 0.0470 - accuracy: 0.9860 - val_loss: 0.0484 - val_accuracy: 0.9860 - lr: 7.0439e-04\n",
            "Epoch 23/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 0.9872\n",
            "Epoch 23: val_loss did not improve from 0.04845\n",
            "2189/2189 [==============================] - 43s 20ms/step - loss: 0.0437 - accuracy: 0.9872 - val_loss: 0.0505 - val_accuracy: 0.9860 - lr: 7.0439e-04\n",
            "Epoch 24/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0431 - accuracy: 0.9873\n",
            "Epoch 24: val_loss did not improve from 0.04845\n",
            "2189/2189 [==============================] - 43s 19ms/step - loss: 0.0431 - accuracy: 0.9873 - val_loss: 0.0519 - val_accuracy: 0.9850 - lr: 7.0439e-04\n",
            "Epoch 25/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0409 - accuracy: 0.9878\n",
            "Epoch 25: val_loss did not improve from 0.04845\n",
            "2189/2189 [==============================] - 42s 19ms/step - loss: 0.0409 - accuracy: 0.9878 - val_loss: 0.0491 - val_accuracy: 0.9864 - lr: 7.0439e-04\n",
            "Epoch 26/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9885\n",
            "Epoch 26: val_loss improved from 0.04845 to 0.04596, saving model to best_model_LSTM_1layer_with_best_hps.h5\n",
            "2189/2189 [==============================] - 42s 19ms/step - loss: 0.0387 - accuracy: 0.9885 - val_loss: 0.0460 - val_accuracy: 0.9872 - lr: 7.0439e-04\n",
            "Epoch 27/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0383 - accuracy: 0.9887\n",
            "Epoch 27: val_loss did not improve from 0.04596\n",
            "2189/2189 [==============================] - 42s 19ms/step - loss: 0.0382 - accuracy: 0.9887 - val_loss: 0.0494 - val_accuracy: 0.9866 - lr: 7.0439e-04\n",
            "Epoch 28/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0361 - accuracy: 0.9888\n",
            "Epoch 28: val_loss improved from 0.04596 to 0.04543, saving model to best_model_LSTM_1layer_with_best_hps.h5\n",
            "2189/2189 [==============================] - 43s 20ms/step - loss: 0.0361 - accuracy: 0.9888 - val_loss: 0.0454 - val_accuracy: 0.9871 - lr: 7.0439e-04\n",
            "Epoch 29/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0398 - accuracy: 0.9880\n",
            "Epoch 29: val_loss did not improve from 0.04543\n",
            "2189/2189 [==============================] - 44s 20ms/step - loss: 0.0398 - accuracy: 0.9880 - val_loss: 0.0493 - val_accuracy: 0.9872 - lr: 7.0439e-04\n",
            "Epoch 30/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0356 - accuracy: 0.9897\n",
            "Epoch 30: val_loss improved from 0.04543 to 0.04538, saving model to best_model_LSTM_1layer_with_best_hps.h5\n",
            "2189/2189 [==============================] - 42s 19ms/step - loss: 0.0356 - accuracy: 0.9897 - val_loss: 0.0454 - val_accuracy: 0.9881 - lr: 7.0439e-04\n",
            "Epoch 31/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0342 - accuracy: 0.9895\n",
            "Epoch 31: val_loss did not improve from 0.04538\n",
            "2189/2189 [==============================] - 43s 19ms/step - loss: 0.0342 - accuracy: 0.9895 - val_loss: 0.0561 - val_accuracy: 0.9845 - lr: 7.0439e-04\n",
            "Epoch 32/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9898\n",
            "Epoch 32: val_loss improved from 0.04538 to 0.04241, saving model to best_model_LSTM_1layer_with_best_hps.h5\n",
            "2189/2189 [==============================] - 43s 20ms/step - loss: 0.0333 - accuracy: 0.9898 - val_loss: 0.0424 - val_accuracy: 0.9884 - lr: 7.0439e-04\n",
            "Epoch 33/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9897\n",
            "Epoch 33: val_loss did not improve from 0.04241\n",
            "2189/2189 [==============================] - 43s 20ms/step - loss: 0.0321 - accuracy: 0.9897 - val_loss: 0.0493 - val_accuracy: 0.9877 - lr: 7.0439e-04\n",
            "Epoch 34/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0322 - accuracy: 0.9899\n",
            "Epoch 34: val_loss did not improve from 0.04241\n",
            "2189/2189 [==============================] - 42s 19ms/step - loss: 0.0322 - accuracy: 0.9899 - val_loss: 0.0497 - val_accuracy: 0.9878 - lr: 7.0439e-04\n",
            "Epoch 35/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0305 - accuracy: 0.9904\n",
            "Epoch 35: val_loss did not improve from 0.04241\n",
            "2189/2189 [==============================] - 36s 17ms/step - loss: 0.0305 - accuracy: 0.9904 - val_loss: 0.0503 - val_accuracy: 0.9869 - lr: 7.0439e-04\n",
            "Epoch 36/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.0326 - accuracy: 0.9895\n",
            "Epoch 36: val_loss did not improve from 0.04241\n",
            "2189/2189 [==============================] - 37s 17ms/step - loss: 0.0327 - accuracy: 0.9895 - val_loss: 0.0490 - val_accuracy: 0.9876 - lr: 7.0439e-04\n",
            "Epoch 37/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9902\n",
            "Epoch 37: val_loss did not improve from 0.04241\n",
            "2189/2189 [==============================] - 37s 17ms/step - loss: 0.0309 - accuracy: 0.9902 - val_loss: 0.0545 - val_accuracy: 0.9863 - lr: 7.0439e-04\n",
            "Epoch 38/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0274 - accuracy: 0.9914\n",
            "Epoch 38: val_loss did not improve from 0.04241\n",
            "2189/2189 [==============================] - 37s 17ms/step - loss: 0.0274 - accuracy: 0.9914 - val_loss: 0.0486 - val_accuracy: 0.9880 - lr: 7.0439e-04\n",
            "Epoch 39/100\n",
            "2188/2189 [============================>.] - ETA: 0s - loss: 0.0266 - accuracy: 0.9914\n",
            "Epoch 39: val_loss did not improve from 0.04241\n",
            "2189/2189 [==============================] - 36s 16ms/step - loss: 0.0266 - accuracy: 0.9914 - val_loss: 0.0472 - val_accuracy: 0.9889 - lr: 7.0439e-04\n",
            "Epoch 40/100\n",
            "2186/2189 [============================>.] - ETA: 0s - loss: 0.0267 - accuracy: 0.9913\n",
            "Epoch 40: val_loss did not improve from 0.04241\n",
            "2189/2189 [==============================] - 36s 17ms/step - loss: 0.0267 - accuracy: 0.9913 - val_loss: 0.0483 - val_accuracy: 0.9885 - lr: 7.0439e-04\n",
            "Epoch 41/100\n",
            "2189/2189 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9911\n",
            "Epoch 41: val_loss did not improve from 0.04241\n",
            "2189/2189 [==============================] - 36s 17ms/step - loss: 0.0278 - accuracy: 0.9911 - val_loss: 0.0481 - val_accuracy: 0.9874 - lr: 7.0439e-04\n",
            "Epoch 42/100\n",
            "2187/2189 [============================>.] - ETA: 0s - loss: 0.0261 - accuracy: 0.9914Restoring model weights from the end of the best epoch: 32.\n",
            "\n",
            "Epoch 42: val_loss did not improve from 0.04241\n",
            "2189/2189 [==============================] - 36s 17ms/step - loss: 0.0261 - accuracy: 0.9914 - val_loss: 0.0538 - val_accuracy: 0.9873 - lr: 7.0439e-04\n",
            "Epoch 42: early stopping\n",
            "548/548 - 3s - loss: 0.0424 - accuracy: 0.9884 - 3s/epoch - 6ms/step\n",
            "Test Loss: 0.042414721101522446\n",
            "Test Accuracy: 0.9884073138237\n",
            "548/548 [==============================] - 4s 6ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99     14579\n",
            "           1       0.94      0.84      0.89       426\n",
            "           2       0.98      0.96      0.97      1112\n",
            "           3       0.79      0.86      0.82       145\n",
            "           4       0.99      0.99      0.99      1249\n",
            "\n",
            "    accuracy                           0.99     17511\n",
            "   macro avg       0.94      0.93      0.93     17511\n",
            "weighted avg       0.99      0.99      0.99     17511\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[0, 1, 2, 3, 4], yticklabels=[0, 1, 2, 3, 4])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.ylabel(\"Actual Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "id": "ignBm-cfwz5t",
        "outputId": "45ce38e6-c2d8-444f-e50f-0874220f07de"
      },
      "id": "ignBm-cfwz5t",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Eg0aF5Z53wEA"
      },
      "id": "Eg0aF5Z53wEA",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:tf-gpu] *",
      "language": "python",
      "name": "conda-env-tf-gpu-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}